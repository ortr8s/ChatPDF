# ChatPDF

A Retrieval-Augmented Generation (RAG) system for conversational interactions with PDF documents. This system combines hybrid retrieval (lexical + semantic), reranking, and LLM integration to deliver accurate, context-grounded answers from your document collections.

## Features

- **PDF Document Processing**: Intelligent text extraction with overlapping chunk windows for context preservation
- **Hybrid Retrieval**: Combines BM25 lexical search with semantic embeddings for comprehensive document matching
- **Reranking**: Cross-encoder reranking ensures top results are contextually relevant
- **Streaming Response**: Real-time token streaming with source attribution
- **Modular Architecture**: Clean separation of concerns across ingestion, retrieval, and generation
- **Configurable**: YAML-based configuration for all parameters (models, chunk sizes, device selection, quantization)
- **Efficient**: GPU acceleration support, model quantization for resource-constrained environments, and intelligent caching

## System Architecture

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DOCUMENT INGESTION PIPELINE                  â”‚
â”‚ PDF â†’ Text Extraction â†’ Chunking â†’ Tokenization      â”‚
â”‚        â†“ Semantic Embeddings â†’ Knowledge Base        â”‚
â”‚        â†“ BM25 Indexing â†’ Lexical Index               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    â”œâ”€â”€â”€ Lexical Index (BM25 sparse vectors)
    â”œâ”€â”€â”€ Semantic Index (Dense embeddings)
    â””â”€â”€â”€ Document Metadata & Source Mapping
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         QUERY PROCESSING PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Lexical Retriever  â†’ Top-k BM25 matches          â”‚
â”‚ 2. Semantic Retriever â†’ Top-k semantic neighbors    â”‚
â”‚ 3. Merge & Deduplicate â†’ Unified result set         â”‚
â”‚ 4. Reranker (Cross-Encoder) â†’ Score & sort         â”‚
â”‚ 5. LLM Generation â†’ Stream context-aware response   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Answer with Source Attribution & Document References
```

## Core Components

| Component | Purpose | Technology |
|-----------|---------|-----------|
| **CLI Interface** ([commands.py](src/cli/commands.py)) | User interaction & command handling | Typer + Rich |
| **RAG Orchestrator** ([rag.py](src/core/rag.py)) | Coordinates retrieval & generation pipelines | Custom integration |
| **Semantic Retriever** ([semantic_retriever.py](src/core/semantic_retriever.py)) | Dense vector retrieval | Sentence Transformers |
| **Lexical Retriever** ([lexical_retriever.py](src/core/lexical_retriever.py)) | Keyword-based search | BM25 + spaCy |
| **Reranker** ([reranker.py](src/core/reranker.py)) | Result relevance scoring | Cross-Encoder models |
| **Text Generator** ([text_generator.py](src/core/text_generator.py)) | LLM-based response generation | HuggingFace Transformers |
| **PDF Reader** ([reader.py](src/scraper/reader.py)) | Document parsing & chunking | pdfplumber, pypdf, pdfminer |
| **Knowledge Base** ([knowledge_base.py](src/core/rag_components/knowledge_base.py)) | Persistent storage of embeddings & indexes | Serialized cache |
| **Search Engine** ([search_engine.py](src/core/rag_components/search_engine.py)) | Query execution & result aggregation | Combined lexical/semantic |
| **Ingestion Pipeline** ([ingestion_pipeline.py](src/core/rag_components/ingestion_pipeline.py)) | Document processing workflow | Document â†’ Index |

## Installation

### Prerequisites

- **Python 3.9+** - Core language runtime
- **CUDA 11.8+** (optional) - For GPU acceleration of embeddings and inference
- **~8GB RAM** minimum (16GB+ recommended with GPU)
- **Disk space** - ~5GB for model weights (varies by quantization settings)

### Quick Setup

1. **Clone and navigate to project**:
   ```bash
   git clone <repository-url>
   cd ChatPDF
   ```

2. **Create and activate virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   python -m spacy download en_core_web_sm
   ```

4. **Verify installation**:
   ```bash
   python -c "import torch, transformers, sentence_transformers; print('âœ“ All dependencies installed')"
   ```

## Configuration

The [config.yaml](config.yaml) file controls all system behavior:

### Model Configuration
```yaml
models:
  semantic_retriever: "sentence-transformers/all-MiniLM-L6-v2"  # 22M params, fast embeddings
  reranker: "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"        # Ranking model
  generator: "microsoft/Phi-3.5-mini-instruct"                   # 3.8B param LLM
```

### Retrieval Settings
```yaml
retrieval:
  chunk_size: 512           # Tokens per document chunk
  chunk_overlap: 64         # Overlap between chunks (context preservation)
  batch_size: 32            # Batch size for embedding
  top_k_lexical: 5          # BM25 results to retrieve
  top_k_semantic: 5         # Semantic results to retrieve
  rerank_top_k: 3           # Final documents for LLM context
```

### LLM Generation Settings
```yaml
llm:
  temperature: 0.7          # Creativity (0=deterministic, 1=random)
  max_tokens: 500           # Max response length
  top_p: 0.9                # Nucleus sampling
  attn_implementation: "eager"  # Attention: "eager", "sdpa", or "flash_attention_2"
  quantize:
    enable: true            # 4-bit quantization for efficiency
    how_many_bits: 4        # 4-bit or 8-bit quantization
  device: "cuda"            # "cuda" for GPU, "cpu" for CPU
```

### Caching
```yaml
cache:
  directory: ".chatpdf_cache"      # Where to store embeddings & indexes
  use_embeddings_cache: true       # Reuse embeddings on restart
  auto_invalidate: true            # Auto-clear stale cache
```

## CLI Commands

| Command | Description |
|---------|-------------|
| `python -m src.main ingest <path>` | Ingest PDF documents from a directory |
| `python -m src.main chat` | Start interactive chat session |
| `python -m src.main cache-info` | Display cache statistics and file list |
| `python -m src.main clear-cache` | Clear all cached embeddings and indexes |

## Quick Start

### 1. Ingest Documents

Prepare a directory with PDF files, then run:

```bash
python -m src.main ingest ./pdfs
```

This will:
- ğŸ“‚ Scan directory for all PDF files
- ğŸ“„ Extract text using PDF parsing libraries
- âœ‚ï¸ Create overlapping chunks (configurable size & overlap)
- ğŸ”¢ Generate semantic embeddings for chunks
- ğŸ·ï¸ Build BM25 lexical index with tokenization
- ğŸ’¾ Cache embeddings and indexes in `.chatpdf_cache/`

Example output:
![alt text](.gifs/ingestion-1.gif)

### 2. Interactive Chat

Start a chat session:

```bash
python -m src.main chat
```

Then interact naturally:

![alt text](.gifs/chat.gif)

Exit with `exit`, `quit`, or `Ctrl+C`.

### 3. Document Summarization

During a chat session, you can request a full document summary:

![alt text](.gifs/summarization.gif)

The summarization feature:
- Extracts all chunks from the specified PDF
- Uses a dedicated summarization prompt for comprehensive coverage
- Provides structured output with key sections highlighted

### 4. Cache Management

View cache information:

```bash
python -m src.main cache-info
```

Output:
![alt text](.gifs/cache-info.gif)

Clear all cached data:

```bash
python -m src.main clear-cache
```
Output:
![alt text](.gifs/clear-cache.gif)

## How It Works

### Document Processing Pipeline

```
PDF Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Text Extraction                      â”‚
â”‚    Multiple PDF libraries for           â”‚
â”‚    robustness (pdfplumber, pypdf)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Intelligent Chunking                 â”‚
â”‚    - Configurable chunk size            â”‚
â”‚    - Overlapping windows preserve       â”‚
â”‚      context at boundaries              â”‚
â”‚    - Token-based splitting              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Tokenization                         â”‚
â”‚    - spaCy NLP processing               â”‚
â”‚    - Lemmatization for normalization    â”‚
â”‚    - Token counting with tiktoken       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Dual Indexing                        â”‚
â”‚    â”œâ”€ Semantic: Dense embeddings        â”‚
â”‚    â”‚  (Sentence Transformers)           â”‚
â”‚    â””â”€ Lexical: BM25 sparse vectors      â”‚
â”‚       (Rank-bm25)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Processing Pipeline

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Lexical Retrieval                    â”‚
â”‚    BM25 scoring â†’ Top-5 matches         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Semantic Retrieval                   â”‚
â”‚    Embedding similarity â†’ Top-5 matches â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Result Merging                       â”‚
â”‚    Deduplicate & combine results        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Reranking                            â”‚
â”‚    Cross-encoder scores relevance       â”‚
â”‚    â†’ Select Top-3 for context           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. LLM Generation                       â”‚
â”‚    Feed context + query to LLM          â”‚
â”‚    Stream response tokens               â”‚
â”‚    Include source attribution           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
ChatPDF/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ config.yaml                        # System configuration
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pdfs/                              # Directory for PDF documents
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                        # Application entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ commands.py                # CLI interface (ingest, chat)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag.py                     # RAG orchestrator
â”‚   â”‚   â”œâ”€â”€ lexical_retriever.py       # BM25 retrieval
â”‚   â”‚   â”œâ”€â”€ semantic_retriever.py      # Dense vector retrieval
â”‚   â”‚   â”œâ”€â”€ reranker.py                # Cross-encoder reranking
â”‚   â”‚   â”œâ”€â”€ text_generator.py          # LLM response generation
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ rag_components/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ ingestion_pipeline.py  # Document processing
â”‚   â”‚       â”œâ”€â”€ knowledge_base.py      # Embeddings & indexes storage
â”‚   â”‚       â””â”€â”€ search_engine.py       # Query execution
â”‚   â”‚
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ reader.py                  # PDF text extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                  # YAML config loader
â”‚   â”‚   â”œâ”€â”€ logger.py                  # Logging utilities
â”‚   â”‚   â”œâ”€â”€ cli_utils.py               # CLI formatting & streaming
â”‚   â”‚   â”œâ”€â”€ lexical_utils.py           # NLP utilities (lemmatization)
â”‚   â”‚   â”œâ”€â”€ generator_utils.py         # LLM utilities
â”‚   â”‚   â”œâ”€â”€ prompt_utils.py            # Prompt templates
â”‚   â”‚   â””â”€â”€ serializer.py              # Cache serialization
â”‚   â”‚
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ evaluate_ragas.py          # RAGAS evaluation framework
â”‚       â””â”€â”€ calculate_scores.py        # Score calculation utilities
â”‚
â””â”€â”€ .chatpdf_cache/                    # Auto-generated embeddings cache
```


## Dependencies

The project uses carefully selected libraries for robustness and performance:

### Core ML & NLP
| Package | Purpose |
|---------|---------|
| `torch` | Deep learning framework for embeddings & LLM |
| `transformers` | HuggingFace models (reranker, LLM) |
| `sentence-transformers` | Semantic embeddings |
| `rank-bm25` | BM25 lexical retrieval |
| `spacy` | NLP tokenization and lemmatization |

### PDF Processing
| Package | Purpose |
|---------|---------|
| `pypdf` | PDF parsing|


### System & Utilities
| Package | Purpose |
|---------|---------|
| `typer` | Modern CLI framework |
| `rich` | Beautiful terminal output |
| `pydantic` | Data validation |
| `PyYAML` | Configuration parsing |
| `tiktoken` | Token counting for OpenAI models |
| `bitsandbytes` | 4-bit model quantization |
| `accelerate` | Efficient model loading |

### Evaluation
| Package | Purpose |
|---------|---------|
| `ragas` | RAG evaluation framework |
| `datasets` | HuggingFace datasets for benchmarking |
| `langchain-community` | LLM integrations for evaluation |

## Performance Optimization

### GPU Acceleration
Enable GPU processing in `config.yaml` for ~10-100x speedup:
```yaml
biencoder:
  device: "cuda"  # Use GPU for embeddings
llm:
  device: "cuda"  # Use GPU for inference
reranker:
  device: "cuda"  # Use GPU for reranking
```

### Model Quantization
Reduce memory footprint with 4-bit quantization:
```yaml
llm:
  quantize:
    enable: true
    how_many_bits: 4  # Reduces 13GB model to ~3GB
```

### Caching
Embeddings are cached automatically in `.chatpdf_cache/`:
- Ingestion only runs once
- Restart immediately loads cached embeddings
- Auto-invalidation clears stale cache

### Batch Processing
Optimize embedding generation:
```yaml
retrieval:
  batch_size: 32  # Process 32 chunks at once
                  # Increase for GPUs, decrease if OOM
```

## Usage Examples

### Example 1: Ingest HR Documentation
```bash
python -m src.main ingest ./docs/hr_policies/
```

### Example 2: Query with Follow-ups
```bash
$ python -m src.main chat
Interactive ChatPDF Terminal
Type 'exit', 'quit', or Ctrl+C to stop.

You: What are the vacation policies?
AI: According to the HR handbook, vacation policies include...
    Sources: hr_handbook.pdf (chunks 12, 45)

You: What about sick days?
AI: Sick day policies are outlined in...
    Sources: hr_handbook.pdf (chunks 18)
```

### Example 3: Large Document Collection
For processing 100+ PDFs:
1. Place all PDFs in a directory: `./large_corpus/`
2. Ingest: `python -m src.main ingest ./large_corpus/`
3. First run takes time (generates embeddings for all chunks)
4. Subsequent runs load from cache instantly
5. Chat will be responsive even with 1000+ chunks

## Troubleshooting

| Issue | Solution |
|-------|----------|
| **"No module named 'rank_bm25'"** | `pip install rank-bm25` |
| **"Error loading spacy model"** | `python -m spacy download en_core_web_sm` |
| **CUDA out of memory (OOM)** | Reduce `batch_size` in config.yaml or set `device: cpu` |
| **Slow semantic retrieval** | Enable GPU acceleration: set `device: cuda` in config.yaml |
| **Large ingestion time** | Check GPU is being used; CPU embedding generation is slow |
| **Cache not loading** | Delete `.chatpdf_cache/` and re-ingest documents |
| **LLM taking too long** | Use quantization (`quantize.enable: true`) to reduce model size |
| **Stale embeddings after deleting PDF** | `auto_invalidate: true` (default) now properly detects deleted files |

### Cache Invalidation Details

The `cache.auto_invalidate` option controls cache validation behavior:

```yaml
cache:
  auto_invalidate: true  # âœ… Recommended: Detect file deletions
```

**With `auto_invalidate: true` (strict mode)**:
- âœ… Detects when PDFs are deleted from the directory
- âœ… Automatically triggers full reindex when deletions detected
- âœ… Processes only new/changed files incrementally
- âœ… Ensures cache integrity and prevents stale embeddings

**With `auto_invalidate: false` (trust cache mode)**:
- âœ… Trusts existing cache without validation
- âœ… Only processes newly added files
- âŒ Does NOT detect deleted files (use with caution)
- â±ï¸ Faster startup but risks stale data

**Recommendation**: Keep `auto_invalidate: true` (default) for data integrity.

## Evaluation

The project includes RAGAS (Retrieval-Augmented Generation Assessment) integration for evaluating RAG pipeline quality:

### Metrics Evaluated

| Metric | Description |
|--------|-------------|
| **Faithfulness** | Measures if the answer is grounded in the retrieved context |
| **Answer Relevancy** | Evaluates how relevant the answer is to the question |
| **Context Precision** | Checks if retrieved context contains relevant information |
| **Context Recall** | Measures coverage of ground truth in retrieved context |

### Running Evaluation

```bash
# Evaluate using the Qasper dataset
python -m src.scripts.evaluate_ragas

# Calculate scores from evaluated results
python -m src.scripts.calculate_scores
```

Results are saved to `evaluated.csv` and `graded.csv` for analysis.

## Advanced Configuration

### Tuning Retrieval
```yaml
retrieval:
  # More chunks = better coverage, slower reranking
  top_k_lexical: 10      # Retrieve more BM25 results
  top_k_semantic: 10     # Retrieve more semantic results
  rerank_top_k: 5        # Keep top 5 for LLM context
```

### Changing Models
```yaml
models:
  # Use larger, more accurate models (requires more VRAM)
  semantic_retriever: "sentence-transformers/all-mpnet-base-v2"
  reranker: "cross-encoder/mmarco-ColBERTv2"
  generator: "mistral-7b"  # Requires 16GB+ VRAM
```

### Custom Chunk Sizes
```yaml
retrieval:
  chunk_size: 256        # Smaller chunks = more granular, slower
  chunk_size: 1024       # Larger chunks = broader context
  chunk_overlap: 128     # Increase for boundary sensitivity
```

## Architecture Deep Dive

### Hybrid Retrieval Strategy

**Why both BM25 and semantic search?**

- **BM25 (Lexical)**: Perfect for exact keywords, acronyms, and proper nouns
  - Example: "What is the AI strategy?" â†’ Finds documents with keyword "AI"
  
- **Semantic**: Understands intent and paraphrasing
  - Example: "What's the artificial intelligence plan?" â†’ Finds same documents despite different wording

- **Combined**: Cover both exact matches and conceptual matches
  - Results are merged and deduplicated before reranking

### Reranking Pipeline

Cross-encoder models are expensive but extremely accurate:
1. Lexical retriever: 5 results (fast)
2. Semantic retriever: 5 results (fast)
3. Merge & deduplicate: ~8 unique results
4. Reranker: Score all 8 (expensive but thorough)
5. LLM gets: Top 3 highest-scoring results (cost-efficient)

This balance minimizes latency while maximizing answer quality.

### Knowledge Base Management

The knowledge base stores:
- **Chunk embeddings**: Dense vectors for semantic search
- **BM25 index**: Term frequencies and IDF scores
- **Document metadata**: Filenames, chunk positions, original text
- **Tokenizer state**: For consistent chunk splitting

All data is serialized to `.chatpdf_cache/` for persistence.

## License

MIT License - See LICENSE file for details

## Support

- ğŸ“– Check [config.yaml](config.yaml) for detailed configuration options
- ğŸ”§ Review [src/core/rag.py](src/core/rag.py) for RAG pipeline implementation
- ğŸ› See **Troubleshooting** section above for common issues
- ğŸ’¬ Open an issue for bugs or feature requests

---