user_input,retrieved_contexts,response,reference,context_recall,context_precision,faithfulness,answer_relevancy
which multilingual approaches do they compare with?,"['oparl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.::: Main Results ::: Results on MultiUN Dataset.\n\nLike experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches as shown in Table TABREF26. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs significantly better than MNMT BIBREF19.baseline and various multilingual NMT approaches.\n\n\n## Introduction\n\nAlthough Neural Machine Translation (NMT) has dominated recent research on translation tasks BIBREF0, BIBREF1, BIBREF2, NMT heavily relies on large-scale parallel data, resulting in poor performance on low-resource or zero-resource language pairs BIBREF3.']","According to the document 1912.01214, the multilingual approaches they compare with include MNMT BIBREF19 and cross-lingual transfer without pretraining BIBREF16.",BIBREF19 BIBREF20,0.0,0.9999999999,1.0,0.9011187062565589
what are the pivot-based baselines?,"['racted widespread attention in academic research. Existing methods are mainly divided into four categories: pivot-based method, transfer learning, multilingual NMT, and unsupervised NMT.\n\nPivot-based Method is a common strategy to obtain a source$\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic.irections, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.\n\n\n## Experiments ::: Main Results ::: Results on Europarl Dataset.\n\nRegarding comparison between the baselines in table TABREF19, we find that pivoting is the strongest baseline that has significant advantage over other two baselines.ersal encoder for the source and pivot languages. Experiments on public datasets show that our approaches significantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations.\n\n\n## Acknowledgments\n\nWe would like to thank the anonymous reviewers for the helpful comments. This work was supported by National Key R&D Program of China (Grant No.']","The pivot-based baselines include pivoting and pivot-synthetic methods. According to the document with the source ID 1912.01214, pivoting is identified as the strongest baseline with a significant advantage over other baselines.",pivoting pivoting$_{\rm m}$,0.0,0.9999999999,0.6666666666666666,0.9022946994377022
which datasets did they experiment with?,"['ersal encoder for the source and pivot languages. Experiments on public datasets show that our approaches significantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations.\n\n\n## Acknowledgments\n\nWe would like to thank the anonymous reviewers for the helpful comments. This work was supported by National Key R&D Program of China (Grant No.luation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.\n\n\n## Experiments ::: Setup ::: Datasets.\n\nThe statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the targetain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.']","According to the document with ID 1912.01214, they conducted experiments on the Europarl and MultiUN corpora.",Europarl MultiUN,1.0,0.9999999999,,0.8318783093788323
what language pairs are explored?,"['tch by mainly using cross-lingual word embedding. Although these methods are successful in the low-resource scene, they have limited effects in zero-shot translation.\n\nMultilingual NMT (MNMT) enables training a single model that supports translation from multiple source languages into multiple target languages, even those unseen language pairs BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21.resource or zero-resource language pairs BIBREF3. Translation between these low-resource languages (e.g., Arabic$\\rightarrow $Spanish) is usually accomplished with pivoting through a rich-resource language (such as English), i.e., Arabic (source) sentence is translated to English (pivot) first which is later translated to Spanish (target) BIBREF4, BIBREF5.n the source and target language representations. Note that although each sentence pair is formed into one sentence, the positions of the target sentence are reset to count form zero.\n\n\n## Approach ::: Bridge Language Model Pretraining\n\nAside from MLM and TLM, we propose BRidge Language Modeling (BRLM) to further obtain word-level representation alignment between different languages.']","Based on the provided documents, particularly Source: 1912.01214, the language pairs explored include Arabic$\rightarrow$Spanish, as mentioned in the context of translation between low-resource languages using a pivot language, such as English.","De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru",0.0,0.0,0.8,0.8865742318384027
what ner models were evaluated?,"['named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial\'s Sequence Tagging with Tensorflow blog post BIBREF15 .ence Tagging with Tensorflow blog post BIBREF15 .\n\n\n## Models\n\nStanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\nspaCy 2.0 uses a CNN-based transition system for named entity recognition.people of Hawaii"") leads to the article Հավայան կղզիներ(Hawaii)).\n\n\n## Conclusion\n\nWe release two named-entity annotated datasets for the Armenian language: a silver-standard corpus for training NER models, and a gold-standard corpus for testing. It is worth to underline the importance of the latter corpus, as we aim it to serve as a benchmark for future named entity recognition systems designed for the Armenian language.']","According to the document 1810.08699, the named entity recognition models that were evaluated are Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14.",Stanford NER spaCy 2.0  recurrent model with a CRF top layer,1.0,0.9999999999,1.0,0.8314060124267989
what is the source of the news sentences?,"['s selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme.assigned the article\'s type they are leading to. Sentences are included in the training corpus only if they contain at least one named entity and all contained capitalized words have an outgoing link to an article of known type. Since in Wikipedia articles only the first mention of each entity is linked, this approach becomes very restrictive and in order to include more sentences, additional links are inferred.nk not targeting a LOC, ORG, or PER article (e.g. ""ԱՄՆ նախագահական ընտրություններում"" (""USA presidential elections"") is linked to the article ԱՄՆ նախագահական ընտրություններ 2016 (United States presidential election, 2016) and as a result [LOC ԱՄՆ] (USA) is lost).\n\n\n## Test dataset\n\nIn order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.']","The source of the news sentences is ilur.am, as indicated in the document 1810.08699.",ilur.am,,0.9999999999,,0.9087127719191138
what are the topics pulled from Reddit?,"[""gmatism influence a conversation on social media? R4: How do other user behaviors (e.g., frequency and breadth of posts) relate to dogmatism?\n\nWe train a predictive model to classify dogmatic posts from Reddit, one of the most popular discussion communities on the web. Posts on Reddit capture discussion and debate across a diverse set of domains and topics – users talk about everything from climate change and abortion, to world news and relationship advice, to the future of artificial intelligence.nguistic features of dogmatism.\n\nData collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model.how it characterizes the site's sub-communities. For example, we might expect to see that subreddits oriented around topics such as abortion or climate change are more dogmatic, and subreddits about cooking are less so.\n\nTo answer this question, we randomly sample 1.6 million posts from the entire Reddit community between 2007 and 2015.""]","The topics pulled from Reddit are diverse, ranging from climate change and abortion to world news, relationship advice, and the future of artificial intelligence. (Source: 1609.00425)","politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.",,0.0,,0.991948365307596
What predictive model do they build?,"['r what makes an argument likely to win BIBREF26 . Our model may allow future researchers to probe these questions more deeply.\n\n\n## Conclusion\n\nWe have constructed the first corpus of social media posts annotated with dogmatism scores, allowing us to explore linguistic features of dogmatism and build a predictive model that analyzes new content.a classifier that predicts dogmatism in comments. A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.\n\nPrediction task. Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies.advice, to the future of artificial intelligence. As a prerequisite to training our model, we have created a corpus of 5,000 Reddit posts annotated with levels of dogmatism, which we are releasing to share with other researchers.\n\nUsing the model, we operationalize key domain-independent aspects of psychological theories of dogmatism drawn from the literature. We find these features have predictive power that largely supports the underlying theory.']","According to the document with Source ID 1609.00425, they build a classifier that predicts dogmatism in comments. This model not only validates their feature sets but also enables scalable analysis of dogmatism in millions of other Reddit comments, as stated in the content associated with Source ID 1609.00425.",logistic regression models,0.0,0.0,0.25,0.8114925405650221
What accuracy does the proposed system achieve?,"['e NER tasks in the dialog and e-commerce domains. The experimental results show that the proposed approach outperforms strong baseline systems.\n\n\n##  Acknowledgments\n\nThis work is supported by the National Natural Science Foundation of China (Grant No. 61572338, 61525205, and 61602160). This work is also partially supported by the joint research project of Alibaba and Soochow University.arning proposed by BIBREF4 nguyen2017aggregating. We use the source code provided by the authors.\n\nThe first three systems are based on the CRF model using traditional handcrafted features, and the last three systems are based on the neural LSTM-CRF model. Among them, CRF-MA, LSTM-Crowd and our system with adversarial learning (ALCrowd) are based on crowd annotation learning that directly trains the model on the crowd-annotations.data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.\n\n\n## Introduction\n\nThere has been significant progress on Named Entity Recognition (NER) in recent years using models based on machine learning algorithms BIBREF0 , BIBREF1 , BIBREF2 . As with other Natural Language Processing (NLP) tasks, building NER systems typically requires a massive amount of labeled training data which are annotated by experts.']","According to the document 1801.05147, the proposed system achieves better scores than strong baseline systems in Chinese NER tasks. However, the exact accuracy percentage is not specified in the provided context.","F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data",0.0,0.0,,0.0
What crowdsourcing platform is used?,"['nd non-scalable, both in terms of time and money. Instead, crowdsourcing is another solution to obtain labeled data at a lower cost but with relative lower quality than those from experts. BIBREF18 snow2008cheap collected labeled results for several NLP tasks from Amazon Mechanical Turk and demonstrated that non-experts annotations were quite useful for training new systems.ot apply the three models on the two EC datasets.\n\n\n## Main Results\n\nIn this section, we show the model performances of our proposed crowdsourcing learning system (ALCrowd), and meanwhile compare it with the other systems mentioned above. Table 2 shows the experimental results on the DL-PS datasets and Table 3 shows the experiment results on the EC-MT and EC-UQ datasets, respectively.ation to model multiple crowdsourcing annotators. We use the source code provided by the authors.\n\nLSTM-CRF: Our baseline system trained on the crowdsourcing labeled data.\n\nLSTM-CRF-VT: Our baseline system trained on the voted corpus, which is the same as CRF-VT.\n\nLSTM-Crowd: The LSTM-CRF model with crowd annotation learning proposed by BIBREF4 nguyen2017aggregating.']","According to the document 1801.05147, Amazon Mechanical Turk is used as a crowdsourcing platform.","They did not use any platform, instead they hired undergraduate students to do the annotation.",0.0,,,0.8747394191919012
How do they match words before reordering them?,"['entences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 .d the reordered sentence Anurag Thakur will meet. Additionally, the Table TABREF5 shows the parse trees for the above sentence with and without pre-ordering.\n\nPre-ordering should also be beneficial for other word order divergence scenarios (e.g., SOV to SVO), but we leave verification of these additional scenarios for future work.\n\n\n## Experimental Setup\n\nIn this section, we describe the languages experimented with, datasets used, the network hyper-parameters used in our experiments.s generated by the encoder for the two languages. In this paper, we propose to pre-order English sentences (assisting language sentences) to match the word-order of the source language and train the parent model on this pre-ordered corpus. In our experiments, we look at scenarios where the assisting language has SVO word order and the source language has SOV word order.\n\nFor instance, consider the English sentence Anurag will meet Thakur.']","The process of matching words before reordering them involves using pre-ordering techniques to rearrange the words of an English sentence to match the word order of the source language, as outlined in the document with the ID 1811.00383. Specifically, sentences are generated by the encoder for both the English and the source languages, after which English sentences are pre-ordered to align with the source language's word order. This pre-ordered English sentence is then used to train the parent model. This method is particularly considered for scenarios where English has a Subject-Object-Verb (SOV) word order and the source language has a Subject-Verb-Object (SVO) word order.",CFILT-preorder system,0.0,0.0,,0.9010437160613501
On how many language pairs do they show that preordering assisting language sentences helps translation quality?,"['lingual NMT in an extremely low-resource setting. We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting. While the current work focused on Indian languages, we would like to validate the hypothesis on a more diverse set of languages.f the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality.\n\n\n## Introduction\n\nDeep Learning approaches have achieved impressive results on various NLP tasks BIBREF0 , BIBREF1 , BIBREF2 and have become the de facto approach for any NLP task.tent leading to decoder generating unknown words. However, the pre-ordered models generate better contextual representations leading to less number of unknown tokens and better translation which is also reflected in the BLEU scores.\n\n\n## Conclusion\n\nIn this paper, we show that handling word-order divergence between source and assisting languages is crucial for the success of multilingual NMT in an extremely low-resource setting.']","According to the content from Source: 1811.00383, experiments on many language pairs show that preordering the assisting language leads to significant improvement in translation quality. However, the exact number of language pairs is not explicitly mentioned.",5,,0.0,1.0,0.0
Which dataset(s) do they experiment with?,"['d the reordered sentence Anurag Thakur will meet. Additionally, the Table TABREF5 shows the parse trees for the above sentence with and without pre-ordering.\n\nPre-ordering should also be beneficial for other word order divergence scenarios (e.g., SOV to SVO), but we leave verification of these additional scenarios for future work.\n\n\n## Experimental Setup\n\nIn this section, we describe the languages experimented with, datasets used, the network hyper-parameters used in our experiments.available may be too small to train a NMT model. Similar to existing works BIBREF3 , BIBREF10 , BIBREF12 , we use transfer learning to overcome data sparsity and train a NMT model between the source and the target languages. Specifically, the NMT model (parent model) is trained on the assisting language and target language pairs. We choose English as the assisting language in all our experiments. In our resource-scarce scenario, we have no parallel corpus for the child task.network hyper-parameters used in our experiments.\n\n\n## Languages\n\nWe experimented with English INLINEFORM0 Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages.']","They experiment with English INLINEFORM0 Hindi translation, Bengali, Gujarati, Marathi, Malayalam, and Tamil datasets. According to the source document 1811.00383, the content mentions these languages and the associated dataset used in the experiments.",IITB English-Hindi parallel corpus BIBREF22 ILCI English-Hindi parallel corpus,0.0,,0.5,0.8577001590429011
Which information about text structure is included in the corpus?,"['and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.es have been shown to be indicative of simple vs. complex texts both theoretically and, using the corpus described in this paper, empirically.\n\nInformation on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification. A set of parallel documents used in machine translation additionally requires sentence alignments, which are still missing from our corpus.er languages, research on German is still scarce. The features exploited as part of machine learning approaches to readability assessment so far typically include surface and/or (deeper) linguistic features. The corpus presented in this paper additionally contains information on text structure, typography, and images. These features have been shown to be indicative of simple vs.']","The corpus includes information on text structure, as stated in the document with the ID 1909.09067.","paragraphs lines Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation",,,0.5,0.8544920483189906
Which information about typography is included in the corpus?,"['and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.er languages, research on German is still scarce. The features exploited as part of machine learning approaches to readability assessment so far typically include surface and/or (deeper) linguistic features. The corpus presented in this paper additionally contains information on text structure, typography, and images. These features have been shown to be indicative of simple vs.Persons with Disabilities (CRPD) in Switzerland. The paper at hand introduces a corpus that represents an enhancement of the corpus of klaper-ebling-volk-2013 in the following ways:\n\nThe corpus contains more parallel data.\n\nThe corpus additionally contains monolingual-only data (simplified German).\n\nThe corpus newly contains information on text structure, typography, and images.']","The corpus includes information on text structure, typography, and images, as stated in the Source: 1909.09067 document.","font type font style Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page",0.0,0.9999999999,0.75,0.8734806988535974
On which benchmarks they achieve the state of the art?,"['n names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.ations (e.g., multiple entities in the question). Finally the highest scored query from the above steps is used to query the KB for answers.\n\nOur main contributions include: (i) An improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks.different from general relation extraction tasks. We propose a novel KB relation detection model, HR-BiLSTM, that performs hierarchical matching between questions and KB relations. Our model outperforms the previous methods on KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. For future work, we will investigate the integration of our HR-BiLSTM into end-to-end systems.']","According to the document 1704.06194, their KBQA system achieves state-of-the-art results on both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",SimpleQuestions WebQSP,1.0,0.9999999999,0.6666666666666666,0.8029501958584548
What they use in their propsoed framework?,"[', by treating the whole relation as a token (i.e. relation id), we could better learn the correspondence between this token and phrases like “tv show” and “play on”.\n\nThe two types of relation representation contain different levels of abstraction. As shown in Table 1 , the word-level focuses more on local information (words and short phrases), and the relation-level focus more on global information (long phrases and skip-grams) but suffer from data sparsity.-1 accuracy), leaving a huge potential (77.5% vs. 58.0%) for the constraint detection module to improve.\n\nFinally, like STAGG, which uses multiple relation detectors (see yih2015semantic for the three models used), we also try to use the top-3 relation detectors from Section ""Relation Detection Results"" .$ s.\n\n(2) Relation as Word Sequence (word-level). In this case, the relation is treated as a sequence of words from the tokenized relation name. It has better generalization, but suffers from the lack of global information from the original relation names. For example in Figure 1 (b), when doing only word-level matching, it is difficult to rank the target relation “starring_roles” higher compared to the incorrect relation “plays_produced”.']","In their proposed framework, they use the top-3 relation detectors as mentioned in the document with the ID 1704.06194. Specifically, they employ a method where the relation is treated as a sequence of words from the tokenized relation name, as detailed in the document with the ID 1704.06194. This method has better generalization but suffers from the lack of global information from the original relation names.",break the relation names into word sequences  relation-level and word-level relation representations bidirectional LSTMs (BiLSTMs)  residual learning method,,,0.6,0.9040076875252829
What does KBQA abbreviate for,"['tions) and multi-relation (WebQSP) QA benchmarks.\n\n\n## Introduction\n\nKnowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB.be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links $n$ -grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a question refers to.\n\nThe main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.This section describes our KBQA pipeline system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build.\n\nFollowing previous work BIBREF4 , BIBREF5 , our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$ , for a question $q$ (“initial entity linking”).']","KBQA stands for Knowledge Base Question Answering, as described in the document with the Source ID 1704.06194.",Knowledge Base Question Answering,1.0,0.9999999999,0.5,0.8719641841181623
What is te core component for KBQA?,"['Title: Improved Neural Relation Detection for Knowledge Base Question Answering\n\nAbstract: Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction.er or comparable results to the state-of-the-art.\n\nThe third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores. Since the re-ranking step relies on the relation detection models, this shows that our HR-BiLSTM model contributes to the good performance in multiple ways. Appendix C gives the detailed performance of the re-ranking step.be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links $n$ -grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a question refers to.\n\nThe main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.']","According to the source document 1704.06194, the core component for Knowledge Base Question Answering (KBQA) is relation detection. This process involves identifying the Knowledge Base (KB) relation(s) a question refers to.",answer questions by obtaining information from KB tuples,,0.0,1.0,0.9139916385253911
What experiments are proposed to test that upper layers produce context-specific embeddings?,"['nal static embeddings such as GloVe and FastText. This may be a means of extracting some use from contextualizing models without incurring the full cost of using them in production.\n\n\n## Conclusion\n\nIn this paper, we investigated how contextual contextualized word representations truly are. For one, we found that upper layers of ELMo, BERT, and GPT-2 produce more context-specific representations than lower layers. This increased context-specificity is always accompanied by increased anisotropy.ntexts have non-identical vector representations. Where vector similarity is defined as cosine similarity, these representations are more dissimilar to each other in upper layers. This suggests that, much like how upper layers of LSTMs produce more task-specific representations BIBREF4, upper layers of contextualizing models produce more context-specific representations.\n\nContext-specificity manifests very differently in ELMo, BERT, and GPT-2.tropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations.']","The experiments proposed to test that upper layers produce context-specific embeddings involve comparing the vector representations of words across different layers of contextualizing models, such as ELMo, BERT, and GPT-2. Specifically, the studies measure cosine similarity to determine the degree of context-specificity. The findings suggest that representations in the upper layers are more dissimilar to each other in context, indicating higher context-specificity (Source: 1909.00512). Additionally, it's noted that while representations of the same word in different contexts still have greater cosine similarity than those of two different words, this self-similarity is significantly lower in the upper layers (Source: 1909.00512).","They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.",0.0,0.9999999999,0.5714285714285714,0.9868938907028619
How do they calculate a static embedding for each word?,"[""ely high anisotropy.\n\n\n## Findings ::: Static vs. Contextualized ::: Principal components of contextualized representations in lower layers outperform GloVe and FastText on many benchmarks.\n\nAs noted earlier, we can create static embeddings for each word by taking the first principal component (PC) of its contextualized representations in a given layer. In Table TABREF34, we plot the performance of these PC static embeddings on several benchmark tasks.replace a word's contextualized representations. The closer $\\textit {MEV}_\\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\\textit {MEV}_\\ell (w) = 1$, then a static embedding would be a perfect replacement for the contextualized representations.\n\n\n## Approach ::: Adjusting for Anisotropy\n\nIt is important to consider isotropy (or the lack thereof) when discussing contextuality.entations can be explained by a static embedding.\n\nRecall from Definition 3 that the maximum explainable variance (MEV) of a word, for a given layer of a given model, is the proportion of variance in its contextualized representations that can be explained by their first principal component. This gives us an upper bound on how well a static embedding could replace a word's contextualized representations.""]","According to the document 1909.00512, static embeddings for each word are created by taking the first principal component (PC) of its contextualized representations in a specific layer. This method is used to approximate the contextualized representations with a static form.",They use the first principal component of a word's contextualized representation in a given layer as its static embedding.,,0.9999999999,,0.9121189803631328
What is the performance of BERT on the task?,"['y, the model is trained using batches of size 64. No more features are included, so the classifier is language-dependent but not domain-dependent.\n\n\n## Materials and Methods ::: Systems ::: BERT\n\nAs introduced earlier, BERT has shown an outstanding performance in NERC-like tasks, improving the start-of-the-art results for almost every dataset and language.EF4, a corpus of real medical reports in Spanish. In these experiments, we compare the performance of BERT with other machine-learning-based systems, some of which use language-specific features. Our aim is to evaluate how good a BERT-based model performs without language nor domain specialisation apart from the training data labelled for the task at hand.e observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall.']","According to the document 2003.03106, BERT achieves a higher recall compared to its competitors, although it may result in more false positive predictions. This implies that while BERT may not be as precise, it is more effective at identifying true positives in the task.","F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967)",0.0,0.0,,0.8810031478927621
What are the other algorithms tested?,"[""panish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.\n\n\n## Introduction\n\nDuring the first two decades of the 21st century, the sharing and processing of vast amounts of data has become pervasive. This expansion of data sharing and processing capabilities is both a blessing and a curse.s below our implementation without the CRF layer.\n\n\n## Materials and Methods\n\nThe aim of this paper is to evaluate BERT's multilingual model and compare it to other established machine-learning algorithms in a specific task: sensitive data detection and classification in Spanish clinical free text. This section describes the data involved in the experiments and the systems evaluated.art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms.""]","The context does not provide specific details about the other algorithms that were tested alongside BERT. Accordingly, I cannot provide this information based on the provided documents.",NER model CRF classifier trained with sklearn-crfsuite classifier has been developed that consists of regular-expressions and dictionary look-up,0.0,0.0,,0.0
What are the clinical datasets used in the paper?,"['art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms.the training data labelled for the task at hand.\n\nThe rest of the paper is structured as follows: the next section describes related work about data anonymisation in general and clinical data anonymisation in particular; it also provides a more detailed explanation and background about the Transformers architecture and BERT. Section SECREF3 describes the data involved in the experiments and the systems evaluated in this paper, including the BERT-based system; finally, it details the experimental design.ved in the experiments and the systems evaluated. Finally, we introduce the experimental setup.\n\n\n## Materials and Methods ::: Data\n\nTwo datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format.']","The clinical datasets used in the paper are plain text containing clinical narrative written in Spanish, as stated in the source document 2003.03106.",MEDDOCAN NUBes-PHI,,0.0,0.6666666666666666,0.8653504355783662
how is model compactness measured?,"[""en with the same grapheme in different languages. These phonemes tend to be phonetically similar to each other.\n\nPerhaps the structure of the phoneme embedding space is what leads to our models' very good performance on WER 100. Even when the model's first predicted pronunciation is not correct, it tends to assign more probability mass to guesses that are more similar to the correct one.ings for languages written in the Latin alphabet. Given a short text in a language, the model predicts the language's orthographic rules. To create phonemic context features from the short text, the model naïvely maps graphemes to IPA symbols written with the same character, and uses the features of these symbols to learn an approximation of the phonotactic constraints of the language.s with ambiguous orthographies is such a problem. Multilingual g2p, in which the various languages have similar but different and possibly contradictory spelling rules, can be seen as an extreme case of that. Therefore, a data-driven sequence-to-sequence model is a natural choice.\n\n\n## Encoder–Decoder Models\n\nIn order to find the best phoneme sequence, we use a neural encoder–decoder model with attention BIBREF9 .""]",I cannot answer this question based on the provided documents.,Using file size on disk,0.0,0.0,1.0,0.0
what was the baseline?,"['s that are only subtly different from each other. However, these statistics have not been widely reported for other g2p systems, so we omit them here.\n\n\n## Baseline\n\nResults on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST. Their results can be divided into two parts:\n\nHigh resource results, computed with wFSTs trained on a combination of Wiktionary pronunciation data and g2p rules extracted from Wikipedia IPA Help pages.ed simply as an element of the grapheme sequence. This is similar to the approach taken by johnson2016google in their zero-shot NMT system. However, their source-side artificial tokens identify the target language, whereas ours identify the source language. An alternative approach, used by ostling2017continuous, would be to concatenate a language embedding to the input at each time step.system outperforms the baseline by a wide margin. The best performance came with the version of our model that was trained on data in all available languages, not just the languages it was tested on. Using a language ID token improves results considerably, but even NoLangID beats the baseline in WER and WER 100.']","The baseline was the system presented by deri2016grapheme, identified as wFST. According to the document 1708.01464, this baseline system was used for comparison in the results.",system presented by deri2016grapheme,1.0,,0.6666666666666666,0.8666343871427085
what evaluation metrics were used?,"['s, it was necessary to compute our error metrics.\n\n\n## Evaluation\n\nWe use the following three evaluation metrics:\n\nPhoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.\n\nWord Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence.exactly match the gold standard phoneme sequence.\n\nWord Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system.\n\nIn system evaluations, WER, WER 100, and PER numbers presented for multiple languages are averaged, weighting each language equally BIBREF13 .\n\nIt would be interesting to compute error metrics that incorporate phoneme similarity, such as those proposed by hixon2011phonemic.system outperforms the baseline by a wide margin. The best performance came with the version of our model that was trained on data in all available languages, not just the languages it was tested on. Using a language ID token improves results considerably, but even NoLangID beats the baseline in WER and WER 100.']","According to the source document 1708.01464, the following three evaluation metrics were used: Phoneme Error Rate (PER), Word Error Rate (WER), and Word Error Rate 100 (WER 100).",Phoneme Error Rate (PER) Word Error Rate (WER) Word Error Rate 100 (WER 100),1.0,,,0.9213417822244949
what datasets did they use?,"[""m for small quantities of data in some languages. jyothilow trained recurrent neural networks for small data sets and found that a version of their system that combined the neural network output with the output of the wFST-based Phonetisaurus system BIBREF1 did better than either system alone.\n\nA different approach came from kim2012universal, who used supervised learning with an undirected graphical model to induce the grapheme–phoneme mappings for languages written in the Latin alphabet.s actually only trained on data in 157 languages. As is noted above, the Adapted set omits 23 languages which are in the High test set.\n\nLangID-All and NoLangID-All: Trained on data in all 311 languages in the Wiktionary training corpus.\n\nIn order to ease comparison to Deri and Knight's system, we limited our use of the training corpus to 10,000 words per language.anguage embedding to the input at each time step. They do not evaluate their approach on grapheme-to-phoneme conversion.\n\n\n## Data\n\nIn order to train a neural g2p system, one needs a large quantity of pronunciation data. A standard dataset for g2p is the Carnegie Mellon Pronouncing Dictionary BIBREF12 . However, that is a monolingual English resource, so it is unsuitable for our multilingual task. Instead, we use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments.""]","They used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments, specifically LangID-All and NoLangID-All, which include data in all 311 languages in the Wiktionary training corpus. This information can be found in the document with the Source ID 1708.01464.",the Carnegie Mellon Pronouncing Dictionary BIBREF12 the multilingual pronunciation corpus collected by deri2016grapheme  ranscriptions extracted from Wiktionary,0.6666666666666666,0.9999999999,,0.8184391705022761
Who were the human evaluators used?,"[""parameters (logistic regression classifier only). While rows 12 and 24 are never the best results, they are close to the best involving tuning. This encourages us to enhance our template model and explore templates not so tailored to our data.\n\nHuman Evaluation Results. While automated evaluation metrics like ROUGE measure lexical similarity between machine and human summaries, humans can better measure how coherent and readable a summary is.t was done in parallel and is very close to ours. However, in addition to the difference in both our and their model, we think it might be infeasible to back generate student reflections from a human summary, especially an abstractive one.\n\n\n## Reflection Summarization Dataset\n\nStudent reflections are comments provided by students in response to a set of instructor prompts. The prompts are directed towards gathering students' feedback on course material.y), and CS (Data Structures in Computer Science). All reflections were collected in response to two pedagogically-motivated prompts BIBREF16: “Point of Interest (POI): Describe what you found most interesting in today's class” and “Muddiest Point (MP): Describe what was confusing or needed more detail.”\n\nFor each reflection document, at least one human (either a TA or domain expert) created summaries. Table TABREF4 shows example reference summary produced by one annotator for the CS course.""]",The human evaluators used were TAs and domain experts. This information is sourced from the document with ID 2002.03407.,20 evaluators were recruited from our institution and asked to each perform 20 annotations,0.0,0.0,,0.9606153272680871
What is the recent abstractive summarization method in this paper?,"['Title: Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis\n\nAbstract: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections.proposed model compared to a synthesis baseline. Lastly, we combine both directions. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.\n\n\n## Related Work\n\nAbstractive Summarization. Abstractive summarization aims to generate coherent summaries with high readability, and has seen increasing interest and improved performance due to the emergence of seq2seq models BIBREF8 and attention mechanisms BIBREF9.re domain transfer for abstractive summarization. While domain transfer is not new, compared to prior summarization studies BIBREF6, BIBREF7, our training (news) and tuning (student reflection) domains are quite dissimilar, and the in-domain data is small. Second, we propose a template-based synthesis method to create synthesized summaries, then explore the effect of enriching training data for abstractive summarization using the proposed model compared to a synthesis baseline.']","The recent abstractive summarization method in this paper is a neural abstractive summarization method. It is evaluated across four student reflection corpora, demonstrating the utility of all three methods (domain transfer, data synthesis, and neural model). (Source: 2002.03407)",pointer networks with coverage mechanism (PG-net),0.0,0.0,1.0,0.9644509978639837
Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?  ,"[""scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models.aries different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary.or the student model learned during distillation. In addition, instead of distilling solely on the teacher model's final-layer outputs, our model leverages layer-wise teacher model parameters to directly optimize the parameters of the corresponding layers in the student model. Specifically, our contributions are:\n\n[leftmargin=*]\n\nDual Training: Our teacher and student models have different vocabularies and incompatible tokenizations for the same sequence.""]","According to the document with the ID 1909.11687, prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models because these techniques do not account for differences in vocabulary between the teacher and student models. The document introduces a novel knowledge distillation technique that specifically addresses this issue by employing a dual-training mechanism to optimize word embeddings for the student vocabulary. This approach allows for the student model to learn with a significantly smaller vocabulary and lower embedding and hidden state dimensions, which prior techniques failed to achieve.","While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.",,,0.625,0.9733099841032923
What state-of-the-art compression techniques were used in the comparison?,"['n a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.\n\n\n## Introduction\n\nRecently, contextual-aware language models such as ELMo BIBREF0, GPT BIBREF1, BERT BIBREF2 and XLNet BIBREF3 have shown to greatly outperform traditional word embedding models including Word2Vec BIBREF4 and GloVe BIBREF5 in a variety of NLP tasks.Title: Extreme Language Model Compression with Optimal Subwords and Shared Projections\n\nAbstract: Pre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices.ion size) compared to the teacher BERTBASE model. We conduct experiments for measuring both generalized language modeling perspective and for downstream tasks, demonstrating competitive performance with high compression ratios for both families of tasks.\n\n\n## Related Work\n\nResearch in neural network model compression has been concomitant with the rise in popularity of neural networks themselves, since these models have often been memory-intensive for the hardware of their time.']","According to the document 1909.11687, the state-of-the-art compression techniques used in the comparison were Optimal Subwords and Shared Projections. These techniques were evaluated for their performance in generalized language modeling and downstream tasks, demonstrating competitive performance with high compression ratios. (Source: 1909.11687)",baseline without knowledge distillation (termed NoKD) Patient Knowledge Distillation (PKD),0.0,,0.0,0.9431035556715756
What is the size of the dataset?,"['that generate image descriptions (e.g. BIBREF2 ). An untested assumption behind the dataset is that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K):\n\n“By asking people to describe the people, objects, scenes and activities that are shown in a picture without giving them any further information about the context in which the picture was taken, we were able to obtain conceptual descriptions that focus only on the information9660), groups of young people as friends (36979).\n\nAnnotators will often guess the status or occupation of people in an image. Sometimes these guesses are relatively general (e.g. college-aged people being called students in image 36979), but other times these are very specific (e.g.ences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.\n\n\n## Introduction\n\nThe Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ).']","The dataset consists of over 30,000 images, as stated in the document with the ID 1605.06083.","30,000",1.0,0.9999999999,0.5,0.8480434979195323
Which methods are considered to find examples of biases and unwarranted inferences??,"['rences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases.\n\n\n## Stereotype-driven descriptions\n\nStereotypes are ideas about how other (groups of) people commonly behave and what they are likely to do. These ideas guide the way we talk about the world.hese ideas guide the way we talk about the world. I distinguish two kinds of verbal behavior that result from stereotypes: (i) linguistic bias, and (ii) unwarranted inferences. The former is discussed in more detail by beukeboom2014mechanisms, who defines linguistic bias as “a systematic asymmetry in word choice as a function of the social category to which the target belongs.” So this bias becomes visible through the distribution of terms used to describe entities in a particular category.Title: Stereotyping and Bias in the Flickr30K Dataset\n\nAbstract: An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they""focus only on the information that can be obtained from the image alone""(Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset.']","The paper presents methods to analyze the data to detect biases and unwarranted inferences, as indicated in the content from the Source: 1605.06083. However, the specific methods are not detailed in the provided text.",spot patterns by just looking at a collection of images tag all descriptions with part-of-speech information I applied Louvain clustering,0.0,,,0.0
What biases are found in the dataset?,"['Title: Stereotyping and Bias in the Flickr30K Dataset\n\nAbstract: An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they""focus only on the information that can be obtained from the image alone""(Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset.icult to automatically detect bias from the data. So how should we deal with stereotype-driven descriptions?\n\n\n## Conclusion\n\nThis paper provided a taxonomy of stereotype-driven descriptions in the Flickr30K dataset. I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators\' choice of words when confronted with an image that may or may not match their stereotypical expectancies.problem is not with the lack of consistency here. Quite the contrary: the problem is that stereotypes may be pervasive enough for the data to be consistently biased. And so language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions.\n\nThis paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices.']","The dataset contains biases such as linguistic bias and unwarranted inferences resulting from stereotypes and prejudices (Source: 1605.06083). Specifically, language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions (Source: 1605.06083).",Ethnic bias,0.0,0.0,,0.9685238795095411
What discourse relations does it work best/worst for?,"['are logically connected (e.g., causal, contrast). Recognizing discourse relations is one crucial step in discourse analysis and can be beneficial for many downstream NLP applications such as information extraction, machine translation and natural language generation.\n\nCommonly, explicit discourse relations were distinguished from implicit ones, depending on whether a discourse connective (e.g., “because” and “after”) appears between two discourse units BIBREF0 .ns as well as global discourse relation patterns.\n\n\n## Dataset and Preprocessing\n\nThe Penn Discourse Treebank (PDTB): We experimented with PDTB v2.0 BIBREF7 which is the largest annotated corpus containing 36k discourse relations in 2,159 Wall Street Journal (WSJ) articles. In this work, we focus on the top-level discourse relation senses which are consist of four major semantic classes: Comparison (Comp), Contingency (Cont), Expansion (Exp) and Temporal (Temp).discourse relations that involve one of the DUs.\n\nIn addition, we introduce two novel designs to further improve discourse relation classification performance of our paragraph-level neural net model. First, previous work has indicated that recognizing explicit and implicit discourse relations requires different strategies, we therefore untie parameters in the discourse relation prediction layer of the neural networks and train two separate classifiers for predicting explicit and implicit discourse relation']","The model performs distinctly for explicit and implicit discourse relations. For explicit discourse relations, it utilizes a separate classifier, as indicated in the source document 1804.05918. The model also recognizes four major semantic classes: Comparison (Comp), Contingency (Cont), Expansion (Exp), and Temporal (Temp). However, the document does not specify which of these classes the model performs best or worst with. Therefore, based on the provided documents, I cannot specify which discourse relations the model works best or worst with.",explicit discourse relations,,0.0,0.8571428571428571,0.0
How much does this model improve state-of-the-art?,"['s when building representation for individual DU. We can see that this model has greatly improved implicit relation classification performance across all the four relations and improved the macro-average F1-score by over 7 percents. In addition, compared with the baseline Bi-LSTM model with tensor, this model improved implicit relation classification performance across the three small classes, with clear performance gains of around 2 and 8 percents on contingency and temporal relations respectively, and oveformance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. Furthermore, the ensemble model achieves the best performance for predicting both implicit and explicit discourse relations simultaneously.gency and temporal relations respectively, and overall improved the macro-average F1-score by 2.2 percents.\n\nThe second row shows the performance of our basic paragraph-level model which predicts both implicit and explicit discourse relations in a paragraph. Compared to the variant system (the first row), the basic model further improved the classification performance on the first three implicit relations.']","According to the document with Source ID 1804.05918, the model improves state-of-the-art performance by 3.02 in macro F1-scores for predicting implicit discourse relations and by 0.96 for explicit discourse relations. Furthermore, the model improves the macro-average F1-score by 2.2 percent for predicting both implicit and explicit discourse relations simultaneously.","the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively.",0.2,,1.0,0.7969522433865261
Where is a question generation model used?,"['wledge base such as Freebase/ProBase/Reverb/NELL. The question answering model gives each candidate answer a score by measuring the semantic relevance between representation and the candidate answer representation in vector space. The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer.and the candidate answer, and outputs a question. We adopt the sequence to sequence architecture as our basic question generation model due to its effectiveness in natural language generation tasks. The hierarchical encoder and the hierarchical attention mechanism are introduced to take into account the structure of the input facts.m - S(q_i,a_i) + S(q_i,\\bar{a})\\rbrace ,$$   (Eq. 15)\n\nFor testing, given a question $q$ , the model predicts the answer based on the following equation, where $ \\mathcal {A}(q)$ is the candidate answer set.\n\n$$\\hat{a} = {\\mbox{argmax}}_{a^{\\prime } \\in \\mathcal {A}(q)} S(q,a^{\\prime })$$   (Eq. 16)\n\n\n## The Question Generation Model\n\nIn this section, we present the generation model which generates a question based on the semantics of a candidate answer.']","A question generation model is used in the process of generating questions based on the semantics of candidate answers, as detailed in the document with the Source ID 1809.04267.",The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer.,1.0,,0.6666666666666666,0.8708677919944509
Which demographic dimensions of people do they obtain?,"[""Title: Stateology: State-Level Interactive Charting of Language, Feelings, and Values\n\nAbstract: People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level.00 Lakes) and Utah (home of the Great Salt Lake). Our demo described in Section SECREF4 , can also be used to generate maps for function words, which can be very telling regarding people's personality BIBREF8 .\n\n\n## Psycholinguistic and Semantic Maps\n\nLIWC. In addition to individual words, we can also create maps for word categories that reflect a certain psycholinguistic or semantic property.ender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.\n\nAnother profile element that can lead to interesting maps is the Industry field BIBREF6 .""]","According to the document with Source ID 1612.06685, the study aims to portray various demographic, linguistic, and psychological dimensions at the state level through the analysis of vocabul0ry used by Blogger users in the U.S.",occupation industry profile information language use gender,0.75,0.9999999999,,0.7829998688087486
How do they obtain psychological dimensions of people?,"[""and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.\n\n\n## Introduction\n\nBlogging gained momentum in 1999 and became especially popular after the launch of freely available, hosted platforms such as blogger.com or livejournal.com.n effectively leverage a prodigious blog dataset. Not only does the dataset bring out the extensive linguistic content reflected in the blog posts, but also includes location information and rich metadata. These data allow for the generation of maps that reflect the demographics of the population, variations in language use, and differences in psycholinguistic and semantic categories. These mappings can be valuable to both psychologists and linguists, as well as lexicographers.Title: Stateology: State-Level Interactive Charting of Language, Feelings, and Values\n\nAbstract: People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level.""]","According to the Source 1612.06685, psychological dimensions of people are obtained through the analysis of vocabulary used by Blogger users in the U.S., aiming to geographically portray various demographic, linguistic, and psychological dimensions.",using the Meaning Extraction Method,0.0,0.0,0.6666666666666666,
What is the baseline?,"[""baselines with other variations described below.\n\nApart from learning-based baselines, we also create two naive baselines, one each for the Dosage and Frequency extraction tasks. For Dosage extraction, the baseline we consider is `Nearest Number', where we take the number nearest to the Medication Name as the prediction, and `none' if no number is mentioned or if the Medication Name is not detected in the input.edications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'.o with the timestamps of its first and last word.\n\nThe transcript of the conversations are annotated with summaries and Medication Regimen tags (MR tags), both grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators, refer to Table TABREF1. The transcript for a typical conversation can be quite long, and not easy for many of the high performing deep learning models to act on.""]","The baseline for the Dosage extraction task is `Nearest Number', as per the document 1912.04961.",QA PGNet Multi-decoder QA PGNet with lookup table embedding,,0.0,,0.8333477628395426
What embeddings are used?,"[""fter a large number of words from the medication.\n\nOther Variations: Considering various models' performance (without pretraining) and the resource constraint, we choose ELMo and BERT embeddings to analyze the effects of pretraining the encoder. When the network's encoder (and embedding) is pretrained with the summarization task, we 1) see a small decrease in the average number of iterations required for training, 2) improvement in individual performances of all models for both the sub-tasks, and 3) get besforms equally well when the tag is `none` or not. In most categories, we see an increase in performance when using pretrained encoders.\n\n\n## Results and Discussion ::: Training Dataset Size\n\nWe vary the number of MR tags used to train the model and analyze the model's performance when training the networks, using publicly available contextual embeddings, compared to using pretrained embeddings and encoder (pretrained on the summarization task).lBERT BIBREF13 (trained and provided by the authors). Refer to Table TABREF5 for the performance comparisons.\n\nWe derive embeddings from ELMo by learning a linear combination of its last three layer's hidden states (task-specific fine-tuning BIBREF11). Similarly, for BERT-based embeddings, we take a linear combination of the hidden states from its last four layers, as this combination performs best without increasing the size of the embeddings BIBREF16.""]","According to the documents, ELMo and BERT-based embeddings are used. Specifically, for BERT-based embeddings, a linear combination of the hidden states from the last four layers is utilized, as this combination performs best without increasing the size of the embeddings (Source: 1912.04961).","simple lookup table embeddings learned from scratch using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13",0.0,0.9999999999,0.8,0.9241396710681156
What datasets did they use for evaluation?,"['ions between two people discussing a given topic. We used same training and test splits as BIBREF3 in which 1374 and 1372 documents are used for training and testing respectively. For validation of our model, we used 10% of training dataset and the remaining 90% was used for actual model training. The number of topics in this data set is 40.\n\n\n## Experiments ::: Dataset Statistics\n\nTable TABREF22 shows statistics of our datasets.ings also helped us to create a balanced dataset. This dataset contains 4331 calls and we split them into 3 sets for our experiments: 2866 calls for training, 362 calls for validation and, finally, 1103 calls for testing.\n\nWe obtained the transcripts by employing an ASR system. The ASR system uses TDNN-LSTM acoustic model trained on Fisher and Switchboard datasets with lattice-free maximum mutual information criterion BIBREF18.does not change much throughout these documents. However, CSAT may vary during the call, and in some cases a naive assumption that the sequential nature of the transcripts is irrelevant may lead to wrong conclusions.\n\nTable TABREF33 compares our results with previous works. It can be seen that our model ToBERT outperforms CNN based experiments by significant margin on CSAT and Fisher datasets.']","The datasets used for evaluation were Fisher and Switchboard datasets, as stated in the document with the ID 1910.10781.",CSAT dataset 20 newsgroups Fisher Phase 1 corpus,0.3333333333333333,0.9999999999,0.0,0.9132625493501024
On top of BERT does the RNN layer work better or the transformer layer?,"[""satisfaction BIBREF6, BIBREF7, BIBREF8, BIBREF9. In most of these works, logistic regression, SVM, CNN are applied on different kinds of representations.\n\nIn BIBREF17, authors use BERT for document classification but the average document length is less than BERT maximum length 512. TransformerXL BIBREF2 is an extension to the Transformer architecture that allows it to better deal with long inputs for the language modelling task.), we refer to them as Hierarchical Transformers. To the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences.\n\nOur novel contributions are:\n\nTwo extensions - RoBERT and ToBERT - to the BERT model, which enable its application in classification of long texts by performing segmentation and using another layer on top of the segment representations.\n\nState-of-the-art results on the Fisher topic classification task.mponent has negligible linear complexity $O(k)$). The positional embeddings are also no longer an issue.\n\n\n## Method ::: Transformer over BERT\n\nGiven that Transformers' edge over recurrent networks is their ability to effectively capture long distance relationships between words in a sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.).""]","According to the document 1910.10781, the Transformer model is experimented with as a replacement for the LSTM recurrent layer in favor of capturing long distance relationships between words in a sequence more effectively. Therefore, the transformer layer is considered to work better in this context. (Source: 1910.10781)",Transformer over BERT (ToBERT),1.0,,0.6666666666666666,0.8435889929341074
How was this data collected?,"['system is in using information gained from users. The dialog system should maximize the number of correctly answered questions without requesting too many explanations and answers from users. To evaluate different systems using the collected data, we propose the following evaluation measure:\n\n$$ \nS_D = \\frac{n_c - w_i n_i - w_e n_e - w_a n_a}{|D|}$$   (Eq.LU)"" ) that were used during the data collection. On top of that, each dialog is labeled by the correct answer for the question and expert annotation of the user answer hint which tells whether the hint points to the correct answer, incorrect answer, or no answer at all.\n\n351 of all collected dialogs contain correct answer provided by users and 702 dialogs have incorrect answer. In the remaining 847 dialogs users did not want to answer the question.rect because human users naturally made mistakes. We intentionally keep these mistakes in the dataset because real systems must address them as well.\n\n\n## Dataset Collection Process\n\nA perfect data collection scenario for our dataset would use real running dialog system providing general information from the knowledge base to real users. This system could then ask for explanations and answers for questions which it is not able to answer.\n\nHowever, getting access to systems with real users is usually hard.']","The data was collected through a process where a running dialog system provides general information from a knowledge base to users, as stated in the document with the ID 1603.09631. The system then asks for explanations and answers for questions it cannot answer. This process is described in the document with the ID 1603.09631.",CrowdFlower,0.0,0.0,0.0,0.9831482152719628
What is the average length of dialog?,"['s the number of simulated dialogs in the dataset. $w_i$ , $w_e$ , $w_a$ are penalization weights.\n\nThe penalization weights are used to compensate for different costs of obtaining different types of information from the user. For example, gaining broader explanation from the user is relatively simple because it is in their favor to cooperate with the system on a question they are interested in.LU)"" ) that were used during the data collection. On top of that, each dialog is labeled by the correct answer for the question and expert annotation of the user answer hint which tells whether the hint points to the correct answer, incorrect answer, or no answer at all.\n\n351 of all collected dialogs contain correct answer provided by users and 702 dialogs have incorrect answer. In the remaining 847 dialogs users did not want to answer the question.system is in using information gained from users. The dialog system should maximize the number of correctly answered questions without requesting too many explanations and answers from users. To evaluate different systems using the collected data, we propose the following evaluation measure:\n\n$$ \nS_D = \\frac{n_c - w_i n_i - w_e n_e - w_a n_a}{|D|}$$   (Eq.']","I'm sorry, but I cannot answer this question based on the provided documents.",4.49 turns,0.0,,0.0,0.0
How are models evaluated in this human-machine communication game?,"['Title: Learning Autocomplete Systems as a Communication Game\n\nAbstract: We study textual autocomplete---the task of predicting a full sentence from a partial sentence---as a human-machine communication game. Specifically, we consider three competing goals for effective communication: use as few tokens as possible (efficiency), transmit sentences faithfully (accuracy), and be learnable to humans (interpretability).oncontiguous) subsequence of the target sentence. Our hypothesis is that such subsequence schemes naturally ensure interpretability, as efficient human and machine communication schemes are both likely to involve keeping important content words.\n\n\n## Approach ::: Modeling with autoencoders.\n\nTo learn communication schemes without supervision, we model the cooperative communication between a user and system through an encoder-decoder framework.ion approach when specifying 53.38% of sentences. Finally, we demonstrate that humans can easily adapt to the keyword-based autocomplete system and save nearly 50% of time compared to typing a full sentence in our user study.\n\n\n## Approach\n\nConsider a communication game in which the goal is for a user to communicate a target sequence $x= (x_1, ..., x_m)$ to a system by passing a sequence of keywords $z= (z_1, ..., z_n)$.']","Models in this human-machine communication game are evaluated based on three criteria: efficiency, accuracy, and interpretability. According to the document 1911.06964, the evaluation of the model is demonstrated through user studies, where it is shown that users can easily adapt to the system, and they save nearly 50% of time compared to typing a full sentence. This suggests that the model's effectiveness is measured by its ability to improve communication efficiency. (Source: 1911.06964)",by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews,0.0,0.0,,0.9365941246770836
How many participants were trying this communication game?,"['Title: Learning Autocomplete Systems as a Communication Game\n\nAbstract: We study textual autocomplete---the task of predicting a full sentence from a partial sentence---as a human-machine communication game. Specifically, we consider three competing goals for effective communication: use as few tokens as possible (efficiency), transmit sentences faithfully (accuracy), and be learnable to humans (interpretability).ion approach when specifying 53.38% of sentences. Finally, we demonstrate that humans can easily adapt to the keyword-based autocomplete system and save nearly 50% of time compared to typing a full sentence in our user study.\n\n\n## Approach\n\nConsider a communication game in which the goal is for a user to communicate a target sequence $x= (x_1, ..., x_m)$ to a system by passing a sequence of keywords $z= (z_1, ..., z_n)$.a user to write sentences by specifying keywords. We empirically show that our framework produces communication schemes that are 52.16% more accurate than rule-based baselines when specifying 77.37% of sentences, and 11.73% more accurate than a naive, weighted optimization approach when specifying 53.38% of sentences.']",The provided documents do not specify the number of participants involved in the communication game study.,100,0.0,0.0,1.0,0.0
What user variations have been tested?,"['and optimizing the efficiency-accuracy tradeoff. Our experiments show that this approach results in an autocomplete system that is 52% more accurate at a given efficiency level compared to baselines, is robust to user variations, and saves time by nearly 50% compared to typing full sentences.x and for space.\n\n\n## Experiments ::: User study.\n\nWe recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. Each user was shown alternating autocomplete and writing tasks across 50 sentences (see Appendix for user interface). For the autocomplete task, we gave users a target sentence and asked them to type a set of keywords into the system.d them to type a set of keywords into the system. The users were shown the top three suggestions from the autocomplete system, and were asked to mark whether each of these three suggestions was semantically equivalent to the target sentence. For the writing task, we gave users a target sentence and asked them to either type the sentence verbatim or a sentence that preserves the meaning of the target sentence.\n\nTable TABREF13 shows two examples of the autocomplete task and actual user-provided keywords.']","User variations such as typing accuracy and completion times were tested. This is based on the document titled ""1911.06964"" which mentions a user study where crowdworkers on Amazon Mechanical Turk were used to measure these aspects across 50 sentences.",completion times and accuracies,,0.0,0.5,0.8484016114331401
What are the baselines used?,"[""pproximate the expectation in Eq (DISPLAY_FORM9). We use a single sample from $q_{\\alpha }(z\\mid x)$ and moving-average of rewards as a baseline to reduce variance.\n\n\n## Experiments\n\nWe evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details).ly more stable as a function of $\\epsilon $ (e.g. points for $\\epsilon $ are more evenly spaced than $\\lambda $).\n\n\n## Experiments ::: Efficiency-accuracy tradeoff.\n\nWe quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\\delta =0$) or half of the time ($\\delta =0.5$).a user to write sentences by specifying keywords. We empirically show that our framework produces communication schemes that are 52.16% more accurate than rule-based baselines when specifying 77.37% of sentences, and 11.73% more accurate than a naive, weighted optimization approach when specifying 53.38% of sentences.""]","The baselines used in the experiments are Unif and Stopword. According to the document with the ID 1911.06964, these are rule-based systems used for comparison.",Unif and Stopword,1.0,0.9999999999,0.5,0.9006733242503633
How does the IPA label data after interacting with users?,"['us on solely mathematical and hence more cognitively demanding questions. Second, by interacting with users, it augments the resources with structured and labeled training data for further possible implementation of learnable dialogue components. The realization of such a system was connected with many challenges. Among others were missing structured data, ambiguous or erroneous user-generated text and the necessity to deal with already existing corporate tools and their design.Title: Multipurpose Intelligent Process Automation via Conversational Assistant\n\nAbstract: Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems.setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data.']","According to the document from 2001.02284, the IPA augments resources with structured and to some extent labeled training data after interacting with users.",It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).,0.0,0.0,0.6666666666666666,0.8243988916584998
What kind of repetitive and time-consuming activities does their assistant handle?,"['Title: Multipurpose Intelligent Process Automation via Conversational Assistant\n\nAbstract: Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems.parameter did not bring significant improvements. To cope with this problem, we implemented a verification step, where a user is given a chance to correct the erroneously retrieved information.\n\nThe overall feedback from the tutors included reduced repetitive activities as well as reduced waiting times for students until their questions were processed.gently – by means of machine learning algorithms. Such IPA-systems undertake time-consuming and routine tasks, and thus enable smart workflows and free up skilled workers to accomplish higher-value activities.\n\nOne of the potential applications of Natural Language Processing (NLP) within the IPA domain are conversational interfaces that enable human-to-machine interaction.']","According to the document 2001.02284, the assistant handles time-consuming and routine tasks such as reducing repetitive activities and waiting times for students. It also enables smart workflows by undertaking these tasks, which allows skilled workers to focus on higher-value activities.","What kind of topic (or sub-topic) a student has a problem with At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now  the exact question number and exact problem formulation",,0.0,0.25,0.8371643090774677
How was the audio data gathered?,"['e N vectors and get D dimensional representation. We then feed this feature to the projection layer followed by softmax for predicting the language label.\n\n\n## DATASET\n\nIn this section, we describe our dataset collection process. We collected and curated around 635Hrs of audio data for 7 Indian languages, namely Kannada, Hindi, Telugu, Malayalam, Bengali, and English. We collected the data from the All India Radio news channel where an actor will be reading news for about 5-10 mins.n actor will be reading news for about 5-10 mins. To cover many speakers for the dataset, we crawled data from 2010 to 2019. Since the audio is very long to train any deep neural network directly, we segment the audio clips into smaller chunks using Voice activity detector.identification performance for Indian languages. We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages. Our experimental results shows that our approach outperforms the previous state of the art methods by an absolute 1.88% F1-score.']","The audio data was gathered by collecting and curating around 635 hours of data from the All India Radio news channel for 7 Indian languages. The process involved actors reading news for about 5-10 minutes, with data collected from 2010 to 2019. To manage the length of the audio clips for training deep neural networks, the audio was segmented into smaller chunks using a Voice activity detector. This information is sourced from Document ID: 2002.01664.",Through the All India Radio new channel where actors read news.,1.0,0.9999999999,0.8333333333333334,0.9167917625285167
What is the GhostVLAD approach?,"['of the art methods by an absolute 1.88% F1-score. We have also conducted experiments with different pooling strategies proposed in the past, and the GhostVLAD pooling approach turns out to be the best approach for aggregating frame-level features into a single utterance level feature. Our experiments also prove that our approach works much better even if the input during training contains smaller durations.hieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features.Title: Identification of Indian Languages using Ghost-VLAD pooling\n\nAbstract: In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time.']","The GhostVLAD approach is a pooling strategy used for language identification. It generates an utterance level feature vector for variable length input audio by aggregating local frame level features across time (Source: 2002.01664). The method is shown to be effective for robust language recognition, especially in the context of Indian languages (Source: 2002.01664). It also creates embedd0ings with good language discriminative features (Source: 2002.01664).",extension of the NetVLAD adds Ghost clusters along with the NetVLAD clusters,,,,0.9327382246121703
Which 7 Indian languages do they experiment with?,"['ating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data.identification performance for Indian languages. We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages. Our experimental results shows that our approach outperforms the previous state of the art methods by an absolute 1.88% F1-score.est pooling strategy for language identification. Our model obtains the best accuracy of 98.24%, and it outperforms all the other previously proposed pooling methods. We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel. The paper is organized as follows. In section 2, we explain the proposed pooling method for language identification. In section 3, we explain our dataset.']","According to the documents 2002.01664 and 11, the experiment is conducted on 7 Indian languages. However, the specific names of these languages are not mentioned in the provided texts.","Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam",0.0,0.0,0.5,0.0
What datasets do they evaluate on?,"['plied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) BIBREF35 and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distribution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM BIBREF40 on unsupervised POS tags induced from our Markov-structured model described in sec:pos.n our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction.FORM3 , where INLINEFORM4 is the input dimension. We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy BIBREF23 and V-Measure (VM) BIBREF24 . Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs.']","According to the content in Source: 1808.09111, they evaluate on the Penn Treebank and also perform part-of-speech (POS) induction.

Source: 1808.09111
Content: n our model so long as the prior is well-beed. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction.

Source: 1808.09111
Content: FORM3 , where INLINEFORM4 is the input dimension. We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy BIBREF23 and V-Measure (VM) BIBREF24 . Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs.

In these documents, the datasets mentioned for evaluation are the Penn Treebank and part-of-speech (POS) induction.",Wall Street Journal (WSJ) portion of the Penn Treebank,,0.0,0.6666666666666666,0.8228760807378342
What is the invertibility condition?,"['ve not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved.t with discrete latent variables ( INLINEFORM2 ). Under the invertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by BIBREF16 for independent components estimation., for an arbitrary projection INLINEFORM2 in Eq. ( EQREF17 ), which means inference and learning may be difficult. In sec:opt, we address this issue by constraining INLINEFORM3 to be invertible, and show that this constraint enables tractable exact inference and marginal likelihood computation.\n\n\n## Learning & Inference\n\nIn this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge.']","According to the document with the ID 1808.09111, the invertibility condition is introduced to address the optimization challenge in the learning process. This condition is specifically applied to constrain INLINEFORM3, enabling tractable exact inference and marginal likelihood computation.",The neural projector must be invertible.,1.0,,0.8,0.8105723189080011
Which neural architecture do they use as a base for their attention conflict mechanisms?,"[""ed attention first in neural machine translation. It used a feed-forward network over addition of encoder and decoder states to compute alignment score. Our work is very similar to this except we use element wise difference instead of addition to build our conflict function. BIBREF3 came up with a scaled dot-product attention in their Transformer model which is fast and memory-efficient. Due to the scaling factor, it didn't have the issue of gradients zeroing out.always a match to be found inside the sequences. We provide a theoretical limitation to it and propose another technique called conflict that looks for contrasting relationship between words in two sequences. We empirically verify that our proposed conflict mechanism combined with attention can outperform the performance of attention working solely.\n\n\n## Related Work\n\nBahdanau et al. BIBREF2 introduced attention first in neural machine translation.ned model correctly marked them as non-duplicate.\n\n\n## Conclusion\n\nIn this work, we highlighted the limits of attention especially in cases where two sequences have a contradicting relationship based on the task it performs. To alleviate this problem and further improve the performance, we propose a conflict mechanism that tries to capture how two sequences repel each other.""]","According to the document 1906.08593, they use the Transformer model as a base for their attention conflict mechanisms. This model is specifically noted for its scaled dot-product attention, which is recognized for being fast and memory-efficient.","GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.",,0.0,,0.8733033566463893
On which tasks do they test their conflict method?,"['ts and report the results in Table 1 and Table 2. We observe that model with both attention and conflict combined gives the best results.\n\nWe also show the training loss curve for both the models having attention and attention combined with conflict respectively. Figure 4 and 5 shows these curves for Task 1 and Task 2 respectively. The curves are smoothed using moving average having an window size of 8.there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.es to capture how two sequences repel each other. This acts like the inverse of attention and, empirically, we show that how conflict and attention together can improve the performance.\n\nFuture research work should be based on alternative design of conflict mechanism using other difference operators other than element wise difference which we use.']","They test their conflict method on Task 1 and Task 2, as shown in Figure 4 and 5. (Source: 1906.08593)",Task 1: Quora Duplicate Question Pair Detection Task 2: Ranking questions,0.0,,0.0,0.9603395465336341
What are the sources of the datasets?,"[""m a (potentially infinite) real-word data stream. The datasets typically used in the literature (TDT, Reuters) have a small number of clusters ( INLINEFORM0 20) with coarse topics (economy, society, etc.), and therefore are not relevant to the use case of media monitoring we treat - as it requires much more fine-grained story clusters about particular events. To evaluate our approach, we adapted a dataset constructed for the different purpose of binary classification of joining cluster pairs.M2 on the development set, yielding INLINEFORM3 . To compute IDF scores (which are global numbers computed across a corpus), we used a different and much larger dataset that we collected from Deutsche Welle's news website (http://www.dw.com/). The dataset consists of 77,268, 118,045 and 134,243 documents for Spanish, English and German, respectively., using real news datasets in multiple languages. Our method is simple to implement, computationally efficient and produces state-of-the-art results on datasets in German, English and Spanish.\n\n\n## Introduction\n\nFollowing developing news stories is imperative to making real-time decisions on important political and public safety matters. Given the abundance of media providers and languages, this endeavor is an extremely difficult task.""]","The datasets were collected from Deutsche Welle's news website, as mentioned in Source: 1809.00540.",rupnik2016news,,0.0,,0.9338189215607873
What metric is used for evaluation?,"[""e, has a very relevant contribution in all cases. Although the results for both the baseline and our models seem to differ across languages, one can verify a consistent improvement from the latter to the former, suggesting that the score differences should be mostly tied to the different difficulty found across the datasets for each language.ing been validated with qualitative user testing.\n\n\n## Acknowledgments\n\nWe would like to thank Esma Balkır, Nikos Papasarantopoulos, Afonso Mendes, Shashi Narayan and the anonymous reviewers for their feedback. This project was supported by the European H2020 project SUMMA, grant agreement 688139 (see http://www.summa-project.eu) and by a grant from Bloomberg.M2 on the development set, yielding INLINEFORM3 . To compute IDF scores (which are global numbers computed across a corpus), we used a different and much larger dataset that we collected from Deutsche Welle's news website (http://www.dw.com/). The dataset consists of 77,268, 118,045 and 134,243 documents for Spanish, English and German, respectively.""]","The metric used for evaluation is the IDF scores, which are computed across a corpus as mentioned in the document with the Source ID 1809.00540.","F1, precision, recall, accuracy",0.0,0.0,0.3333333333333333,0.9112147931303184
Which eight NER tasks did they evaluate on?,"[""nput space of a general-domain PTLM (here: BERT). We evaluate on eight biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model (Lee et al., 2020). We cover over 50% of the BioBERT-BERT F1 delta, at 5% of BioBERT's CO_2 footprint and 2% of its cloud compute cost.\n\n\n## Introduction\n\nPretrained Language Models (PTLMs) such as BERT BIBREF1 have spearheaded advances on many NLP tasks.= 768$ on PubMed+PMC (see Appendix for details). Then, we follow the procedure described in Section SECREF3 to update the wordpiece embedding layer and tokenizer of general-domain BERT.\n\n\n## Experiment 1: Biomedical NER ::: Finetuning\n\nWe finetune GreenBioBERT on the eight publicly available NER tasks used in BIBREF2. We also do reproduction experiments with general-domain BERT and BioBERTv1.0, using the same setup as our model. We average results over eight random seeds.and/or CORD-19 (Covid-19 Open Research Dataset). We improve over general-domain BERT on eight out of eight biomedical NER tasks, using a fraction of the compute cost associated with BioBERT. In Section SECREF5, we show how to quickly adapt an existing Question Answering model to text about the Covid-19 pandemic, without any target-domain Language Model pretraining or finetuning.""]","The provided documents do not specify the eight Named Entity Recognition (NER) tasks that were evaluated. Therefore, I cannot answer this question based on the provided documents.","BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800",,0.0,0.5,
What in-domain text did they use?,"['Title: Inexpensive Domain Adaptation of Pretrained Language Models: A Case Study on Biomedical Named Entity Recognition\n\nAbstract: Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by pretraining on in-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO_2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on in-domain text and align the resulting word vectors with the input space of a general-domain PTLM (here: BERT).d by pretraining on unlabeled target-domain text. Some examples of such models are BioBERT BIBREF2, which was pretrained on the PubMed and/or PubMed Central (PMC) corpora, SciBERT BIBREF5, which was pretrained on papers from SemanticScholar, ClinicalBERT BIBREF6, BIBREF7 and ClinicalXLNet BIBREF8, which were pretrained on clinical patient notes, and AdaptaBERT BIBREF9, which was pretrained on Early Modern English text.REF1 have spearheaded advances on many NLP tasks. Usually, PTLMs are pretrained on unlabeled general-domain and/or mixed-domain text, such as Wikipedia, digital books or the Common Crawl corpus.\n\nWhen applying PTLMs to specific domains, it can be useful to domain-adapt them. Domain adaptation of PTLMs has typically been achieved by pretraining on target-domain text. One such model is BioBERT BIBREF2, which was initialized from general-domain BERT and then pretrained on biomedical scientific publications.']",They used in-domain text such as biomedical scientific publications for pretraining BioBERT BIBREF2. (Source: 2004.03354),PubMed+PMC,1.0,0.9999999999,1.0,0.8403503093743193
Which languages do they test on for the under-resourced scenario?,"['translations and target sentences while training. In translation, the framework produces INLINEFORM0 -best candidates and we then use a beam search with the beam size of 12 to get the best translation.\n\n\n## Under-resourced Translation\n\nFirst, we consider the translation for an under-resourced pair of languages. Here a small portion of the large parallel corpus for English-German is used as a simulation for the scenario where we do not have much parallel data: Translating texts in English to German.r and one decoder for all the languages involved. Moreover, it is not necessary to do any network modification to enable attention mechanism in our NMT systems. We then apply our proprosed framework in two demanding scenarios: under-resourced translation and zero-resourced translation. The results show that bringing multilinguality to NMT helps to improve individual translations.to make use of large monolingual corpora in NMT.\n\n\n## Evaluation\n\nIn this section, we describe the evaluation of our proposed approach in comparisons with the strong baselines using NMT in two scenarios: the translation of an under-resource language pair and the translation of a language pair that does not exist any paralled data at all.\n\n\n## Experimental Settings\n\nTraining Data.']","According to the document with the ID 1611.04798, they test on English-German language pair for the under-resourced scenario.",English German,1.0,0.9999999999,0.0,0.9148505452464422
Do they focus on Reading Comprehension or multiple choice question answering?,"['defined as a worldwide increase in temperature). In the multiple-choice setting, which is the variety of question-answering (QA) that we focus on in this paper, there is also pragmatic reasoning involved in selecting optimal answer choices (e.g., while greenhouse effect might in some other context be a reasonable answer to the second question in Figure FIGREF1, global warming is a preferable candidate).ow box) from a target set of knowledge resources. Each of our probing datasets consists of multiple-choice questions that include a question $\\textbf {q}$ and a set of answer choices or candidates $\\lbrace a_{1},...a_{N}\\rbrace $. This section describes in detail the 5 different datasets we build, which are drawn from two sources of expert knowledge, namely WordNet BIBREF35 and the GNU Collaborative International Dictionary of English (GCIDE).inoculation by fine-tuning strategy of BIBREF23. In contrast, we focus on probing open-domain MCQA models (see BIBREF24 for a related study in the reading comprehension setting) as well as constructing data from much larger sources of structured knowledge.\n\nOur main study focuses on probing the BERT model and fine-tuning approach of BIBREF7, and other variants thereof, which are all based on the transformer architecture of BIBREF25.']","According to the document 1912.13337, the focus is on multiple choice question answering (MCQA). Specifically, it mentions the construction of probing datasets for MCQA and the use of BERT models for probing in this context.",MULTIPLE CHOICE QUESTION ANSWERING,1.0,0.9999999999,1.0,0.7635952560814445
After how many hops does accuracy decrease?,"['nteresting nuances to the overall positive trend. For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops. Further, the accuracy of even our best models on the WordNetQA probe drops by 14-44% under our cluster-based analysis, which assesses whether a model knows several facts about each individual concept, rather than just being good at answering isolated questions.ber of hops $k$ increases (see red dashed boxes). For example, problems that involve hyponym reasoning with sister distractors of distance $k^{\\prime }=1$ (i.e., the second column) degrades from 47% to 15% when the number of hops $k$ increases from 1 to 4. This general tendency persists even after additional fine-tuning, as we discuss next, and gives evidence that models are limited in their capacity for certain types of multi-hop inferences.pacity for certain types of multi-hop inferences.\n\nAs discussed by BIBREF26, the choice of generation templates can have a significant effect on model performance. The results so far should therefore be regarded as a lower bound on model competence. It is possible that model performance is high for definitions, for example, because the associated templates best align with the science training distribution (which we know little about).']","According to the information provided in Source: 1912.13337, the accuracy decreases after increasing the number of hops from 1 to 4. Specifically, problems that involve hyponym reasoning with sister distractors of distance $k^{\prime}=1$ degrades from 47% to 15% when the number of hops $k$ increases from 1 to 4.",1-hop links to 2-hops,1.0,0.9999999999,,0.9106894570037776
How do they control for annotation artificats?,"['asoning and are widespread in benchmark datasets. As an alternative to expensive crowd-sourcing, we introduce a methodology for automatically building datasets from various types of expert knowledge (e.g., knowledge graphs and lexical taxonomies), allowing for systematic control over the resulting probes and for a more comprehensive evaluation. We find automatically constructing probes to be vulnerable to annotation artifacts, which we carefully control for.stion-answer pairs (see also BIBREF33, BIBREF34). In contrast to this work, we focus on generating data in an entirely automatic fashion, which obviates the need for expensive annotation and gives us the flexibility to construct much larger datasets that control a rich set of semantic aspects of the target questions.\n\n\n## Dataset Probes and Construction\n\nOur probing methodology starts by constructing challenge datasets (Figure FIGREF1, yellow box) from a target set of knowledge resources.ive results should be taken with a grain of salt. Developing better ways to evaluate semantic clusters and model robustness would be a step in this direction.\n\nWe emphasize that using synthetic versus naturalistic QA data comes with important trade-offs. While we are able to generate large amounts of systematically controlled data at virtually no cost or need for manual annotation, it is much harder to validate the quality of such data at such a scale and such varying levels of complexity.']","According to the document with Source ID 1912.13337, the methodology introduced controls for annotation artifacts carefully. This is part of their approach to automatically construct datasets, which allows systematic control over the resulting probes and contributes to a more comprehensive evaluation.",we use several of the MCQA baseline models first introduced in BIBREF0,,,1.0,0.8008295589524139
How do they perform multilingual training?,"['iliary-task) decoder are shared across languages.\n\nSince a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch.nating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.\n\nAfter 20 epochs of multilingual training, we perform 5 epochs of monolingual finetuning for each language. For this phase, we reduce the learning rate to a tenth of the original learning rate, i.e.ponent that is actually shared between languages. This observation illustrates the two-fold effect of multi-task training: an auxiliary task can either inform the main task through the parameters the two tasks share, or it can help the main task learning through its regularising effect.']","According to the document 1809.01541, multilingual training is performed by randomly alternating between languages for every new minibatch. They do not pass any information to the auxiliary decoder about the source language of the signal it is receiving. After 20 epochs of multilingual training, they perform 5 epochs of monolingual finetuning for each language. The learning rate is reduced to a tenth of the original learning rate during the finetuning phase.",Multilingual training is performed by randomly alternating between languages for every new minibatch,1.0,0.9999999999,0.75,0.9130343027213494
What languages are evaluated?,"['ual fashion, over sets of two to three languages.\n\nIn analysing the performance of our system, we found that encoding the full context improves performance considerably for all languages: 11.15 percentage points on average, although it also highly increases the variance in results.ir common language family and similar morphology. The other natural pairings, however, fr–es, and de–sv, are not so frequent among the best models for these pairs of languages.\n\nFinally, monolingual finetuning improves accuracy across the board, as one would expect, by 2.72% on average.ng 7.96% higher than monolingual ones on average.\n\nWe studied the five best models for each language as emerging from the multilingual training (listed in Table TABREF27 ) and found no strong linguistic patterns. The en–sv pairing seems to yield good models for these languages, which could be explained in terms of their common language family and similar morphology.']","The languages evaluated are not explicitly mentioned in the provided documents. However, the context suggests that multiple languages are being considered, including but not limited to en-sv pairing due to their common language family and similar morphology. The documents also mention monolingual finetuning and the best models for each language, indicating a study involving various languages. (Source: 1809.01541)","German, English, Spanish, Finnish, French, Russian,  Swedish.",0.0,0.0,0.8571428571428571,0.0
What architecture does the decoder have?,"['e lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the""inflection in context""task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.iliary-task) decoder are shared across languages.\n\nSince a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch.e and contrast our system to the baseline system. A diagram of our system is shown in Figure FIGREF4 .\n\nThe idea behind this modification is to provide the encoder with access to all morpho-syntactic cues present in the sentence. In contrast to the baseline, which only encodes the immediately adjacent context of a target word, we encode the entire context.']","The decoder architecture is shared across languages, as stated in the document 1809.01541.",LSTM,0.0,0.0,,0.8866857497766535
What architecture does the encoder have?,"['e lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the""inflection in context""task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.systems submitted, an overall accuracy of 49.87. We showed in an ablation study that this is due to three core innovations, which extend a character-based encoder-decoder model: (1) a wide context window, encoding the entire available context; (2) multi-task learning with the auxiliary task of MSD prediction, which acts as a regulariser; (3) a multilingual approach, exploiting information across languages.ers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder.']","The encoder uses an LSTM (Long Short-Term Memory) architecture, as stated in the document with the ID 1809.01541 (ChunkID: 11).",LSTM,,,0.5,0.9069233978140755
What is MSD prediction?,"['ure context by encoding it with a backwards LSTM.\n\nWe introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in UID15 . Tags are generated with an LSTM one component at a time, e.g.is all it needs to do in a large number of cases. This issue could be alleviated with simple data augmentation techniques that encourage autoencoding BIBREF2 .\n\n\n## MSD prediction\n\nFigure FIGREF32 summarises the average MSD-prediction accuracy for the multi-tasking experiments discussed above. Accuracy here is generally higher than on the main task, with the multilingual finetuned setup for Spanish and the monolingual setup for French scoring best: 66.59% and 65.35%, respectively.erated with an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, INLINEFORM0 PRO, NOM, SG, 1 INLINEFORM1 .\n\nFor every training instance, we backpropagate the sum of the main loss and the auxiliary loss without any weighting.\n\nAs MSD tags are only available in Track 1, this augmentation only applies to this track.\n\nThe parameters of the entire MSD (auxiliary-task) decoder are shared across languages.']","MSD prediction involves predicting the morpho-syntactic tag (MSD) of a target form, conditioned on the context encoding. The accuracy of MSD-prediction varies, with the best scores being 66.59% for Spanish and 65.35% for French in a multi-tasking experiment setup. The MSD tags are predicted as a sequence of components using an LSTM one at a time, as illustrated with the example tag PRO;NOM;SG;1 being predicted as a sequence of four components: INLINEFORM0 PRO, NOM, SG, INLINEFORM1. The parameters of the MSD (auxiliary-task) decoder are shared across languages. (Source: 1809.01541)","The task of predicting MSD tags: V, PST, V.PCTP, PASS.",0.0,,0.875,0.9553986313666982
What other models do they compare to?,"['orts comparison results in literature published . Our model achieves state-of-the-art on development dataset in setting without pre-trained large language model (ELMo). Comparing with the much complicated model R.M.-Reader + Verifier, which includes several components, our model still outperforms by 0.7 in terms of F1 score. Furthermore, we observe that ELMo gives a great boosting on the performance, e.g., 2.8 points in terms of F1 for DocQA.pair of a passage and a question is unanswerable. The two models share the same lower layer to save the number of parameters, and separate the top layers for different tasks (the span detector and binary classifier).\n\nOur model is pretty simple and intuitive, yet efficient.ion of SAN BIBREF1 , as shown in Figure FIGREF2 . The main difference is the additional binary classifier added in the model justifying whether the question is unanswerable. Roughly, the model includes two different layers: the shared layer and task specific layer. The shared layer is almost identical to the lower layers of SAN, which has a lexicon encoding layer, a contextual layer and a memory generation layer. On top of it, there are different answer modules for different tasks.']","According to the document with the ID 1809.09194, the model is compared with the R.M.-Reader + Verifier model.","SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo",0.0,,0.0,0.7996804559623456
What is the architecture of the span detector?,"['urn answer module for the span detector BIBREF1 . Formally, at time step INLINEFORM0 in the range of INLINEFORM1 , the state is defined by INLINEFORM2 . The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5 , where INLINEFORM6 . Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11 .ssage representation is computed as INLINEFORM5 . After that, we use the method of BIBREF13 to apply self attention to the passage: INLINEFORM6\n\nwhere INLINEFORM0 means that we only drop diagonal elements on the similarity matrix (i.e., attention with itself). At last, INLINEFORM1 and INLINEFORM2 are concatenated and are passed through a BiLSTM to form the final memory: INLINEFORM3 .\n\nSpan detector. We adopt a multi-turn answer module for the span detector BIBREF1 .are different answer modules for different tasks. We employ the SAN answer module for the span detector and a one-layer feed forward neural network for the binary classification task. It can also be viewed as a multi-task learning BIBREF8 , BIBREF5 , BIBREF9 . We will briefly describe the model from ground up as follows. Detailed descriptions can be found in BIBREF1 .\n\nLexicon Encoding Layer.']","The architecture of the span detector involves a multi-turn answer module, lexicon encoding layer, and a BiLSTM layer. The multi-turn answer module is employed for the span detector, and a one-layer feed forward neural network is used for the binary classification task. The final memory is formed by concatenating INLINEFORM1 and INLINEFORM2 and passing it through a BiLSTM, as stated in BIBREF1. Additionally, a self-attention mechanism is applied to the passage using the method of BIBREF13, with attention elements being dropped on the similarity matrix to avoid self-attention. (Source: 1809.09194)",adopt a multi-turn answer module for the span detector BIBREF1,1.0,0.9999999999,,0.9999999999999982
What evaluation metric do they use?,"[""ons' (matches) was used as an evaluation measure. It came out that regularization only worsened the results for both algorithms, so in the Table 1 we report the results without regularization.\n\nFor reference, we also report the accuracy of `quazi-translation' via Damerau-Levenshtein edit distance BIBREF9 , as a sort of a baseline.ation $\\lambda $ from 0 to 5, with a step of 0.5. The resulting matrices were applied to the Ukrainian vectors from the test set and the corresponding Russian `translations' were calculated. The ratio of correct `translations' (matches) was used as an evaluation measure.y outperforms this approach even on the test set. On the training set, its superiority is even more obvious.""]","They use the ratio of correct translations, also known as matches, as an evaluation measure. This information is sourced from document 1604.05372.",Accuracy,0.0,0.9999999999,0.6666666666666666,0.8252464452371419
What are the results from these proposed strategies?,"['such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies.ts the agent to the state with the highest score. The agent then begins training a new policy from that particular state.\n\nSimply freezing the policy that led to the bottleneck, however, can potentially result in a policy one that is globally sub-optimal. We therefore employ a backtracking strategy that restarts exploration from each of the $n$ previous steps—searching for a more optimal policy that reaches that bottleneck.illed in the blanks in the template action-space. They test their approach on Zork1, showing the combination of the knowledge graph and template action selection resulted in improvements over existing methods. They note that their approach reaches a score of 40 which corresponds to a bottleneck in Zork1 where the player is eaten by a “grue” (resulting in negative reward) if the player has not first lit a lamp.']","According to the document 2002.08795, the proposed strategies resulted in improvements over existing methods, particularly in reaching bottleneck states in the game Zork1. The strategies achieved a score of 40, which corresponds to a bottleneck situation where the player is eaten by a ""grue"" if they haven't lit a lamp first. This suggests that these strategies can help an agent navigate through difficult situations more effectively in the game.","Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.",,0.0,0.4,0.8515561275316688
What are the baselines?,"['ethod far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.\n\nThere are a couple of key insights that can be drawn from these results The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability.roduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.\n\n\n## Introduction and Background\n\nMany reinforcement learning algorithms are designed for relatively small discrete or continuous action spaces and so have trouble scaling.e graph as well as a new game state for the cell. After expanding a cell, Go-Explore will continue to sample cells by weight to continue expanding its known states. At the same time, KG-A2C will benefit from the heuristics of selecting preferred cells and be trained on promising states more often.\n\n\n## Evaluation\n\nWe compare our two exploration strategies to the following baselines and ablations:\n\nKG-A2C This is the exact same method presented in BIBREF6 with no modifications.']","According to the document 2002.08795, the baselines for comparison are KG-A2C and Go-Explore.",a score of 40,,0.0,0.5,
What are the two new strategies?,"['roduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.\n\n\n## Introduction and Background\n\nMany reinforcement learning algorithms are designed for relatively small discrete or continuous action spaces and so have trouble scaling.such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies.e graph as well as a new game state for the cell. After expanding a cell, Go-Explore will continue to sample cells by weight to continue expanding its known states. At the same time, KG-A2C will benefit from the heuristics of selecting preferred cells and be trained on promising states more often.\n\n\n## Evaluation\n\nWe compare our two exploration strategies to the following baselines and ablations:\n\nKG-A2C This is the exact same method presented in BIBREF6 with no modifications.']","The two new game state exploration strategies introduced are Go-Explore and KG-A2C. According to the document 2002.08795, Go-Explore expands a cell and continues to sample cells by weight to expand its known states. On the other hand, KG-A2C benefits from the heuristics of selecting preferred cells and is trained on promising states more often.",a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space,0.0,0.9999999999,,0.877704055641349
How much better than the baseline is LiLi?,"['n types, LiLi outperforms Sep with INLINEFORM12 . If we set INLINEFORM13 (training with very few clues), LiLi outperforms Sep with INLINEFORM14 on Freebase considering MCC. Thus, the lifelong learning mechanism is effective in transferring helpful knowledge. Single model performs better than Sep for unknown relations due to the sharing of knowledge (weights) across tasks.tisfies INLINEFORM10 ) compared to INLINEFORM11 . In summary, LiLi’s RL-model can effectively formulate query-specific inference strategies (based on specified parameter values). Evaluation-II: Predictive Performance. Table 6 shows the comparative performance of LiLi with baselines. To judge the overall improvements, we performed paired t-test considering +ve F1 scores on each relation as paired data. Considering both KBs and all relation types, LiLi outperforms Sep with INLINEFORM12 .instances overlap with the +ve ones.\n\nBaselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.\n\nSingle: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations.\n\nSep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL.\n\nF-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 .']","LiLi outperforms the baseline, specifically Sep, with INLINEFORM12, as indicated in the Source document 1802.06024. The exact performance improvement is not quantified in the provided context. However, a paired t-test was performed considering +ve F1 scores on each relation, suggesting a statistically significant improvement. For a precise measure, one would need to refer to the specific results in the Source document 1802.06024.","In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2.",,0.9999999999,0.5714285714285714,0.8797787664022514
What baseline is used in the experiments?,"['instances overlap with the +ve ones.\n\nBaselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.\n\nSingle: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations.\n\nSep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL.\n\nF-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 .d inference is made as a result of INLINEFORM12 . Thus, the formulated inference strategy is: “ INLINEFORM13 "".\n\n\n## Experiments\n\nWe now evaluate LiLi in terms of its predictive performance and strategy formulation abilities.\n\nData: We use two standard datasets (see Table 4): (1) Freebase FB15k, and (2) WordNet INLINEFORM0 . Using each dataset, we build a fairly large graph and use it as the original KB ( INLINEFORM1 ) for evaluation.) is regarded as a +ve instance for INLINEFORM6 . Following BIBREF18 , for each +ve instance ( INLINEFORM7 , INLINEFORM8 ), we generate two negative ones, one by randomly corrupting the source INLINEFORM9 , and the other by corrupting the target INLINEFORM10 . Note that, the test triples are not in INLINEFORM11 or INLINEFORM12 and none of the -ve instances overlap with the +ve ones.\n\nBaselines.']","The baseline used in the experiments is various versions of LiLi, specifically INLINEFORM0 for all test relations. (Source: 1802.06024)",versions of LiLi,1.0,0.9999999999,,0.9931662606516328
In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?,"['active learning and inference (LiLi) to solve it. LiLi works by imitating how humans acquire knowledge and perform inference during an interactive conversation. Our experimental results show LiLi is highly promising.\n\n\n## Introduction\n\nChatbots such as dialog and question-answering systems have a long history in AI and natural language processing.ion. Thus, in this paper, we focus on solving it. We assume that other tasks such as fact/relation extraction and resolution and guessing of related facts of an extracted fact are solved by other sub-systems.\n\nWe solve the OKBC problem by mimicking how humans acquire knowledge and perform reasoning in an interactive conversation. Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge.felong interactive learning and inference (LiLi). Lifelong learning is reflected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning.']","According to the document 1802.06024, LiLi mimics the human process of acquiring knowledge and performing inference during an interactive conversation by imitating how humans encounter unknown concepts or relations while answering queries, perform inference using existing knowledge, retain newly acquired facts in the knowledge base (KB), and use them for future queries. Furthermore, it leverages accumulated knowledge and updated KB, including past inference performances, to guide future interactions and learning, reflecting a lifel0 learning process.","newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning",1.0,0.9999999999,0.9,0.8135683678843084
What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation? ,"['annot be expanded or updated during conversation. In this paper, we propose to build a general knowledge learning engine for chatbots to enable them to continuously and interactively learn new knowledge during conversations. As time goes by, they become more and more knowledgeable and better and better at learning and conversation. We model the task as an open-world knowledge base completion problem and propose a novel technique called lifelong interactive learning and inference (LiLi) to solve it.ss a large number of facts (knowledge) BIBREF10 . It is thus important for a chatbot to continuously learn new knowledge in the conversation process to expand its KB and to improve its conversation ability.\n\nIn recent years, researchers have studied the problem of KB completion, i.e., inferring new facts (knowledge) automatically from existing facts in a KB.Title: Towards a Continuous Knowledge Learning Engine for Chatbots\n\nAbstract: Although chatbots have been very popular in recent years, they still have some serious weaknesses which limit the scope of their applications. One major weakness is that they cannot learn new knowledge during the conversation process, i.e., their knowledge is fixed beforehand and cannot be expanded or updated during conversation.']","The documents propose the use of a general knowledge learning engine for chatbots, which allows them to continuously learn new knowledge during conversations, thereby becoming more knowledgeable and better at conversation. This is established through the modeling of the task as an open-world knowledge base completion problem and the proposal of a novel technique called lifelong interactive learning and inference (LiLi). The effectiveness of this approach is further suggested by the focus on expanding the Knowledge Base (KB) and improving conversation ability, as mentioned in the source document 1802.06024.",Coverage Avg. MCC and avg. +ve F1 score,,,0.875,0.8870005738296601
What are the components of the general knowledge learning engine?,"['annot be expanded or updated during conversation. In this paper, we propose to build a general knowledge learning engine for chatbots to enable them to continuously and interactively learn new knowledge during conversations. As time goes by, they become more and more knowledgeable and better and better at learning and conversation. We model the task as an open-world knowledge base completion problem and propose a novel technique called lifelong interactive learning and inference (LiLi) to solve it.s a Knowledge Store (KS) for knowledge retention. KS has four components: (i) Knowledge Graph ( INLINEFORM0 ): INLINEFORM1 (the KB) is initialized with base KB triples (see §4) and gets updated over time with the acquired knowledge. (ii) Relation-Entity Matrix ( INLINEFORM2 ): INLINEFORM3 is a sparse matrix, with rows as relations and columns as entity-pairs and is used by the prediction model.nowledge learning in human-machine conversations. We first showed that the problem underlying the engine can be formulated as an open-world knowledge base completion (OKBC) problem. We then proposed an lifelong interactive learning and inference (LiLi) approach to solving the OKBC problem. OKBC is a generalization of KBC.']","The general knowledge learning engine comprises three main components: the Knowledge Graph (initialized with base KB triples), the Relation-Entity Matrix (a sparse matrix used by the prediction model), and a Knowledge Store (KS) for knowledge retention. (Source: 1802.06024)","Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.",,0.0,0.375,0.9893557350293198
How many labels do the datasets have?,"['and Music (M). Each domain contains two datasets. Set 1 contains 6000 instances with exactly balanced class labels, and set 2 contains 6000 instances that are randomly sampled from the large dataset, preserving the original label distribution, which we believe better reflects the label distribution in real life. The examples in these two sets do not overlap.omain has a close-to-balanced label distribution. As we also evaluate on set 1 of the target domain, this is also considered as a transductive setting.\n\nSetting (2): Set 2 from both the source and target domains are used as unlabeled sets. Since set 2 is directly sampled from millions of reviews, it better reflects real-life sentiment distribution.\n\nLarge-scale datasets: We further conduct experiments on four much larger datasets: IMDB (I), Yelp2014 (Y), Cell Phone (C), and Baby (B).ble in an unsupervised domain adaptation setting. Furthermore, removing neutral instances may bias the dataset favorably for max-margin-based algorithms like ours, since the resulting dataset has all uncertain labels removed, leaving only high confidence examples. Therefore, we construct new datasets by ourselves.']","According to the document 1809.00530, the datasets contain exactly balanced class labels. However, the exact number of labels is not explicitly stated in the provided text. I cannot provide a specific number based on the provided documents. Based on the document 1809.00530, it is mentioned that each domain contains datasets with balanced class labels, but the exact number of labels is not specified in the provided text. Therefore, I cannot provide a specific number based on the provided documents.",719313,0.0,,0.3333333333333333,0.0
What is the architecture of the model?,"['et examples are available (see Figure FIGREF27 ).\n\nFor the proposed model, we denote INLINEFORM0 parameterized by INLINEFORM1 as a neural-based feature encoder that maps documents from both domains to a shared feature space, and INLINEFORM2 parameterized by INLINEFORM3 as a fully connected layer with softmax activation serving as the sentiment classifier.distribution and corpus size into consideration. The results show that our model is able to obtain significant improvements over strong baselines. We also demonstrate through a series of analysis that the proposed method benefits greatly from incorporating unlabeled target data via semi-supervised learning, which is consistent with our motivation.raining framework for reducing domain difference. In their model, a sub-network is added as a domain discriminator while deep features are learned to confuse the discriminator. The feature adaptation component in our model shares similar intuition with MMD and adversary training. We will show a detailed comparison with them in our experiments.\n\nSemi-supervised Learning: We attempt to treat domain adaptation as a semi-supervised learning task by considering the target instances as unlabeled data.']","The model architecture includes a neural-based feature encoder parameterized by INLINEFORM1, which maps documents from both domains to a shared feature space. It also includes a fully connected layer with softmax activation, parameterized by INLINEFORM2, serving as a sentiment classifier. Furthermore, the model incorporates a sub-network as a domain discriminator to reduce domain difference, sharing similar intuition with MMD and adversary training. This setup is part of an attempt to treat domain adaptation as a semi-supervised learning task, treating target instances as unlabeled data. (Source: 1809.00530)","one-layer CNN structure from previous works BIBREF22 , BIBREF4",0.0,0.0,0.7,
What are the baseline methods?,"['Figure FIGREF17 are presented in Table TABREF38 . We can observe that DAS-EM, DAS-SE, and DAS all achieve substantial improvements over baseline methods under different settings.\n\n\n## CNN Filter Analysis Full Results\n\nAs mentioned in Section SECREF36 , we conduct CNN filter analysis on NaiveNN, FANN, and DAS.cale datasets to reduce the number of iterations. RMSProp optimizer with learning rate set to 0.0005 is used for all experiments.\n\n\n## Models for Comparison\n\nWe compare with the following baselines:\n\n(1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain.\n\n(2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features.distribution and corpus size into consideration. The results show that our model is able to obtain significant improvements over strong baselines. We also demonstrate through a series of analysis that the proposed method benefits greatly from incorporating unlabeled target data via semi-supervised learning, which is consistent with our motivation.']","According to the document 1809.00530, the baseline methods are Naive, which uses bag-of-words representations and an SVM classifier trained on the source domain, and mSDA BIBREF7, which is based on discrete input features and selects the top 1000 bag-of-words features as pivot features.",(1) Naive (2) mSDA BIBREF7 (3) NaiveNN (4) AuxNN BIBREF4 (5) ADAN BIBREF16 (6) MMD,0.3333333333333333,0.9999999999,0.8,0.9212192083314811
What are the source and target domains?,"['data for diverse application domains on-the-fly. We may have sufficient labeled data in an existing domain (called the source domain), but very few or no labeled data in a new domain (called the target domain). This issue has motivated research on cross-domain sentiment classification, where knowledge in the source domain is transferred to the target domain in order to alleviate the required labeling effort.ed for addressing the problem of domain shifting. The aim is to bridge the source and target domains by learning domain-invariant feature representations so that a classifier trained on a source domain can be adapted to another target domain. In cross-domain sentiment classification, many works BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 utilize a key intuition that domain-specific features could be aligned with the help of domain-invariant features (pivot features).on this development set is saved for evaluation. This approach works well on most of the problems since the target domain is supposed to behave like the source domain if the domain difference is effectively reduced.\n\nAnother problem is how to select the values for hyper-parameters.']","According to the information provided in the document with Source ID 1809.00530, the source domain refers to an existing domain where there is sufficient labeled data, while the target domain is a new domain where there are few or no labeled data. The aim is to transfer knowledge from the source to the target domain to alleviate the labeling effort.","Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen",0.0,,0.8,0.9259519329645585
How do they deal with unknown distribution senses?,"['word sense disambiguation have been investigated. The main problem – insufficient number and quality of replacements has been tackled by adding new rich sources of replacements. The quality of the models has indeed improved, especially thanks to replacements based on sense ordering in plWordNet. To deal with the problem of unknown sense distribution, the Bayesian classifier has been modified, removing the bias towards frequent labels in the training data.osemy with heuristics based on wordnet relations. The na\\""ive Bayesian classifier has been modified to account for an unknown distribution of senses. A corpus of 600 million web documents (594 billion tokens), gathered by the NEKST search engine allows us to assess the relationship between training set size and disambiguation accuracy.plementing and evaluating such a tool for Polish. Firstly, the method is expanded by waiving the requirement of monosemy and proposing several new sources of relatives. These previously unexplored sources are based on wordnet data and help gather many training cases from the corpus. Secondly, a well-known problem of uneven yet unknown distribution of word senses is alleviated by modifying a naïve Bayesian classifier.']","They modify a naïve Bayesian classifier to account for an unknown distribution of senses, as stated in the document with the Source ID 1710.07960.",The Näive-Bayes classifier is corrected so it is not biased to most frequent classes,1.0,0.9999999999,0.5,0.7722529919291539
What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?,"[""nderstand the emotional components of propaganda. We rely on Depechemood (a lexical-base emotion analysis method) to detect emotions most likely to be evoked in readers of these materials. The findings indicate that the emotional appeal of ISIS and Catholic materials are similar\n\n\n## Introduction\n\nSince its rise in 2013, the Islamic State of Iraq and Syria (ISIS) has utilized the Internet to spread its ideology, radicalize individuals, and recruit them to their cause.en 0 and 1 for all of these 8 emotion categories. We present our finding using this approach in the result section.\n\n\n## Results\n\nIn this section, we present the results of our analysis based on the contents of ISIS propaganda materials as compared to articles from the Catholic women forum. We then present the results of emotion analysis conducted on both corpora.\n\n\n## Results ::: Content Analysis\n\nAfter pre-processing the text, both corpora were analyzed for word frequencies.women-related articles in ISIS' online magazines?\n\nWhat similarities and/or differences do these topics have with non-violent, non-Islamic religious material addressed specifically to women?\n\nWhat kind of emotions do these articles evoke in their readers and are there similarities in the emotions evoked from both ISIS and non-violent religious materials?""]","According to the document 1912.03804, the authors conclude that there are similarities in the emotions evoked by ISIS and Catholic materials, as indicated by their analysis which found 0 and 1 for all of these 8 emotion categories. This suggests that both types of materials may share common emotional appeals. (Source: 1912.03804)","both corpuses used words that aim to inspire readers while avoiding fear actual words that lead to these effects are very different in the two contexts our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda",0.0,,,0.9389050554403546
How id Depechemood trained?,"['feelings of people after reading a news article. One solution for this type of problem is to use an approach called Depechemood.\n\nDepechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article.ip, and marriage/divorce when they address women. Moreover, we used Depechemood methods to analyze the emotions that these materials are likely to elicit in readers. The result of our emotion analysis suggests that both corpuses used words that aim to inspire readers while avoiding fear. However, the actual words that lead to these effects are very different in the two contexts.atter the size of the corpus we are working with. Although, finding the similarities/differences between topics in these two groups of articles might provide some new insights, we turn to emotional analysis to also compare the emotions evoked in the audience.\n\n\n## Results ::: Emotion Analysis\n\nWe rely on Depechemood dictionaries to analyze emotions in both corpora. These dictionaries are freely available and come in multiple arrangements.']","Depechemood was trained using crowd-annotated news BIBREF24 documents, as mentioned in the Source: 1912.03804 document. The training involved gathering approximately 23.5K documents with an average of 500 words per document from rappler.com.",By multiplying crowd-annotated document-emotion matrix with emotion-word matrix.,0.0,,0.8,0.9510356127098492
How are similarities and differences between the texts from violent and non-violent religious groups analyzed?,"[""ext mining and natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (ISIS's online English language publications) to identify prominent topics. To identify similarities or differences between these texts and those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda.“being afraid” as the lowest score in each issue.\n\n\n## Conclusion and Future Work\n\nIn this paper, we have applied natural language processing methods to ISIS propaganda materials in an attempt to understand these materials using available technologies. We also compared these texts with a non-violent religious groups' (both focusing on women related articles) to examine possible similarities or differences in their approaches. To compare the contents, we used word frequency and topic modeling with NMF.in both ISIS and non-violent religious materials.\n\n\n## Method ::: Content Analysis\n\nThe key task in comparing ISIS material with that of a non-violent group involves analyzing the content of these two corpora to identify the topics. For our analysis, we considered a simple uni-gram model where each word is considered as a single unit. Understanding what words appear most frequently provides a simple metric for comparison.""]","According to the document 1912.03804, the similarities and differences between the texts from violent and non-violent religious groups are analyzed using word frequency and topic modeling with NMF. In addition, the content analysis method involves considering each word as a single unit in a uni-gram model.",By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum,0.0,0.9999999999,1.0,0.7880942010721803
How are prominent topics idenified in Dabiq and Rumiyah?,"[""ext mining and natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (ISIS's online English language publications) to identify prominent topics. To identify similarities or differences between these texts and those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda.h “ to our sisters ” incorporated into the title. Seven out of fifteen Dabiq issues and all thirteen issues of Rumiyah contain articles targeting women, clearly suggesting an increase in attention to women over time.\n\nWe converted all the ISIS magazines to texts using pdf readers and all articles that addressed women in both magazines (20 articles) were selected for our analysis.iew or they do not talk specifically about women. However, ISIS' online magazines are valuable resources for understanding how the organization attempts to appeal to western audiences, particularly women. Looking through both Dabiq and Rumiyah, many issues of the magazines contain articles specifically addressing women, usually with “ to our sisters ” incorporated into the title.""]","Prominent topics in Dabiq and Rumiyah are identified by extracting articles published in these ISIS's online English language publications. A specific focus is placed on articles targeting women, as indicated by the incorporation of ""to our sisters"" in the titles (Source: 1912.03804). Furthermore, twenty articles specifically addressing women from both magazines were selected for analysis to understand the organization's attempt to appeal to western audiences, particularly women (Source: 1912.03804).",LDA non-negative matrix factorization (NMF),,0.0,,0.9729029250877517
Which datasets are used?,"['ew and lacks standardized benchmarks and metrics.\n\n\n## Related work ::: Synthetic datasets\n\nRecently, many synthetic datasets have been proposed as diagnostic tools for deep learning models, such as CLEVR BIBREF21 for visual question answering (VQA), the bAbI tasks BIBREF22 for text understanding and reasoning, and ShapeWorld BIBREF11 for visually grounded language understanding.dICE datasets are illustrated in Figure FIGREF17. We can see that the diversity of inferred captions is largely sensitive to the caption variability in the dataset itself. For simple datasets (such as Existential-OneShape) where language constructions in the training set are less diverse, the output captions tend to have uniform sentence structures.or color-shape combination (e.g., “red square”). So the statement “A square is red” and “A circle is blue” are considered the same, while “A shape is red” is different.\n\n\n## Experimental Setup ::: Datasets\n\nWe develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper.']","The datasets used are ShapeWorldICE datasets, as mentioned in the document with the Source ID 1912.08960.","Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE",,,0.5,0.8956250958192346
Which existing models are evaluated?,"['atasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE. We empirically demonstrate that the existing metrics BLEU and SPICE do not capture true caption-image agreement in all scenarios, while the GTD framework allows a fine-grained investigation of how well existing models cope with varied visual situations and linguistic constrordance with how a model is eventually evaluated. In future work, we plan to investigate the feasibility of deliberately encoding the GTD signal in the training process, for instance, by implementing a GTD-aware loss. We also plan to extend the existing ShapeWorldICE benchmark to include more linguistic constructions (such as relative clauses, compound sentences and coreference).ween a caption and the underlying visual content. We introduce a new diagnostic evaluation framework for the task of image captioning, with the goal of directly assessing models for grammaticality, truthfulness and diversity (GTD) of generated captions. We demonstrate the potential of our evaluation framework by evaluating existing image captioning models on a wide ranging set of synthetic datasets that we construct for diagnostic evaluation.']","According to the document with the Source ID 1912.08960, existing image captioning models are evaluated using the ShapeWorldICE benchmark.",Show&Tell and LRCN1u,,0.0,0.6666666666666666,0.8435217113036737
How is diversity measured?,"['aptions tend to have uniform sentence structures. The high diversity ratios of generated Spatial and Quantification captions suggest that caption diversity benefits from heterogeneous language constructions in complex datasets.\n\n\n## Discussions and Conclusions\n\nEvaluation metrics are required as a proxy for performance in real applications. As such, they should, as far as possible, allow measurement of fundamental aspects of the performance of models on tasks.sity of the evaluated model on these test images. Practically, we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number:\n\nLanguage constructions here correspond to reduced caption representations which only record whether an object is described by shape (e.g., “square”), color (e.g., “red shape”) or color-shape combination (e.g., “red square”).ding to significantly different results BIBREF16. Its validity as a metric for tasks other than machine translation has been questioned BIBREF17, particularly for tasks for which the output content is not narrowly constrained, like dialogue BIBREF18.\n\nSome recent work focuses on increasing the diversity of generated captions, for which various measures are proposed. Devlin et al. BIBREF19 explored the concept of caption diversity by evaluating performance on compositionally novel images.']","Diversity is measured by computing the ratio of observed number of different language constructions to the optimal number, as stated in the document with the ID 1912.08960. Specifically, it is calculated based on reduced caption representations that record whether an object is described by shape, color, or color-shape combination.",diversity score as the ratio of observed number versus optimal number,,0.9999999999,0.75,0.9153878928771615
What state-of-the-art deep neural network is used?,"['the quality of named entity recognition followed. In this paper we integrate a boundary assembling method with the state-of-the-art deep neural network model, and incorporate the updated word boundary information into a conditional random field model for named entity recognition.which could benefit downstream tasks such as NER.\n\nInspired by BIBREF12, we integrate in this paper a boundary assembling step into the state-of-the-art LSTM model for Chinese word segmentation, and feed the output into a CRF model for NER, resulting in a 2% absolute improvement on the overall F1 score over current state-of-the-art methods.\n\nThis paper is organized as follows.as information extraction and question answering. NER requires phrases referring to entities in text be identified and assigned to particular entity types, thus can be naturally modeled as a sequence labeling task. In recent years, a lot of progress has been made on NER by applying sequential models such as conditional random field (CRF) or neural network models such as long short-term memory (LSTM) (e.g., BIBREF0, BIBREF1, BIBREF2, BIBREF3).']",The state-of-the-art deep neural network model used is the long short-term memory (LSTM) model. This is based on the information from the document with the Source ID of 2002.11910.,LSTM model,,0.9999999999,0.5,0.9065099785565303
What boundary assembling method is used?,"['domain or a corpus of different characteristics. Our improvement indicates that the boundary assembling method is not sensitive to the specific domain, and is a robust method for cross-domain scenarios.\n\nThe identification of two nouns that are next to each other depends on the pre-trained lexical features.the quality of named entity recognition followed. In this paper we integrate a boundary assembling method with the state-of-the-art deep neural network model, and incorporate the updated word boundary information into a conditional random field model for named entity recognition.d segmentation and better semantic understanding. Sentences in the PKU corpus are often segmented into the smallest word units. This results in too fragmented information and incorrect lexical units when input into a named entity recognition model, although this may benefit some other natural language process tasks. By applying the boundary assembling method, sense group in a sentence is better preserved.']",The boundary assembling method integrates with a deep neural network model and incorporates updated word boundary information into a conditional random field model for named entity recognition. (Source: 2002.11910),"This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.",0.0,,1.0,0.8893695807301153
What are previous state of the art results?,"['random field model for named entity recognition. Our method shows a 2% absolute improvement over previous state-of-the-art results.\n\n\n## Introduction\n\nNamed entity recognition (NER) is a challenging problem in Natural Language Processing, and often serves as an important step for many popular applications, such as information extraction and question answering.dropout rate of 0.1, and a learning rate of 0.05. Our results are compared with state-of-the-art models BIBREF15, BIBREF19, BIBREF20 on the same Sina Weibo training and test datasets. Our model shows an absolute improvement of 2% for the overall F1 score.\n\nThis significant improvement validates our method of applying boundary assembling to the segmented sentences, which results in more accurate word segmentation and better semantic understanding.which could benefit downstream tasks such as NER.\n\nInspired by BIBREF12, we integrate in this paper a boundary assembling step into the state-of-the-art LSTM model for Chinese word segmentation, and feed the output into a CRF model for NER, resulting in a 2% absolute improvement on the overall F1 score over current state-of-the-art methods.\n\nThis paper is organized as follows.']","According to the document 2002.11910, previous state-of-the-art results were achieved by models BIBREF15, BIBREF19, and BIBREF20. The model in the discussed paper shows an absolute improvement of 2% on the overall F1 score over these methods.","Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11",,,,0.8846593323367468
What is the model performance on target language reading comprehension?,"['tation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.\n\n\n## Introduction\n\nReading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries.pable to some extent between different languages. This implies that multi-BERT model might encode semantic and syntactic information in language-agnostic ways and explains how zero-shot transfer learning could be done.\n\nTo take a step further, while transfering model from source dataset to target dataset, we align representations in two proposed way, to improve performance on target dataset.Title: Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model\n\nAbstract: Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with a language representation model pre-trained on multi-lingual corpus.']","The model performance on target language reading comprehension can be improved through cross-lingual transfer learning. According to the source document `1909.09587`, this method leverages a pre-trained language representation model on a multi-lingual corpus, which has shown feasibility in zero-shot learning scenarios. This suggests that the model can effectively transfer its learned language-agnostic representations to enhance performance on target language reading comprehension tasks.",Table TABREF6 Table TABREF8,0.0,0.0,1.0,0.9611807026596155
What source-target language pairs were used in this work? ,"['nces (multi-BERT), in cross-lingual zero-shot RC. We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT. Surprisingly, we find that the models have the ability to transfer between low lexical similarity language pair, such as English and Chinese.ersions of SQuAD/DRCD are parallel to each other. Given a source-target language pair, we calculate cosine similarity of the mean pooling of tokens representation within corresponding answer-span as a measure of how much they look like in terms of the internal representation of multi-BERT. The results are shown in Fig.rce language into target language, or vice versa. These methods may not require a well-annotated RC dataset for the target language, whereas a high-quality MT model is needed as a trade-off, which might not be available when it comes to low-resource languages.\n\nIn this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC.']","The document does not provide specific details about the source-target language pairs used in the work. However, it mentions that the researchers used a method that involves translating language into target language, or vice versa, indicating that multiple language pairs were likely used. (Source: 1909.09587)","En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean",0.0,,0.6666666666666666,0.0
What model is used as a baseline?  ,"['with great practical value in various industries. In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance.BERT were pre-trained on corpus in 104 languages. Data in different languages were simply mixed in batches while pre-training, without additional effort to align between languages. When fine-tuning, we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged.pable to some extent between different languages. This implies that multi-BERT model might encode semantic and syntactic information in language-agnostic ways and explains how zero-shot transfer learning could be done.\n\nTo take a step further, while transfering model from source dataset to target dataset, we align representations in two proposed way, to improve performance on target dataset.']","The baseline model used is BERT, as indicated in the document with the ID 1909.09587. Specifically, the text ""bert were pre-trained on corpus in 104 languages"" supports this.",pre-trained multi-BERT,,,0.6666666666666666,0.8902394357766029
what does the model learn in zero-shot setting?,"['ilarities between the embedding of the languages. The results can be found in the Appendix.\n\n\n## What Does Zero-shot Transfer Model Learn? ::: Code-switching Dataset\n\nWe observe linguistic-agnostic representations in the last subsection. If tokens are represented in a language-agnostic way, the model may be able to handle code-switching data.tation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.\n\n\n## Introduction\n\nReading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries.matching words in questions and context BIBREF20. Is zero-shot learning feasible because the model simply learns this kind of language independent strategies on one language and apply to the other?\n\nTo verify whether multi-BERT largely counts on a language independent strategy, we test the model on the languages unseen during pre-training. To make sure the languages have never been seen before, we artificially make unseen languages by permuting the whole vocabulary of existing languages.']","According to the document 1909.09587, the model learns linguistic-agnostic representations in the zero-shot setting. This suggests that the model can handle data from languages it has not seen before, without the need for translating the source data into the target language, as this can even degrade performance.","we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged",0.0,0.0,0.75,0.7844576356439319
Which types of named entities do they recognize?,"['ict it correctly as a non-named entity (a fruit). Trending entities (musicians or DJs such as `CID\', `Duke Dumont\', `Marshmello\', etc.) are also recognized correctly with strengthened contexts from visual information (describing concert scenes) despite lack of surrounding textual contexts.such as in free-form text or images without tags. Named entity recognition (NER), the task of recognizing named entities from free-form text, is thus a critical step for building structural information, allowing for its use in personalized assistance, recommendations, advertisement, etc.\n\nWhile many previous approaches BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 on NER have shown success for well-formed text in recognizing named entities via word context resolution (e.g.named entities via word context resolution (e.g. LSTM with word embeddings) combined with character-level features (e.g. CharLSTM/CNN), several additional challenges remain for recognizing named entities from extremely short and coarse text found in social media posts. For instance, short social media posts often do not provide enough textual contexts to resolve polysemous entities (e.g. “monopoly is da best "", where `monopoly\' may refer to a board game (named entity) or a term in economics).']","According to the document 1802.07862, they recognize named entities such as musicians or DJs, like `CID', `Duke Dumont', `Marshmello', etc., and fruits, correctly identifying them from visual information even without surrounding textual contexts.","PER, LOC, ORG, MISC",0.0,0.0,0.0,0.858424038922004
How large is their MNER SnapCaptions dataset?,"['periority, the module itself is flexible and thus can work with other NER architectures or for other multimodal applications.\n\n\n## SnapCaptions Dataset\n\nThe SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories).the NER performance on the Snap Captions dataset. We report both entity types recognition (PER, LOC, ORG, MISC) and named entity segmentation (named entity or not) results.\n\nParameters: We tune the parameters of each model with the following search space (bold indicate the choice for our final model): character embeddings dimension: {25, 50, 100, 150, 200, 300}, word embeddings size: {25, 50, 100, 150, 200, 300}, LSTM hidden states: {25, 50, 100, 150, 200, 300}, and INLINEFORM0 dimension: {25, 50, 100, 150,ontexts, bringing significant challenges for NER. To this end, we create a new dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities). We then build upon the state-of-the-art Bi-LSTM word/character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while a']",The SnapCaptions dataset consists of 10K user-generated image and textual caption pairs. (Source: 1802.07862),10K user-generated image (snap) and textual caption pairs,,,0.5,0.909921838857522
What is masked document generation?,"['diction (NSP), which is different from this task. NSP is a classification task, but NSG is a generation task, which intends to pre-train a generation model.\n\n\n## Sequence-to-Sequence Transformer Pre-training ::: Pre-training Tasks ::: Masked Document Generation\n\nThe third task we consider is Masked Document Generation (MDG) that learns to recover a document with a masked span of tokens (see Figure FIGREF5).tence generation, and masked document generation. When we only employ the unlabeled documents in the training splits of summarization datasets to pre-training Step with our proposed tasks, the summarization model based on the pre-trained Step outperforms the best published abstractive system. Involving large scale data to pre-training leads to larger performance gains.tence generation, and masked document generation. Experiments on two summarization datasets show that all three tasks can improve performance upon a heavily tuned large Seq2Seq Transformer which already includes a strong pre-trained encoder by a large margin.']","Masked Document Generation is a task that learns to recover a document with a masked span of tokens, as described in the document with the ID 2004.01853.",A task for seq2seq model pra-training that recovers a masked document to its original form.,,,,0.9874070018167275
Which of the three pretraining tasks is the most helpful?,"['y. Results of Step are listed in the third block. Similar to the trends in CNNDM, Step leads significant performance gains (with $p<0.05$) compared to all other models in Table TABREF26.\n\nAmong all three pre-training tasks, SR works slightly better than the other two tasks (i.e., NSG and MDG). We also tried to randomly use all the three tasks during training with 1/3 probability each (indicated as ALL).nt after the application of our masking strategy. We could create infinite amount of $\\langle \\hat{X}_M,X\\rangle $ pairs to train Step.\n\nDuring pre-training, we could also employ all the three tasks (i.e., SR, NSG, MDG) together. For each training batch, we randomly choose one task and each task is used for $1/3$ of the time.o pre-training leads to larger performance gains. By using the best performing pre-training task, Step achieves 0.8 absolute ROUGE-2 improvements on CNN/DailyMail and 2.4 absolute ROUGE-2 improvements on New York Times. In the future, we would like to investigate other tasks to pre-train the seq2seq transformer model. Pre-training for unsupervised abstractive summarization is also an interesting direction and worth exploration.']","According to the source document 2004.01853, Step achieves the most significant performance gains when using the best performing pre-training task. Specifically, it leads to larger performance gains as evidenced by the 0.8 absolute ROUGE-2 improvements on CNN/DailyMail and 2.4 absolute ROUGE-2 improvements on New York Times. Therefore, the best performing pre-training task is the most helpful. (Source: 2004.01853)",SR,1.0,,0.6,0.8957037257009336
What useful information does attention capture?,"['ional equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.\n\n\n## Introduction\n\nNeural machine translation (NMT) has gained a lot of attention recently due to its substantial improvements in machine translation quality achieving state-of-the-art performance for several languages BIBREF0 , BIBREF1 , BIBREF2 .alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information.g captured by the attention mechanism in general. The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in different syntactic phenomena?\n\nOur analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns.']","Attention models capture information beyond traditional alignments, as indicated in the document (1710.03348). For example, attention agrees with traditional alignments to a high degree in the case of nouns, suggesting it captures relevant information such as noun alignments. However, it also captures useful information other than alignments, particularly for verbs, where a large portion of attention is paid to words other than alignment points. Thus, attention models are not solely modeling alignment but are also capturing other relevant syntactic and semantic information.",it captures other information rather than only the translational equivalent in the case of verbs,1.0,0.9999999999,0.75,0.8987755839076316
What datasets are used?,"['TH German-English dataset as the hard alignments. The statistics of the data are given in Table TABREF8 . We convert the hard alignments to soft alignments using Equation EQREF10 . For unaligned words, we first assume that they have been aligned to all the words in the source side and then do the conversion.e most attended source word for each target word. We also use GIZA++ BIBREF10 to produce automatic alignments over the data set to allow for a comparison between automatically generated alignments and the attentions generated by our systems. GIZA++ is run in both directions and alignments are symmetrized using the grow-diag-final-and refined alignment heuristic.PE BIBREF13 which operates at the sub-word level.\n\nWe report alignment error rate (AER) BIBREF14 , which is commonly used to measure alignment quality, in Table TABREF20 to show the difference between attentions and human alignments provided by RWTH German-English dataset. To compute AER over attentions, we follow DBLPjournalscorrLuongPM15 to produce hard alignments from attentions by choosing the most attended source word for each target word.']","The datasets used are the German-English dataset from RWTH and the sub-word level dataset from DBLPjournalscorrLuongPM15. According to the source 1710.03348, the former is used for hard alignments and comparison with system-generated attentions, while the latter is used to compute alignment error rate (AER) over attentions.",WMT15 German-to-English RWTH German-English dataset,0.0,0.9999999999,,0.9064479576317668
In what cases is attention different from alignment?,"['ional equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.\n\n\n## Introduction\n\nNeural machine translation (NMT) has gained a lot of attention recently due to its substantial improvements in machine translation quality achieving state-of-the-art performance for several languages BIBREF0 , BIBREF1 , BIBREF2 .g captured by the attention mechanism in general. The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in different syntactic phenomena?\n\nOur analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns.sis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information.']","According to the document with Source ID 1710.03348, attention models are found to capture information beyond traditional alignment in certain instances. For example, attention agrees with traditional alignments to a high degree in the case of nouns, but in other cases, it captures additional useful information.","For certain POS tags, e.g. VERB, PRON.",0.0,0.9999999999,0.6666666666666666,
How do they calculate variance from the model outputs?,"[""he norm of the gradient incurred by the instance. EGL has previously been justified as intuitively measuring the expected change in a model's parameters BIBREF3 .We formalize this intuition from the perspective of asymptotic variance reduction, and experimentally, we show EGL to be superior to confidence-based methods on speech recognition tasks.of the gradient norm over all possible labelings. BIBREF3 interprets EGL as “expected model change”. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.\n\n\n## Variance in the Asymptote\n\nAssume the joint distribution of INLINEFORM0 has the following form, DISPLAYFORM0\n\nwhere INLINEFORM0 is the true parameter, and INLINEFORM1 is independent of INLINEFORM2 .e EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3 .\n\nHere we have provided a more formal characterization of EGL to complement its intuitive interpretation as “expected model change” in BIBREF3 . For notational convenience, we denote Eq. ( EQREF8 ) as EGL in subsequent sections.\n\n\n## Experiments\n\nWe empirically validate EGL on speech recognition tasks.""]","According to the document 1612.03226, they calculate variance from the model outputs using the concept of Expected Gradient Length (EGL). The EGL method is almost the same as Equation (EQREF8), which is not explicitly defined in the provided context. The EGL is interpreted as ""expected model change"", suggesting a measure of how much the model's parameters are expected to change, which is related to variance reduction in the model's estimations.","reducing the variance of an estimator  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3",,,0.6666666666666666,0.9288029836666994
How much data samples do they start with before obtaining the initial model labels?,"['e a probability distribution over the characters. CTC loss BIBREF8 is then computed from the timestep-wise probabilities.\n\nA base model, INLINEFORM0 , is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances.ue is that we do not know INLINEFORM0 in advance. We could instead substitute an estimate INLINEFORM1 from a pre-trained model, where it is reasonable to assume the INLINEFORM2 to be close to the true INLINEFORM3 . The batch selection then works by taking the samples that have largest gradient norms, DISPLAYFORM0\n\nFor RNNs, the gradients for each potential label can be obtained by back-propagation.ovel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.\n\n\n## Introduction\n\nState-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize. Labeling thousands of hours of audio, however, is expensive and time-consuming.']","According to the document with Source ID 1612.03226, the initial model labels are obtained from a subset of a 1,700-hour unlabeled dataset.","1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset",,,0.5,0.8404374058787818
Which model do they use for end-to-end speech recognition?,"[""methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores.INEFORM1 the corresponding label (transcription). A speech recognition system models the conditional distribution INLINEFORM2 , where INLINEFORM3 are the parameters in the model, and INLINEFORM4 is typically implemented by a Recurrent Neural Network (RNN). A training set is a collection of INLINEFORM5 pairs, denoted as INLINEFORM6 .r training end-to-end speech recognition systems.\n\n\n## Conclusion and Future Work\n\nWe formally explained EGL from a variance reduction perspective and experimentally tested its performance on end-to-end speech recognition systems. Initial experiments show a notable gain over random selection, and that it outperforms confidence score methods used in the ASR community. We also show EGL measures sample informativeness in a very different way from confidence scores, giving rise to open research questions.""]","They use a Recurrent Neural Network (RNN) model, as indicated in the source document 1612.03226.",RNN,1.0,0.9999999999,0.5,0.810160733851102
Which dataset do they use?,"['pid reduction in error than all other approaches. Specifically, when querying 20% of the unlabeled dataset, EGL has 11.58% lower CER and 11.09% lower WER relative to random. The performance of EGL at querying 20% is on par with random at 40%, suggesting that using EGL can lead to an approximate 50% decrease in data labeling.\n\n\n## Similarity between Query Methods\n\nIt is useful to understand how the three active learning methods differ in measuring the informativeness of an instance.on of INLINEFORM5 pairs, denoted as INLINEFORM6 . The parameters of the model are estimated by minimizing the negative log-likelihood on the training set: DISPLAYFORM0\n\nActive learning seeks to augment the training set with a new set of utterances and labels INLINEFORM0 in order to achieve good generalization on a held-out test dataset. In many applications, there is an unlabeled pool INLINEFORM1 which is costly to label in its entirety.s a variable percentage of the unlabeled dataset. The queries are then included into training set, and the model continues training until convergence. Fig. FIGREF9 reports the metrics (Exact values are reported in Table TABREF12 in the Appendix) on the test set as the query percentage varies. All the active learning methods outperform the random baseline. Moreover, EGL shows a steeper, more rapid reduction in error than all other approaches.']","The context mentions an ""unlabeled pool INLINEFORM1"" which is used in the active learning process. However, specific details about the dataset are not provided in the given documents. Therefore, I cannot provide specific details about the dataset from the provided documents.",190 hours ( INLINEFORM1 100K instances),,0.0,,0.0
Which baselines did they compare against?,"['e experimental results are depicted in Figure 3 . As the chart shows, our model outperforms all the other options we have considered. In detail, the left part of the chart shows that the leaf-LSTM is the most effective option compared to its competitors. Note that the sequential leaf-LSTM is somewhat superior or competitive than the bidirectional leaf-LSTM when both have a comparable number of parameters.of two sentences, the premise and the hypothesis. Our objective given the data is to predict the correct relationship between the two sentences among three options— contradiction, neutral, or entailment.\n\nWe use the siamese architecture to encode both the premise ( $p_{1:m}$ ) and hypothesis ( $h_{1:n}$ ) following the standard of sentence-encoding models in the literature.aid of syntactic trees or linguistic information. The latent tree models have the advantage of being able to find the optimized task-specific order of composition rather than a sequential or syntactic one. In experiments, we compare our model with not only syntactic tree-based models but also latent tree models, demonstrating that modeling with explicit linguistic knowledge can be an attractive option.']","According to the document 1809.02286, the model was compared against syntactic tree-based models and latent tree models.","Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks",0.0,,1.0,0.7645750409859072
What baselines did they consider?,"['ctors of all discourse arguments in that message. Average embeddings have performed well for representing text sequences in other tasks BIBREF5 .\n\nWe first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message).each subtask of causal explanation analysis (i.e. CEI only focused on data already identified to contain causal explanations). Here, we seek to evaluate the complete pipeline of CP and CEI, starting from all of test data (those or without causality) and evaluating the final accuracy of CEI predictions.optimism (and its converse, pessimism) BIBREF0 . For example, in “My parser failed because I always have bugs.”, the emphasized text span is considered a causal explanation which indicates pessimistic personality – a negative event where the author believes the cause is pervasive.']","They considered state-of-the-art PDTB taggers as baselines for their evaluation, as mentioned in the document with the Source ID 1809.01202.",state-of-the-art PDTB taggers,1.0,0.9999999999,0.5,0.91649088467907
What types of social media did they consider?,"[""f discourse connectives (e.g., `cuz' and `bcuz').\n\nPrevailing approaches for social media analyses, utilizing traditional linear models or bag of words models (e.g., SVM trained with n-gram, part-of-speech (POS) tags, or lexicon-based features) alone do not seem appropriate for this task since they simply cannot segment the text into meaningful discourse units or discourse arguments such as clauses or sentences rather than random consecutive token sequences or specific word tokens.presenting the whole sequence BIBREF3 , BIBREF5 . As with the full architectures, for CP both of these variants ends with a many-to-one classification per message, while the CEI model ends with a sequence of classifications.\n\n\n## Experiment\n\nWe explored three types of models (RBF SVM, Linear SVM, and Random Forest Classifier) which have previously been shown empirically useful for the language analysis in social media.tream tasks in social media BIBREF19 , BIBREF18 . In addition to these features, we also extracted First-Last, First3 features and Word Pairs from every adjacent pair of discourse arguments since these features were most helpful for causal relation prediction BIBREF9 . First-Last, First3 features are first and last word and first three words of two discourse arguments of the relation, and Word Pairs are the cross product of words of those discourse arguments.""]","According to the document 1809.01202, they considered social media tasks in BIBREF19 and BIBREF18.",Facebook status update messages,,0.0,0.0,0.7692505644877968
How was the dataset annotated?,"['dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.\n\n\n## Dataset ::: In-Scope Data Collection\n\nWe defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents.such data is expensive and difficult to generate. We believe our analysis and dataset will lead to developing better, more robust dialog systems.\n\nAll datasets introduced in this paper can be found at https://github.com/clinc/oos-eval.n and test distributions unrealistically similar. We note that this is a recommendation from concurrent work by BIBREF3. We also used this procedure for the out-of-scope set, except that we split the data into train/validation/test based on task prompt instead of worker.\n\n\n## Dataset ::: Dataset Variants\n\nIn addition to the full dataset, we consider three variations. First, Small, in which there are only 50 training queries per each in-scope intent, rather than 100.']","The dataset was manually grouped into intents based on data generated through scoping tasks. This process was guided by queries collected using a scoping crowdsourcing task, prompting crowd workers to provide questions and commands related to topic domains in the manner they would interact with an AI assistant. (Source: 1909.02027)",intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task,1.0,0.9999999999,0.875,0.8756380653068011
Which classifiers are evaluated?,"['ifier models and out-of-scope prediction schemes.\n\n\n## Benchmark Evaluation ::: Classifier Models\n\nSVM: A linear support vector machine with bag-of-words sentence representations.\n\nMLP: A multi-layer perceptron with USE embeddings BIBREF4 as input.\n\nFastText: A shallow neural network that averages embeddings of n-grams BIBREF5.\n\nCNN: A convolutional neural network with non-static word embeddings initialized with GloVe BIBREF6.al different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.tyle made by real users of task-oriented systems. The queries cover 150 intents, plus out-of-scope queries that do not fall within any of the 150 in-scope intents.\n\nWe evaluate a range of benchmark classifiers and out-of-scope handling methods on our dataset. BERT BIBREF1 yields the best in-scope accuracy, scoring 96% or above even when we limit the training data or introduce class imbalance. However, all methods struggle with identifying out-of-scope queries.']","According to the document 1909.02027, the classifiers evaluated include SVM, MLP, FastText, and CNN.",SVM MLP FastText CNN BERT Google's DialogFlow Rasa NLU,0.7142857142857143,,,0.9079924449655566
What is the size of this dataset?,"['tents and an enormous number of examples of each. Snips does present a low-data variation, with 70 training queries per intent, in which performance drops slightly. The dataset presented in BIBREF10 has a large number of intent classes, yet also contains a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases).n the research and development of dialog systems. All data introduced in this paper can be found at https://github.com/clinc/oos-eval.\n\n\n## Dataset\n\nWe introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.that perform slightly worse on in-scope queries. The trend is mostly the opposite when evaluating out-of-scope, where recall increases under the Small and Imbalanced training conditions. Under these two conditions, the size of the in-scope training set was decreased, while the number of out-of-scope training queries remained constant. This indicates that out-of-scope performance can be increased by increasing the relative number of out-of-scope training queries.']","The dataset contains 23,700 queries, as stated in the document with the ID 1909.02027.","23,700",,0.9999999999,,0.844381307611154
Where does the data come from?,"['ped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system.c areas found on Quora, Wikipedia, and elsewhere. To help ensure the richness of this additional out-of-scope data, each of these task prompts contributed to at most four queries. Since we use the same crowdsourcing method for collecting out-of-scope data, these queries are similar in style to their in-scope counterparts.dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.\n\n\n## Dataset ::: In-Scope Data Collection\n\nWe defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents.']","The data originates from multiple sources including Quora, Wikipedia, and other locations as indicated in the document source 1909.02027.",crowsourcing platform,1.0,0.9999999999,,0.9759404113878004
What are method improvements of F1 for paraphrase identification?,"['SC loss is complementary to the model structures.\n\n\n## Experiments ::: Paraphrase Identification\n\nParaphrases are textual expressions that have the same semantic meaning using different surface words. Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning or not. We use BERT BIBREF11 and XLNet BIBREF43 as backbones and report F1 score for comparison.Table shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively.ieves 87.65 on EM and 89.51 on F1 for SQuAD v2.0. Moreover, on QuoRef, the proposed method surpasses XLNet results by +1.46 on EM and +1.41 on F1. Another observation is that, XLNet outperforms BERT by a huge margin, and the proposed DSC loss can obtain further performance improvement by an average score above 1.0 in terms of both EM and F1, which indicates the DSC loss is complementary to the model structures.']","According to the document 1911.02855, the proposed method achieves F1 improvements of +0.29 on English datasets such as CoNLL2003 and +0.96 on OntoNotes5.0. Furthermore, it shows substantial performance boosts on Chinese datasets, with F1 improvements of +0.97 on MSRA and +2.36 on OntoNotes4.0.",Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP,,0.0,,0.8461546803632259
What are method's improvements of F1 for NER task for English and Chinese datasets?,"['Table shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively.improve the representations of a Bi-LSTM encoder.\n\nBert-Tagger: devlin2018bert treats NER as a tagging task.\n\nGlyce-BERT: wu2019glyce combines glyph information with BERT pretraining.\n\nBERT-MRC: The current SOTA model for both Chinese and English NER datasets proposed by xiaoya2019ner, which formulate NER as machine reading comprehension task.\n\n\n## Experiments ::: Named Entity Recognition ::: Results\n\nTable shows experimental results on NER datasets.d recall. Experiment results are shown in Table . The highest F1 for Chinese OntoNotes4.0 is 84.67 when $\\alpha $ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha $ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha $ changes in distinct datasets, which shows that the hyperparameters $\\alpha ,\\beta $ play an important role in the proposed method.\n\n\n## Conclusion\n\nIn this paper, we alleviate the severe data imbalance issue in NLP tasks.']","For English datasets, the proposed method improves the F1 score by +0.29 on CoNLL2003 and +0.96 on OntoNotes5.0. For Chinese datasets, the method achieves an F1 improvement of +0.97 on MSRA and +2.36 on OntoNotes4.0. (Source: 1911.02855)","English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively",1.0,,0.8,0.9025926855758432
What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?,"['Table shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively.2018bert treats part-of-speech as a tagging task.\n\n\n## Experiments ::: Part-of-Speech Tagging ::: Results\n\nTable presents the experimental results on the POS task. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by +1.86 in terms of F1 score on CTB5, +1.80 on CTB6 and +2.19 on UD1.4. As far as we are concerned, we are achieving SOTA performances on the three datasets.ieves 87.65 on EM and 89.51 on F1 for SQuAD v2.0. Moreover, on QuoRef, the proposed method surpasses XLNet results by +1.46 on EM and +1.41 on F1. Another observation is that, XLNet outperforms BERT by a huge margin, and the proposed DSC loss can obtain further performance improvement by an average score above 1.0 in terms of both EM and F1, which indicates the DSC loss is complementary to the model structures.']","According to the document 1911.02855, the proposed method achieves F1 improvements by +0.97 on MSRA and +2.36 on OntoNotes4.0 for Chinese POS datasets. This indicates significant performance enhancement over the baseline BERT tagger.",+1.86 in terms of F1 score on CTB5 +1.80 on CTB6 +2.19 on UD1.4,1.0,0.9999999999,0.25,0.8946823728920293
How are weights dynamically adjusted?,"['between hard-negative examples and positive ones. Inspired by the idea of focal loss BIBREF16 in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds.weight dynamically changes as training proceeds. This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples.\n\nCombing both strategies, we observe significant performance boosts on a wide range of data imbalanced NLP tasks.IBREF21 is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss BIBREF16 used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning BIBREF22, example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first.']","Weights are dynamically adjusted based on the training progress, associating each training example with a weight inversely proportional to $(1-p)$. This adjustment strategy, as proposed in the documents 1911.02855 and 1911.02855, aims to deemphasize confident examples as their $p$ value approaches 1, thus making the model pay more attention to hard-negative examples. This method is used to alleviate the dominating effect of easy-negative examples and is observed to boost performance on data imbalanced NLP tasks.","One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.",0.5,0.9999999999,,0.8487321825422215
Ngrams of which length are aligned using PARENT?,"['th human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics.M7 in INLINEFORM8 and INLINEFORM9 , respectively. We use INLINEFORM10 to denote the count of n-gram INLINEFORM11 in INLINEFORM12 , and INLINEFORM13 to denote the minimum of its counts in INLINEFORM14 and INLINEFORM15 . Our goal is to assign a score to the model, which correlates highly with human judgments of the quality of that model.\n\n\n## PARENT\n\nPARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3 .CS, RG and RG-F metrics discussed in § SECREF4 .\n\nText & Table: We compare a variant of BLEU, denoted as BLEU-T, where the values from the table are used as additional references. BLEU-T draws inspiration from iBLEU BIBREF26 but instead rewards n-grams which match the table rather than penalizing them. For PARENT, we compare both the word-overlap model (PARENT-W) and the co-occurrence model (PARENT-C) for determining entailment.']","According to the document 1906.01081, PARENT aligns n-grams of INLINEFORM1 length against INLINEFORM2 and INLINEFORM3.",Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4,,0.9999999999,0.6666666666666666,0.8920012329500641
How many people participated in their evaluation study of table-to-text models?,"['to generate text describing 1-5 RDF triples (e.g. John E Blaha, birthPlace, San Antonio), and human ratings were collected for the outputs of 9 participating systems on 223 instances. These systems include a mix of pipelined, statistical and neural methods. Each instance has upto 3 reference texts associated with the RDF triples, which we use for evaluation.system does not align records to the input text.\n\n\n## Experiments & Results\n\nIn this section we compare several automatic evaluation metrics by checking their correlation with the scores assigned by humans to table-to-text models. Specifically, given INLINEFORM0 models INLINEFORM1 , and their outputs on an evaluation set, we show these generated texts to humans to judge their quality, and obtain aggregated human evaluation scores for all the models, INLINEFORM2 (§ SECREF33 ).nstructed and contains many divergent references. In § SECREF47 we also present results on the data released as part of the WebNLG challenge.\n\nWe developed several models of varying quality for generating text from the tables in WikiBio. This gives us a diverse set of outputs to evaluate the automatic metrics on. Table TABREF32 lists the models along with their hyperparameter settings and their scores from the human evaluation (§ SECREF33 ).']","According to the document 1906.01081, human ratings were collected for the outputs of 9 participating systems on 223 instances.",about 500,0.0,,,0.769041620074644
By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?,"['th human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics.nts better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.ments as humans between pairs of generated texts. Both variants of PARENT are significantly better than the other metrics, however the best accuracy is only INLINEFORM0 for the binary task. This is a challenging task, since there are typically only subtle differences between the texts.']","According to the document 1906.01081, PARENT correlates with human judgments better than existing text generation metrics. However, the exact degree of improvement is not specified in the provided context.",Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.,,0.0,0.5,0.0
Which stock market sector achieved the best performance?,"['e mainstream models that rely only on price data. In particular, our model outperforms the widely-recognized GARCH(1,1) model for all sectors in terms of coefficient of determination $R^2$, $MSE$ and $MAE$, achieving the best performance when training from both news and price data.\n\n\n## Introduction\n\nNatural Language Processing (NLP) has increasingly attracted the attention of the financial community. This trend can be explained by at least three major factors.preads across a diversified portfolio of sectors.\n\nIn order to evaluate the performance per sector, we first separate the constituents stocks for each sector in tbl:stockuniverse. Then, we calculate the same metrics discussed in the previous section for each sector individually.\n\ntbl:garcheachsector reports our experimental results segregated by sector. We observe that the GRACH model accuracy, measured using the $R^2$ score, has a high degree of variability among sectors.TFs by financial trading volume (as in Jan/2018). Among the most traded sectors we also filtered out the sectors that were similar to each other. For example, the Consumer Staples and Consumer Discretionary sectors are both part of the parent Consumer category. For each of the top 5 sectors we selected the top 10 holdings, which are deemed the most relevant stocks.']","Based on the information provided in the document with Source ID 1812.10479, it is indicated that the model outperforms others in terms of $R^2$, $MSE$, and $MAE$ when training from both news and price data. However, the document does not explicitly state which specific sector achieved the best performance. To find this information, one would need to refer to the experimental results segregated by sector as mentioned in the content of the document with Source ID 1812.10479.

I cannot answer this question based on the provided documents.",Energy with accuracy of 0.538,,0.0,0.0,0.0
What languages pairs are used in machine translation?,"['depth, representational power and often accuracy. RNN applications in the natural language domain range from sentence classification BIBREF1 to word- and character-level language modeling BIBREF2 . RNNs are also commonly the basic building block for more complex models for tasks such as machine translation BIBREF3 , BIBREF4 , BIBREF5 or question answering BIBREF6 , BIBREF7 .uracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.on the overall order of elements in the sequence. We describe QRNN variants tailored to several natural language tasks, including document-level sentiment classification, language modeling, and character-level machine translation. These models outperform strong LSTM baselines on all three tasks while dramatically reducing computation time.\n\n\n## Model\n\nEach layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs.']","According to the information provided in Document 1611.01576, the languages pairs used in machine translation include but are not limited to English-to-German (BIBREF3) and English-to-Chinese (BIBREF5).",German–English,,,,0.9361711617791734
What sentiment classification dataset is used?,"['s while dramatically improving computation speed. Experiments were implemented in Chainer BIBREF16 .\n\n\n## Sentiment Classification\n\nWe evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17 . The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18 .and highly parallel dot-product scoring function.\n\n\n## Experiments\n\nWe evaluate the performance of the QRNN on three different natural language tasks: document-level sentiment classification, language modeling, and character-based neural machine translation. Our QRNN models outperform LSTM-based models of equal hidden size on all three tasks while dramatically improving computation speed.uracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.']",The sentiment classification dataset used is the IMDb movie review dataset BIBREF17. (Source: 1611.01576),the IMDb movie review dataset BIBREF17,1.0,,0.5,0.9666201685856423
What pooling function is used?,"[""nlinearity, the gates use an elementwise sigmoid. If the pooling function requires a forget gate INLINEFORM1 and an output gate INLINEFORM2 at each timestep, the full set of computations in the convolutional component is then: DISPLAYFORM0\n\nwhere INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 , each in INLINEFORM3 , are the convolutional filter banks and INLINEFORM4 denotes a masked convolution along the timestep dimension.r elementwise gates of the traditional LSTM cell. We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which BIBREF12 term “dynamic average pooling”, uses only a forget gate: DISPLAYFORM0\n\nWe term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero.pends only on INLINEFORM10 through INLINEFORM11 . This concept, known as a masked convolution BIBREF11 , is implemented by padding the input to the left by the convolution's filter size minus one.\n\nWe apply additional convolutions with separate filter banks to obtain sequences of vectors for the elementwise gates that are needed for the pooling function. While the candidate vectors are passed through a INLINEFORM0 nonlinearity, the gates use an elementwise sigmoid.""]","The documents describe three types of pooling functions: f-pooling, fo-pooling, and ifo-pooling. The simplest option, f-pooling, uses only a forget gate, initialized to zero. The other two types involve additional convolutions with separate filter banks for the elementwise gates. The specific implementation details are found in Source: 1611.01576.",dynamic average pooling,1.0,0.9999999999,0.16666666666666666,0.891543853655305
What neural network modules are included in NeuronBlocks?,"[', and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by saving their learning cost and guiding them to find optimal solutions to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture.to construct networks with complex architecture. By following the interface guidelines, users can also contribute to this gallery of components with their own modules.\n\nThe technical contributions of NeuronBlocks are summarized into the following three aspects.\n\n\n## Related Work\n\nThere are several general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, which have gained popularity in NLP community.covers as many commonly used modules as possible. The Block Zoo is an open framework, and more modules can be added in the future.\n\n[itemsep= -0.4em,topsep = 0.3em, align=left, labelsep=-0.6em, leftmargin=1.2em]\n\nEmbedding Layer: Word/character embedding and extra handcrafted feature embedding such as pos-tagging are supported.\n\nNeural Network Layers: Block zoo provides common layers like RNN, CNN, QRNN BIBREF2 , Transformer BIBREF3 , Highway network, Encoder Decoder architecture, etc.']","According to the document 1904.09535, NeuronBlocks includes common neural network layers such as RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, and Encoder Decoder architecture. It also supports embedding layers like Word/character embedding and extra handcrafted feature embedding such as pos-tagging.",Embedding Layer Neural Network Layers Loss Function Metrics,,0.9999999999,1.0,0.8790705427895203
"How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?","['Title: NeuronBlocks: Building Your NLP DNN Models Like Playing Lego\n\nAbstract: Deep Neural Networks (DNN) have been widely employed in industry to address various Natural Language Processing (NLP) tasks. However, many engineers find it a big overhead when they have to choose from multiple frameworks, compare different types of models, and understand various optimization mechanisms.s, which have gained popularity in NLP community. These frameworks offer huge flexibility in DNN model design and support various NLP tasks. However, building models under these frameworks requires a large overhead of mastering these framework details. Therefore, higher level abstraction to hide the framework details is favored by many engineers.\n\nThere are also several popular deep learning toolkits in NLP, including OpenNMT BIBREF0 , AllenNLP BIBREF1 etc., and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by saving their learning cost and guiding them to find optimal solutions to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture.']","The authors evidence this claim by introducing NeuronBlocks, a toolkit that encapsulates a suite of neural network modules as building blocks to construct various DNN models with complex architecture. This toolkit is designed to reduce the learning cost and complexity associated with mastering multiple frameworks, models, and optimization mechanisms, as indicated in the document (Source: 1904.09535).",By conducting a survey among engineers,,0.0,,0.897572061493083
what datasets did they use?,"['using another dataset containing only stop words. At second step, we work without eliminating the stop words from the text which gives better result than the first setup.\n\n\n## Implementation of the System ::: Classification Algorithms ::: Multi Layer Perceptron (MLP)\n\nMLP contains three layers - an input layer, an output layer and some hidden layers.eliminating stop words while extracting features. Figure FIGREF21 shows the average results of different classifiers in a bar chart with and without eliminating stop words from the questions.\n\nOverall, SGD has shown the best performance on our dataset as it introduces non-linearity and uses back-propagation for updating parameter weights using loss function calculated on training set into classification.seball league in America for a particular season. LUNAR BIBREF5 system answers questions about soil samples taken from Apollo lunar exploration. Some of the most popular QA Systems are IBM Watson, Apple Siri and Wolfram Alpha. Examples of some QA systems based on different languages are: Zhang Yu Chinese question classification BIBREF6 based on Incremental Modified Bayes, Arabic QA system (AQAS) BIBREF7 by F. A. Mohammed, K. Nasser, & H. M.']","The documents mention the use of a dataset containing only stop words (Source: 1911.03059). Another dataset is used where stop words are not eliminated during feature extraction (Source: 1911.03059). Additionally, a dataset related to a seball league in America for a particular season and soil samples from Apollo lunar exploration are used (Source: 1911.03059, Source: 1911.03059 respectively).","Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.",,,0.5,0.8629519603336441
what ml based approaches were compared?,"['glish language such as IBM Watson, Wolfram Alpha. Bengali speakers often fall in difficulty while communicating in English BIBREF1.\n\nIn this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions tto an appropriate answer type BIBREF14 BIBREF15. Machine Learning based approaches have been used by Zhang et al and Md. Aminul Islam et al in BIBREF16 and BIBREF0. Many classifiers have been used in machine learning for QC such as Support Vector Machine (SVM) BIBREF16 BIBREF17, Support Vector Machines and Maximum Entropy Model BIBREF18, Naive Bayes (NB), Kernel Naive Bayes (KNB), Decision Tree (DT) and Rule Induction (RI) BIBREF13.n calculated on training set into classification. K-NN has shown the weakest performance overall, as this algorithm has a bad reputation of not working well in high dimensional data BIBREF28. MLP and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition. SVM accomplishes this same feat by taking the samples to a higher dimensional hyperplane where the samples are linearly separable.']","According to the document 1911.03059, seven machine learning based classifiers were compared in classifying Bengali questions. These include Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN), and Random Forest (RF).","Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)",1.0,,0.0,0.8171876251699461
What parallel corpus did they use?,"['ward translation and back-translation on testing. In this paper, however, we do not use a beam search for simplicity and effectiveness.\n\n\n## Experiments\n\nWe evaluated the encoder-decoder-reconstructor framework for NMT on English-Japanese and Japanese-English translation tasks.\n\n\n## Datasets\n\nWe used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 .REF0 and NTCIR PatentMT Parallel Corpus BIBREF1 . Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity. Japanese sentences were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus.ce agreement between source and target sentences.\n\nIn order to confirm the language independence of the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor. Our experiments show that their method offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, though the difference is not significant on Japanese-English tra']","They used the Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1, as stated in the document with the Source ID 1706.08198.",Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 NTCIR PatentMT Parallel Corpus BIBREF1,1.0,0.9999999999,0.6666666666666666,0.8157435988622164
How much does their model outperform existing models?,"['ns, at the confidence level 0.01 ( INLINEFORM0 ). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work.ning rate INLINEFORM0 and a drop out rate of 0.3. We use a mini-batch with a batch size of 32 documents, and the size of the GRU hidden states is 300. For word embeddings, we use GloVe BIBREF26 with dimension 300, pre-trained on the Wikipedia and Gigaword. The vocabulary size of our model is 50000. All the above parameters were set based on BIBREF18 without any fine-tuning.er, with another uni-directional LSTM as decoder. To decrease the number of parameters while maintaining the accuracy, summarunner present SummaRuNNer, a simple RNN-based sequence classifier without decoder, outperforming or matching the model of BIBREF2 . They take content, salience, novelty, and position of each sentence into consideration when deciding if a sentence should be included in the extractive summary. Yet, they do not capture any aspect of the topical structure, as we do in this paper.']","The documents do not provide specific quantitative information on how much the model outperforms existing models. However, they mention that the neural extractive models outperform traditional extractive models on informativeness, as measured by ROUGE-1 and ROUGE-2, but the results are mixed on ROUGE-L. The documents suggest further exploration is needed for optimization and comparison with existing models. (Source: 1909.08089)","Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)",0.0,0.0,,0.0
What do they mean by global and local context?,"['gment information and global context information.\n\nAttentive context As local context and global context are all contextual information of the given sentence, we use an attention mechanism to decide the weight of each context vector, represented as INLINEFORM0\n\nwhere the INLINEFORM0 is the weighted context vector of each sentence INLINEFORM1 , and assume sentence INLINEFORM2 is in topic INLINEFORM3 .Title: Extractive Summarization of Long Documents by Combining Global and Local Context\n\nAbstract: In this paper, we propose a novel neural single document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores.ith the Bonferroni correction at ( INLINEFORM0 ).\n\nFrom these tables, we can see that on both datasets the performance significantly improves when local topic information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance.']","According to the document 1909.08089, global context refers to the representation of the whole document, while local context refers to the local context within the current topic. The paper found that incorporating local topic information significantly improves performance, especially for long documents, whereas adding a representation of the whole document (global context) does not significantly enhance performance.","global (the whole document) local context (e.g., the section/topic)",1.0,0.9999999999,,0.8717391815571863
What are the 18 propaganda techniques?,"['18 propagandist techniques at the fragment level. We hope that the corpus would raise interest outside of the community of researchers studying propaganda. For example, the techniques related to fallacies and the ones relying on emotions might provide a novel setting for researchers interested in Argumentation and Sentiment Analysis.\n\n\n## Related Work\n\nPropaganda has been tackled mostly at the article level. BIBREF3 created a corpus of news articles labelled as propaganda, trusted, hoax, or satire.d rhetorical techniques to achieve its objective. Such techniques include the use of logical fallacies and appeal to emotions. For the shared task, we use 18 techniques that can be found in news articles and can be judged intrinsically, without the need to retrieve supporting information from external resources. We refer the reader to BIBREF10 for more details on the propaganda techniques; below we report the list of techniques:\n\n\n## Propaganda Techniques ::: 1.ok for the use of specific propaganda techniques. Going at this fine-grained level can yield more reliable systems and it also makes it possible to explain to the user why an article was judged as propagandist by an automatic system.\n\nWith this in mind, we organised the shared task on fine-grained propaganda detection at the NLP4IF@EMNLP-IJCNLP 2019 workshop. The task is based on a corpus of news articles annotated with an inventory of 18 propagandist techniques at the fragment level.']",I cannot answer this question based on the provided documents.,"Loaded language Name calling or labeling Repetition Exaggeration or minimization Doubt Appeal to fear/prejudice Flag-waving Causal oversimplification Slogans  Appeal to authority Black-and-white fallacy, dictatorship Thought-terminating cliché Whataboutism Reductio ad Hitlerum Red herring Bandwagon Obfuscation, intentional vagueness, confusion Straw man",0.0,0.0,0.0,0.0
What dataset was used?,"['g the dataset by dropping some negative examples.\n\nTeam JUSTDeep BIBREF31 used a combination of models and features, including word embeddings based on GloVe BIBREF32 concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT.\n\nTeam YMJA BIBREF33 also based their approach on fine-tuned BERT.itted systems managed to outperform the baseline. The fragment-level task proved to be much more challenging, with lower absolute scores, but most teams still managed to outperform the baseline.\n\nWe plan to make the schema and the dataset publicly available to be used beyond NLP4IF.resentation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERT$_{BASE}$ and BERT$_{LARGE}$\n\nTeam LIACC BIBREF30 used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples.']","The dataset used was made publicly available for use beyond NLP4IF, as mentioned in the document with the Source ID 1910.09982.",news articles in free-text format,,0.0,,0.8763763543008163
What was the baseline for this task?,"['asure for the task is the standard F$_1$ measure. We further report Precision and Recall.\n\n\n## Baselines\n\nThe baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34.\n\nThe baseline for the FLC task generates spans and selects one of the 18 techniques randomly.itted systems managed to outperform the baseline. The fragment-level task proved to be much more challenging, with lower absolute scores, but most teams still managed to outperform the baseline.\n\nWe plan to make the schema and the dataset publicly available to be used beyond NLP4IF.d task on fine-grained propaganda identification. We received 25 and 12 submissions on the test set for the sentence-level classification and the fragment-level classification tasks, respectively. Overall, the sentence-level task was easier and most submitted systems managed to outperform the baseline.']","The baseline for this task was a very simple logistic regression classifier with default parameters, representing input instances with a single feature: the length of the sentence (1910.09982).",The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.,1.0,0.9999999999,,0.9774826849885834
What is a second order co-ocurrence matrix?,"['emantic knowledge found in a biomedical ontology. Our results show that incorporating semantic similarity into a second order co--occurrence matrices improves correlation with human judgments for both similarity and relatedness, and that our method compares favorably to various different word embedding methods that have recently been evaluated on the same reference standards we have used.mantic similarity scores derived from a taxonomy. There are two stages to our approach. First, a co–occurrence matrix must be constructed. Second, this matrix is used to construct a second–order co–occurrence vector for each concept in a pair of concepts to be measured for relatedness.uch as word2vec BIBREF9 have become very popular. Despite the prominent role that neural networks play in many of these approaches, at their core they remain distributional techniques that typically start with a word by word co–occurrence matrix, much like many of the more traditional approaches.\n\nHowever, despite these successes distributional methods do not perform well when data is very sparse (which is common). One possible solution is to use second–order co–occurrence vectors BIBREF10 , BIBREF11 .']","A second order co-occurrence matrix is a method used in semantic similarity scores derived from a taxonomy, as described in the document with the Source ID of 1609.00559. The process involves two stages: first, a co-occurrence matrix is constructed, and then this matrix is used to construct a second-order co-occurrence vector for each concept in a pair of concepts to measure relatedness. This approach is akin to many traditional methods, which also typically start with a word-by-word co-occurrence matrix. (Source: 1609.00559)","frequencies of the other words which occur with both of them (i.e., second order co–occurrences)",,0.9999999999,,0.9475757593405234
How many humans participated?,"['nd is maintained by National Library of Medicine. The 2015 Medline Baseline encompasses approximately 5,600 journals starting from 1948 and contains 23,343,329 citations, of which 2,579,239 contain abstracts. In this work, we use Medline titles and abstracts from 1975 to present day. Prior to 1975, only 2% of the citations contained an abstract.age System (UMLS) Metathesaurus (version 2016AA). The Metathesaurus contains approximately 2 million biomedical and clinical concepts from over 150 different terminologies that have been semi–automatically integrated into a single source. Concepts in the Metathesaurus are connected largely by two types of hierarchical relations: INLINEFORM0 / INLINEFORM1 (PAR/CHD) and INLINEFORM2 / INLINEFORM3 (RB/RN).coders and three physicians from the Mayo Clinic. The relatedness of each term pair was assessed based on a four point scale: (4.0) practically synonymous, (3.0) related, (2.0) marginally related and (1.0) unrelated. MiniMayoSRS is a subset of the MayoSRS and consists of 30 term pairs on which a higher inter–annotator agreement was achieved. The average correlation between physicians is 0.68. The average correlation between medical coders is 0.78.']","According to the document with the ID 1609.00559, four humans participated: coders and three physicians from the Mayo Clinic.",16,0.0,0.0,0.5,0.920885827453349
What embedding techniques are explored in the paper?,"['s (e.g., BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 ). These methods are all based on the distributional hypothesis, which holds that two terms that are distributionally similar (i.e., used in the same context) will also be semantically similar BIBREF7 , BIBREF8 . Recently word embedding techniques such as word2vec BIBREF9 have become very popular.s do various different word embedding approaches. However, a direct comparison was not possible due to variations in the subsets of the UMNSRS evaluation dataset used. In the future, we would not only like to conduct a direct comparison but also explore integrating semantic similarity into various kinds of word embeddings by training on pair–wise values of semantic similarity as well as co–occurrence statistics.high varies depending on the dataset and measure.\n\n\n## Comparison with Previous Work\n\nRecently, word embeddings BIBREF9 have become a popular method for measuring semantic relatedness in the biomedical domain. This is a neural network based approach that learns a representation of a word by word co–occurrence matrix.']","The paper explores various word embedding approaches, including word2vec (BIBREF9). The authors also express interest in integrating semantic similarity into these embeddings and conducting a direct comparison of these techniques in the future. (Source: 1609.00559)",Skip–gram CBOW,,0.0,0.8,0.8854192423591366
What word level and character level model baselines are used?,"['mbeddings for questions, entities and predicates.\n\nBoth word-level and character-level models perform comparably well when predicting the predicate, reaching an accuracy of around 80% (Table 3). However, the word-level model has considerable difficulty generalizing to unseen entities, and is only able to predict 45% of the entities accurately from the mixed set.hen they encounter unseen words during test time. Consequently, they often rely on significant data augmentation from sources such as Paralex BIBREF2 , which contains 18 million question-paraphrase pairs scraped from WikiAnswers, to have sufficient examples for each word they encounter BIBREF11 , BIBREF1 , BIBREF0 .\n\nAs opposed to word-level modeling, character-level modeling can be used to handle the OOV issue.both the topic entity and predicate are correct). Our single character-level model achieves SQ accuracies of 70.9% and 70.3% on the FB2M and FB5M settings, outperforming the previous state-of-art results by 8.2% and 6.4%, respectively. Compared to the character-level model, which only has 1.2M parameters, our word-level model has 19.9M parameters, and only achieves a best SQ accuracy of 53.9%.']","According to the document 1604.00727, both word-level and character-level models are used as baselines. The word-level model reaches an accuracy of around 80% for predicting the predicate, while the character-level model achieves SQ accuracies of 70.9% and 70.3% on the FB2M and FB5M settings, respectively.",,,,0.75,0.8789613680250316
How were the human judgements assembled?,"['used in the Tensorflow BIBREF11 seq2seq library.\n\n\n## Results and Discussion\n\nThe BLEU metric parameters (modified 1-gram, 2-gram, 3-gram and 4-gram precision values) and human evaluation metrics of adequacy, fluency and relative ranking values were used to evaluate the performance of the models.\n\n\n## BLEU Evaluation\n\nThe BLEU scores obtained using the various models used in the experiment are tabulated in Table TABREF25 .split up into various modules as expanded in Fig. FIGREF17 .\n\n\n## Bucketing\n\nThe input source and target language sentences used for training were taken and divided into bucketed pairs of sentences of a fixed number of sizes. This relationship was determined by examining the distribution of words in the corpus primarily to minimize the number of PAD tokens in the sentence.the two translations to have ties in the ranking. The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12 , BIBREF13 .\n\nThe human evaluation Kappa co-efficient results are calculated with respect to: DISPLAYFORM0\n\nIt was observed that the ranking Kappa co-efficient for intra-annotator ranking of the RNNMorph model was at 0.573, higher that the 0.410 of the RNNSearch+Word2Vec model, implying that the annotators found the RNNMorph model to produce better']","The human judgements were assembled by computing intra-annotator values for certain metrics, as shown in Table TABREF32 (Source: 1612.02482). The process involved evaluating the ranking produced by different models, with the Kappa co-efficient used to measure the consistency of the rankings among different annotators. The higher Kappa co-efficient for the RNNMorph model (0.573) compared to the RNNSearch+Word2Vec model (0.410) indicates that the annotators found the RNNMorph model to produce better rankings.","50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.",,,0.7142857142857143,0.9055933236086832
Which other approaches do they compare their model with?,"['ntity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes. We can see comparisons of our model made on Wiki(gold) in Table TABREF20 . We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset.omes a limitation for supervised learning models. The primary reason being the lack of datasets where entity boundaries are properly annotated, whilst covering a large spectrum of entity types. Furthermore, many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata).score is the variant which is most commonly used. This is defined as: DISPLAYFORM0\n\nSince most NER systems involve multiple entity types, it is often required to assess the performance across all entity classes. Two measures are commonly used for this purpose: the macro-averaged F-1 score and the micro-averaged F-1 score. The macro-averaged F-1 score computes the F-1 score independently for each entity type, then takes the average (hence treating all entity types equally).']","According to the document 1904.10503, their model's performance is compared to existing systems, specifically those trained or tuned on the Wiki(gold) dataset, using the macro-averaged F-1 score.","Akbik et al. (2018), Link et al. (2012)",0.0,0.0,0.0,0.8506887510106105
What results do they achieve using their proposed approach?,"['score is the variant which is most commonly used. This is defined as: DISPLAYFORM0\n\nSince most NER systems involve multiple entity types, it is often required to assess the performance across all entity classes. Two measures are commonly used for this purpose: the macro-averaged F-1 score and the micro-averaged F-1 score. The macro-averaged F-1 score computes the F-1 score independently for each entity type, then takes the average (hence treating all entity types equally).ories such as person, location, organization etc. NER serves as the basis for a variety of natural language processing (NLP) applications such as relation extraction BIBREF4 , machine translation BIBREF5 , question answering BIBREF6 and knowledge base construction BIBREF7 .eing trained or tuned on that particular dataset. Future work may include refining the clustering method described in Section 2.2 to extend to types other than person, location, organization, and also to include disambiguation of entity types.']","According to the document 1904.10503, their proposed approach results in achieving a macro-averaged F-1 score, which treats all entity types equally. This approach is used to assess the performance across multiple entity classes in Named Entity Recognition (NER) systems. (Source: 1904.10503)","F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).",0.0,0.0,,0.8033218891814418
How do they combine a deep learning model with a knowledge base?,"['omes a limitation for supervised learning models. The primary reason being the lack of datasets where entity boundaries are properly annotated, whilst covering a large spectrum of entity types. Furthermore, many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata).eparate tasks, whereas we try to combine the two. It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 .\n\nRedirection: For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity.BIBREF6 and knowledge base construction BIBREF7 . Although early NER systems have been successful in producing adequate recognition accuracy, they often require significant human effort in carefully designing rules or features.\n\nIn recent years, deep learning methods been employed in NER systems, yielding state-of-the-art performance. However, the number of types detected are still not sufficient for certain domain-specific applications.']","According to the source document 1904.10503, they combine a deep learning model, specifically ELMo, with an expansive knowledge base, Wikidata, to increase the semantic information carried by a Named Entity Recognition (NER) system. The process involves linking entities from the deep learning method to related entities from the knowledge base. This is done by recognizing that the lookup will be constrained by the most common lookup name for each entity.",Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.,,,1.0,0.8096715074849183
What are the models used for the baseline of the three NLP tasks?,"['r code-switching and non-standard pronunciations. We also provide baseline results on three core NLP tasks: speech recognition, speech synthesis, and machine translation between Spanish and Mapudungun. We further explore other applications for which the corpus will be suitable, including the study of code-switching, historical orthography change, linguistic structure, and sociological and anthropological studies.nificant for the creation of community resources.\n\nWith this work we make publicly available a large corpus in Mapudungun, a language of the indigenous Mapuche people of southern Chile and western Argentina. We hope to ameliorate the resource gap and the transcription bottleneck in two ways. First, we are providing a larger data set than has previously been available, and second, we are providing baselines for NLP tasks (speech recognition, speech synthesis, and machine translation).tion, speech synthesis, and machine translation). In providing baselines and datasets splits, we hope to further facilitate research on low-resource NLP for this language through our data set. Research on low-resource speech recognition is particularly important in relieving the transcription bottleneck, while tackling the research challenges that speech synthesis and machine translation pose for such languages could lead to such systems being deployed to serve more under-represented communities.']","The models used for the baseline of the three NLP tasks are speech recognition, speech synthesis, and machine translation between Spanish and Mapudungun. (Source: 1912.01772)",state-of-the-art Transformer architecture Kaldi speech clustergen statistical speech synthesizer,,0.0,1.0,0.9999999999999991
How is non-standard pronunciation identified?,"['r code-switching and non-standard pronunciations. We also provide baseline results on three core NLP tasks: speech recognition, speech synthesis, and machine translation between Spanish and Mapudungun. We further explore other applications for which the corpus will be suitable, including the study of code-switching, historical orthography change, linguistic structure, and sociological and anthropological studies.s, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such.\n\n\n## The Resource ::: Cleaning\n\nThe dialogues were originally recorded using a Sony DAT recorder (48kHz), model TCD-D8, and Sony digital stereo microphone, model ECM-DS70P.es an MCD of 5.551, which is a large improvement. The quality of the generated speech goes from understandable, only if you can see the text, to understandable, and transcribable even for non-Mapudungun speakers. We do not believe we are building the best synthesizer with our current (non-neural) techniques, but we do believe we are selecting the best training data for other statistical and neural training techniques in both speech synthesis and speech recognition.']","Non-standard pronunciation is identified in the document 1912.01772, where it is mentioned that ""Foreign words, in this case Spanish words, are also labelled as such."" This suggests that non-standard pronunciations, particularly of foreign words, are marked and likely catalogued in the corpus for further analysis.",Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.,,0.0,,0.9171597516189558
What novel PMI variants are introduced?,"['ributes almost exclusively syntactic information. Our findings deepen our understanding of distributional semantics, while also introducing novel $PMI$ variants and grounding the popular $PPMI$ measure.\n\n\n## Introduction\n\nDense word vectors (or embeddings) are a key component in modern NLP architectures for tasks such as sentiment analysis, parsing, and machine translation.NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:\n\nWe also experimented with Laplace smoothing as in BIBREF18 for various pseudocounts but found it to work consistently worse than both $\\mathit {CPMI_z}$ and $\\mathit {NNrnative weightings for word-context cooccurrence. In the field of collocation extraction, BIBREF7 address the negative infinity issue with $\\mathit {PMI}$ by introducing the normalized $\\mathit {PMI}$ metric. BIBREF8 show theoretically that the popular Skip-gram model BIBREF9 performs implicit factorization of shifted $\\mathit {PMI}$.\n\nRecently, work in explicit low-rank matrix factorization of $\\mathit {PMI}$ variants has achieved state of the art results in word embedding.']","According to the document 1908.06941, novel PMI variants such as $NNEGPMI$ and $CPMI_z$ are introduced. $NNEGPMI$ is a variant that only normalizes the negative part of PMI, while $CPMI_z$ is not explicitly described in the provided context.",clipped PMI; NNEGPMI,,0.9999999999,1.0,0.9360491907364419
What semantic and syntactic tasks are used as probes?,"[""ctorization of different $\\mathit{PMI}$ matrices. Using various semantic and syntactic tasks as probes into models which use either negative or positive $\\mathit{PMI}$ (or both), we find that most of the encoded semantics and syntax come from positive $\\mathit{PMI}$, in contrast to $\\mathit{\\texttt{-} PMI}$ which contributes almost exclusively syntactic information.# Results\n\nAll results are shown in tab:senteval.\n\nNegative PMI: We observe that using only $\\mathit {\\texttt {-}PMI}$ (rows $\\mathit {\\texttt {-}CPMI_{\\texttt {-}2}}$ and $\\mathit {\\texttt {-}NNEGPMI}$) performs similarly to all other models in POS tagging and both syntactic probing tasks, but very poorly on all semantic tasks, strongly supporting our main claim that $\\mathit {\\texttt {-}PMI}$ mostly encodes syntactic information.e sentence representation) from SentEval BIBREF22.\n\nSyntax: Similarly, we use the Google Syntactic analogies (GSyn) BIBREF9 to evaluate word-level syntactic information, and Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks from SentEval BIBREF22 for sentence-level syntax. Classifiers for all SentEval probing tasks are multilayer perceptrons with a single hidden layer of 100 units and dropout of $.1$.""]","According to the document 1908.06941, various semantic and syntactic tasks are used as probes. These include Google Syntactic analogies (GSyn) BIBREF9 for word-level syntactic information, and Depth (Dep) and Top Constituent (TopC) probing tasks from SentEval BIBREF22 for sentence-level syntax.",Word Content (WC) probing task Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks,,,,0.8798507632303899
What are the disadvantages to clipping negative PMI?,"['BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16. However, it is not clear what is lost by clipping the negative spectrum of $\\mathit {PMI}$, which makes the use of $\\mathit {PPMI}$, though it works well in practice, seem unprincipled.\n\nIn the study of language acquisition, BIBREF17 argue that indirect negative evidence might play an important role in human acquisition of grammar, but do not link this idea to distributional semantics.rpora which lead to a large number of such pairs. A common practice is to clip negative $\\mathit{PMI}$ ($\\mathit{\\texttt{-} PMI}$) at $0$, also known as Positive $\\mathit{PMI}$ ($\\mathit{PPMI}$). In this paper, we investigate alternative ways of dealing with $\\mathit{\\texttt{-} PMI}$ and, more importantly, study the role that negative information plays in the performance of a low-rank, weighted factorization of different $\\mathit{PMI}$ matrices.a low-rank factorization to obtain dense vectors.\n\nUnfortunately, $\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora. Many models work around this issue by clipping negative $\\mathit {PMI}$ values at 0, a measure known as Positive $\\mathit {PMI}$ ($\\mathit {PPMI}$), which works very well in practice.']","According to the document 1908.06941, clipping negative PMI, which is known as Positive PMI (PPMI), can lead to negative infinity values for word-context pairs that do not appear in the training corpus. This is due to unreliable statistics, which often happens in finite corpora. This issue suggests that clipping negative PMI may not always be a reliable method as it can result in inaccurate values for certain word-context pairs. (Source: 1908.06941)",It may lead to poor rare word representations and word analogies.,0.0,,0.375,0.9113632054386721
Why are statistics from finite corpora unreliable?,"['a low-rank factorization to obtain dense vectors.\n\nUnfortunately, $\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora. Many models work around this issue by clipping negative $\\mathit {PMI}$ values at 0, a measure known as Positive $\\mathit {PMI}$ ($\\mathit {PPMI}$), which works very well in practice.Title: Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics\n\nAbstract: In distributional semantics, the pointwise mutual information ($\\mathit{PMI}$) weighting of the cooccurrence matrix performs far better than raw counts. There is, however, an issue with unobserved pair cooccurrences as $\\mathit{PMI}$ goes to negative infinity. This problem is aggravated by unreliable statistics from finite corpora which lead to a large number of such pairs.irical results shows it outperforms SVD BIBREF15. b) The weighting of reconstruction errors by statistical confidence is particularly important for $\\mathit {\\texttt {-}PMI}$, where negative cooccurrence between a pair of frequent words is more significant and should be better approximated than that between a pair of rare words.']","According to the document 1908.06941, statistics from finite corpora are unreliable because they often lead to a large number of unobserved pair cooccurrences, causing the pointwise mutual information (PMI) to go to negative infinity. This issue is particularly problematic in distributional semantics, where PMI performs better than raw counts. The document highlights the importance of statistical confidence when weighting reconstruction errors, especially for -PMI, where negative cooccurrence between frequent words is more significant and should be better approximated than that between rare words. (Source: 1908.06941)","$\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus",1.0,,0.6666666666666666,0.9507299321440261
what is the domain of the corpus?,"['LLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model.es B and C, leading to a common assignment to c2. Such inconsistencies in turn affect the translation distribution conditioned on c2.\n\nMany of these errors are due to cross-speaker matches, which are known to be more challenging for UTD BIBREF16 , BIBREF17 , BIBREF18 . Most matches in our corpus are across calls, yet these are also the least accurate (Table TABREF9 ).ipts and their crowdsourced English translations. The transcripts contain 168,195 Spanish word tokens (10,674 types), and the translations contain 159,777 English word tokens (6,723 types). Though our system does not require Spanish transcripts, we use them to evaluate UTD and to simulate a perfect UTD system, called the oracle.\n\nFor MT training, we use the pseudotext and translations of 50 calls, and we filter out stopwords in the translations with NLTK BIBREF15 .']","The domain of the corpus is Spanish-English speech translation, as stated in the source document 1702.03856.",telephone calls,0.0,0.0,0.5,0.921542932690382
what challenges are identified?,"['el text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.\n\n\n## Introduction\n\nTypical speech-to-text translation systems pipeline automatic speech recognition (ASR) and machine translation (MT) BIBREF0 .ls in a variety of Spanish dialects (§ SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (§ SECREF4 ).kers are included in both training and test data. Although the utterance-level split is optimistic, it allows us to investigate how multiple speakers affect system performance. In either case, the oracle has about 38k Spanish tokens to train on.\n\n\n## Analysis of challenges from UTD\n\nOur system relies on the pseudotext produced by ZRTools (the only freely available UTD system we are aware of), which presents several challenges for MT.']","The identified challenges include low coverage of audio and difficulty in cross-speaker clustering (Source: 1702.03856). The system also faces issues due to the use of pseudotext produced by ZRTools (Source: 1702.03856). Additionally, there is an issue with the oracle having about 38k Spanish tokens to train on (Source: 1702.03856).","Assigning wrong words to a cluster Splitting words across different clusters sparse, giving low coverage",0.3333333333333333,0.9999999999,0.75,0.8843683033719206
what is the size of the speech corpus?,"['LLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model.fair since the UTD has access to only the audio.\n\n\n## Dataset\n\nAlthough we did not have access to a low-resource dataset, there is a corpus of noisy multi-speaker speech that simulates many of the conditions we expect to find in our motivating applications: the CALLHOME Spanish–English speech translation dataset (LDC2014T23; Post el al., 2013). We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations.kers are included in both training and test data. Although the utterance-level split is optimistic, it allows us to investigate how multiple speakers affect system performance. In either case, the oracle has about 38k Spanish tokens to train on.\n\n\n## Analysis of challenges from UTD\n\nOur system relies on the pseudotext produced by ZRTools (the only freely available UTD system we are aware of), which presents several challenges for MT.']","The speech corpus contains approximately 38,000 Spanish tokens, as stated in the document 1702.03856.","104 telephone calls transcripts contain 168,195 Spanish word tokens  translations contain 159,777 English word tokens",0.0,0.0,,0.8614562092662209
Which two pairs of ERPs from the literature benefit from joint training?,"['has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships.can suggest how components relate to each other. At present, these joint-training benefit relationships are only suggestive, but if these relationships ultimately lead to insights about what drives each ERP component, then the components become more useful tools for studying human language comprehension. By using multitask learning as a method of characterization, we have found some expected relationships (LAN+P600 and ELAN+P600) and several more surprising relationships.ata and the ERP signals is qualitatively similar. Finally, TABREF25 shows the Pearson correlation coefficient between different measures. We note that the patterns of correlation are different than the patterns of which measures benefit from joint training with each other.']","According to the document 1904.01548, the two pairs of ERPs previously identified in the literature that benefit from joint training are LAN+P600 and ELAN+P600.","Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP",0.0,0.9999999999,0.5,0.970855846541267
What datasets are used?,"['in particular between behavioral and neural data. Given the small datasets associated with human language processing, using heterogeneous data is a potentially major advantage of a multitask approach. In future work, we will further explore what information is encoded into the model representations when neural and behavioral data are used to train neural networks, and how these representations differ from the representations in a model trained on language alone.+P600) and several more surprising relationships. We believe that this is exactly the kind of finding that makes multitask learning an interesting exploratory technique in this area. Additionally, we have shown that information can be shared between heterogeneous types of data (eye-tracking, self-paced reading, and ERP components) in the domain of human language processing prediction, and in particular between behavioral and neural data.Title: Understanding language-elicited EEG data by predicting it from a fine-tuned language model\n\nAbstract: Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension.']","The datasets used include human language processing, eye-tracking, self-paced reading, and EEG components. This information is sourced from document 1904.01548.","Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)",0.0,,0.8,0.8879417979044911
which datasets did they experiment with?,"['r the preceding word and the tag(s) provided by the external lexicon for the following word.\n\nIn order to select the best performing feature set, we carried out a series of experiments using the multilingual dataset provided during the SPMRL parsing shared task BIBREF42 .sented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse.\n\n\n## Lexicons\n\nWe generate our external lexicons using the set of source lexicons listed in Table TABREF3 . Since external lexical information is exploited via features, there is no need for the external lexicons and the annotated corpora to use the same PoS inventory.s) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English).']","According to the document 1606.03676, the experiments were carried out using a multilingual dataset provided during the SPMRL parsing shared task BIBREF42.","Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish",0.0,0.0,,0.8783907426727469
which languages are explored?,"['sented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse.\n\n\n## Lexicons\n\nWe generate our external lexicons using the set of source lexicons listed in Table TABREF3 . Since external lexical information is exploited via features, there is no need for the external lexicons and the annotated corpora to use the same PoS inventory.disposal, and for which BIBREF20 provide results. We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian).e lexicon often provides a list of possible tags. Moreover, tagging accuracy for out-of-vocabulary (OOV) words is improved, as a result of the fact that words unknown to the training corpus might be known to the external lexicon.\n\nDespite a few experiments published with MElt on languages other than French BIBREF12 , BIBREF40 , BIBREF41 , the original feature set used by MElt (standard and lexical features) was designed and tested mostly on this language, by building and evaluating tagging models on a varia']","According to the document with the ID 1606.03676, the languages explored include Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.","Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish",1.0,,,0.8799556100088951
Which dataset do they use?,"['guage processing and network science literatures. The approach helps to better exploit information found in textual data, and to locate dense clusters in complex and multilayered networks in ways that were previously not computationally possible. Drawing on a new dataset of state speeches in the UN General Debate, together with voting data from the UNGA, these tools were employed to better estimate revealed state preferences in international politics and to locate preference affinity blocs which exist acrosifferent communities have dissimilar preferences.\n\n\n## UN votes and speeches\n\nThe most widely used source for deriving preferences in IR is UN roll call data BIBREF10 . Voting behavior represent a valuable source of revealed preference information, comparable across states and over time.rly reveal the hostility between the two nations. Both Greece and Turkey discussed the Cyprus invasion at length in their speeches, with each blaming the crisis on the other.\n\nWe draw on a recently released corpus of state speeches delivered during the annual UN General Debate that provides the first dataset of textual output from states that is recorded at regular time-series intervals and includes a sample of all countries that deliver speeches BIBREF11 .']","They use the dataset of state speeches delivered during the annual UN General Debate, specifically referred to as BIBREF11. (Source: 1802.00396)",corpus of state speeches delivered during the annual UN General Debate,,,,0.8821074456259125
What kind of celebrities do they obtain tweets from?,"[""ing a dataset of tweets from fifteen celebrities. Such analysis is useful for generating excerpts of high-volume Twitter profiles, and understanding how representativeness relates to tweet popularity. We first consider the related task of binary author detection (is x the author of text T?), and report a test accuracy of 90.37% for the best of five approaches to this problem. We then use these models to compute characterization scores among all of an author's texts.01-Jan-2008 to 01-Dec-2018, as positive examples. Properties of these Twitter celebrities are provided in Table TABREF10.\n\nNegative examples: We have collected 1% of tweets from Twitter's daily feed using the Twitter API (https://developer.twitter.com/en/docs.html) to use as negative examples.\n\nPreprocessing and Filtering: We have preprocessed and filtered the data to remove tweets that are unrepresentative or too short for analysis.90.37% over a dataset of 15 Twitter celebrities. We use the best performing model to compute a score (the probability of authorship), which quantifies how characteristic of the user a given tweet is.\n\nHuman evaluation study: To verify that our results are in agreement with human judgment of how `characteristic' a tweet is, we ask human evaluators which of a pair of tweets sounds more characteristic of the given celebrity.""]","According to the document 1909.04002, they obtain tweets from a dataset of fifteen Twitter celebrities.","Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey",0.0,0.0,0.0,0.8310424250833502
How did they extend LAMA evaluation framework to focus on negation?,"['Title: Negated LAMA: Birds cannot fly\n\nAbstract: Pretrained language models have achieved remarkable improvements in a broad range of natural language processing tasks, including question answering (QA). To analyze pretrained language model performance on QA, we extend the LAMA (Petroni et al., 2019) evaluation framework by a component that is focused on negation. We find that pretrained language models are equally prone to generate facts (""birds can fly"") and their negation (""birds cannot fly"").knowledge bases with access to oracle knowledge.\n\nThis work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement (e.g., “The theory of relativity was not developed by [MASK].”).a negation element in each template or statement. We did the same for a subset of ConceptNet that is easy to negate. We selected this subset by filtering for sentence length and extracting common queries.\n\n\n## Models\n\nWe use the source code provided by BIBREF0 and BIBREF7 and evaluate using Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERT-large (Bl).\n\n\n## Results\n\nTable TABREF1 compares the predictions of original LAMA and negated LAMA.']","They extended the LAMA evaluation framework to focus on negation by introducing the negated LAMA dataset. This was achieved by inserting negation elements, such as ""not"", into LAMA cloze statements (for example, ""The theory of relativity was not developed by [MASK]""). This approach was also applied to a subset of ConceptNet that was selected based on sentence length and common queries (Source: 1911.03343).","To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement",1.0,,0.8333333333333334,0.9863794567242895
What summarization algorithms did the authors experiment with?,"['summaries using the ROUGE BIBREF22 unigram score. For comparing performance of our ILP-based summarization algorithm, we explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases.cided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference.to predefined broad perspectives on performance. Logistic Regression classifier was observed to produce the best results for this topical classification. Finally, we proposed an ILP-based summarization technique to produce a summary of peer feedback comments for a given employee and compared it with manual summaries.\n\nThe PA process also generates much structured data, such as supervisor ratings.']","The authors experimented with summarization algorithms provided by the Sumy package, as stated in the document with the Source ID 1712.00991.","LSA, TextRank, LexRank and ILP-based summary.",0.0,0.0,0.6666666666666666,0.842263481958574
What evaluation metrics were used for the summarization task?,"['ORM6\n\nINLINEFORM0 (For determining number of phrases to select to include in summary)\n\n\n## Evaluation of auto-generated summaries\n\nWe considered a dataset of 100 employees, where for each employee multiple peer comments were recorded. Also, for each employee, a manual summary was generated by an HR personnel. The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE BIBREF22 unigram score.es not show statistically significant difference. Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries.\n\n\n## Conclusions and Further Work\n\nIn this paper, we presented an analysis of the text generated in Performance Appraisal (PA) process in a large multi-national IT company.summaries using the ROUGE BIBREF22 unigram score. For comparing performance of our ILP-based summarization algorithm, we explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases.']","The summarization task was evaluated using the ROUGE BIBREF22 unigram score. This information can be found in the document with the Source ID 1712.00991, specifically in the content labeled ""Evaluation of auto-generated summaries"".",ROUGE BIBREF22 unigram score,1.0,,0.3333333333333333,0.9321509434181894
What clustering algorithms were used?,"['ers in strengths and in weaknesses, respectively. We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms. Carrot2 Lingo discovered 167 clusters and also assigned labels to these clusters. We then generated 167 clusters using CLUTO as well. CLUTO does not generate cluster labels automatically, hence we used 5 most frequent words within the cluster as its labels. Table TABREF19 shows the largest 5 clusters by both the algorithms.ws the largest 5 clusters by both the algorithms. It was observed that the clusters created by CLUTO were more meaningful and informative as compared to those by Carrot2 Lingo. Also, it was observed that there is some correspondence between noun clusters and sentence clusters. E.g. the nouns cluster motivation expertise knowledge talent skill (Table TABREF15 ) corresponds to the CLUTO sentence cluster skill customer management knowledge team (Table TABREF19 ).can now answer question (1) in Section SECREF1 . From 12742 sentences predicted to have label STRENGTH, we extract nouns that indicate the actual strength, and cluster them using a simple clustering algorithm which uses the cosine similarity between word embeddings of these nouns. We repeat this for the 9160 sentences with predicted label WEAKNESS or SUGGESTION as a single class. Tables TABREF15 and TABREF16 show a few representative clusters in strengths and in weaknesses, respectively.']","According to the document 1712.00991, the clustering algorithms used were CLUTO BIBREF19 and Carrot2 Lingo BIBREF20.",CLUTO Carrot2 Lingo,,0.9999999999,1.0,0.9256643406089758
What evaluation metrics are looked at for classification tasks?,"[""ses and suggestions to a fixed set of attributes. In Section SECREF6 , we discuss how the feedback from peers for a particular employee can be summarized. In Section SECREF7 we draw conclusions and identify some further work.\n\n\n## Related Work\n\nWe first review some work related to sentence classification. Semantically classifying sentences (based on the sentence's purpose) is a much harder task, and is gaining increasing attention from linguists and NLP researchers.NGTH, WEAKNESS, SUGGESTION or OTHER respectively. We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation.PA) process in a large multi-national IT company. We performed sentence classification to identify strengths, weaknesses and suggestions for improvements found in the supervisor assessments and then used clustering to discover broad categories among them. As this is non-topical classification, we found that SVM with ADWS kernel BIBREF18 produced the best results. We also used multi-class multi-label classification techniques to match supervisor assessments to predefined broad perspectives on performance.""]","The evaluation metrics for classification tasks are not explicitly mentioned in the provided documents. However, Table TABREF10 is referenced which shows results of experiments, suggesting that some form of evaluation metric was used. (Source: 1712.00991)",Precision Recall F-measure accuracy,0.0,,0.6,0.0
What methods were used for sentence classification?,"['PURPOSE, METHOD, RESULT, CONCLUSION. Cohen et al. BIBREF9 applied SVM and other techniques to learn classifiers for sentences in emails into classes, which are speech acts defined by a verb-noun pair, with verbs such as request, propose, amend, commit, deliver and nouns such as meeting, document, committee; see also BIBREF10 . Khoo et al.PA) process in a large multi-national IT company. We performed sentence classification to identify strengths, weaknesses and suggestions for improvements found in the supervisor assessments and then used clustering to discover broad categories among them. As this is non-topical classification, we found that SVM with ADWS kernel BIBREF18 produced the best results. We also used multi-class multi-label classification techniques to match supervisor assessments to predefined broad perspectives on performance.ing attention from linguists and NLP researchers. McKnight and Srinivasan BIBREF7 and Yamamoto and Takagi BIBREF8 used SVM to classify sentences in biomedical abstracts into classes such as INTRODUCTION, BACKGROUND, PURPOSE, METHOD, RESULT, CONCLUSION. Cohen et al.']","According to the documents, specifically BIBREF9 and BIBREF18, Support Vector Machine (SVM) techniques were used for sentence classification. In BIBREF9, SVM was applied along with other techniques to learn classifiers for sentences into classes defined by a verb-noun pair. In BIBREF18, SVM with ADWS kernel was found to produce the best results.","Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based",0.3333333333333333,0.0,,0.9100034249449585
What is the average length of the sentences?,"['pus of supervisor assessment has 26972 sentences. The summary statistics about the number of words in a sentence is: min:4 max:217 average:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19.\n\n\n## Sentence Classification\n\nThe PA corpus contains several classes of sentences that are of interest. In this paper, we focus on three important classes of sentences viz., sentences that discuss strengths (class STRENGTH), weaknesses of employees (class WEAKNESS) and suggestions for improving her performance (class SUGGESTION).cided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference.ORM6\n\nINLINEFORM0 (For determining number of phrases to select to include in summary)\n\n\n## Evaluation of auto-generated summaries\n\nWe considered a dataset of 100 employees, where for each employee multiple peer comments were recorded. Also, for each employee, a manual summary was generated by an HR personnel. The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE BIBREF22 unigram score.']","According to the document with the ID 1712.00991, the average length of the sentences, based on the number of words, is 15.5.",15.5,,0.9999999999,0.0,0.918767367825951
What is the size of the real-life dataset?,"['en employee and compare it with manual summaries. All techniques are illustrated using a real-life dataset of supervisor assessment and peer feedback text produced during the PA of 4528 employees in a large multi-national IT company.\n\n\n## Introduction\n\nPerformance appraisal (PA) is an important HR process, particularly for modern organizations that crucially depend on the skills and expertise of their workforce.ributes are: 321, 1070, 470 and 139 respectively. We trained several multi-class multi-label classifiers on this dataset. Table TABREF21 shows the results of 5-fold cross-validation experiments on dataset D2.\n\nPrecision, Recall and F-measure for this multi-label classification are computed using a strategy similar to the one described in BIBREF21 . Let INLINEFORM0 be the set of predicted labels and INLINEFORM1 be the set of actual labels for the INLINEFORM2 instance.Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0 , where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.']","The real-life dataset contains assessment and peer feedback text for 4528 employees. This information is derived from the Source: 1712.00991 document, specifically from the content under the ChunkID: 3.",26972,0.0,0.0,0.3333333333333333,0.8745105305898594
What are the language pairs explored in this paper?,"['man-English language pair with different systems. Blue highlights the parts of the reference that are correct and red highlights the corresponding problematic parts of translations, including repetitions, dropped words or mistranslations.ttention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods.sed to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.\n\n\n## Introduction\n\nNeural machine translation (NMT) emerged in the last few years as a very successful paradigm BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 .']","The language pairs explored in this paper are De-En, Ja-En BIBREF19, and Ro-En. This information is derived from the document with the ID 1805.08241.",De-En Ja-En Ro-En,,0.9999999999,0.75,0.9146198231843338
What features are absent from MRC gold standards that can result in potential lexical ambiguity?,"['l cues as a lower bound for the requirement of understanding on the other hand. We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter. In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension compleof gold standards that are used to evaluate them. There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses. As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of undiew of the annotation results in Figure FIGREF23. The full annotation results can be found in Appendix . We centre our discussion around the following main points:\n\n\n## Application of the Framework ::: Qualitative Analysis ::: Linguistic Features\n\nAs observed in Figure FIGREF23 the gold standards feature a high degree of Redundancy, peaking at 76% of the annotated HotpotQA samples and synonyms and paraphrases (labelled Synonym), with ReCoRd samples containing 58% of them, likely to be attributed to the elab']","According to the source 2003.04642, the gold standards used in MRC tasks lack features that contribute towards lexical ambiguity. The specific details of these absent features are not explicitly mentioned in the provided context.",Restrictivity  Factivity  Coreference,0.0,0.9999999999,,0.8345377104261232
What modern MRC gold standards are analyzed?,"['l cues as a lower bound for the requirement of understanding on the other hand. We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter. In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complethey are used further in the complexity analysis.\n\n\n## Framework for MRC Gold Standard Analysis ::: Dimensions of Interest ::: Factual Correctness\n\nAn important factor for the quality of a benchmark is its factual correctness, because on the one hand, the presence of factually wrong or debatable examples introduces an upper bound for the achievable performance of models on those gold standards.ting the F1 score from those averages (macro F1). Free-text answers, meanwhile, are evaluated by means of text generation and summarisation metrics such as BLEU BIBREF16 or ROUGE-L BIBREF17.\n\n\n## Framework for MRC Gold Standard Analysis ::: Dimensions of Interest\n\nIn this section we describe a methodology to categorise gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness.']",The modern MRC gold standards analyzed are evaluated using text generation and summarization metrics such as BLEU BIBREF16 or ROUGE-L BIBREF17. (Source: 2003.04642),"fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations",,0.0,1.0,0.9639840281259602
How does proposed qualitative annotation schema looks like?,"[""l cues as a lower bound for the requirement of understanding on the other hand. We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter. In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complekground knowledge, and their factual correctness. Specifically, we use those dimensions as high-level categories of a qualitative annotation schema for annotating question, expected answer and the corresponding context. We further enrich the qualitative annotations by a metric based on lexical cues in order to approximate a lower bound for the complexity of the reading comprehension task.author of this paper, using the proposed schema. In order to validate our findings, we further take 20% of the annotated samples and present them to a second annotator (second author). Since at its core, the annotation is a multi-label task, we report the inter-annotator agreement by computing the (micro-averaged) F1 score, where we treat the first annotator's labels as gold.""]","The proposed qualitative annotation schema uses dimensions such as ground knowledge, factual correctness, and lexical cues as high-level categories for annotating questions, expected answers, and the corresponding context. It's further enriched by a metric based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task (Source: 2003.04642). The validity of the schema is confirmed by presenting 20% of the annotated samples to a second annotator and computing the inter-annotator agreement using the (micro-averaged) F1 score (Source: 2003.04642).",The resulting taxonomy of the framework is shown in Figure FIGREF10,0.0,,,0.9196322148165114
How many tweets were collected?,"['lexicons led to a collection of $19,300$ tweets. In order to have lexical diversity, we added 2500 randomly sampled tweets to our dataset. In spite of the sparse nature of these posts, the lexical characteristics act as information cues.\n\nFigure FIGREF4 pictorially represents our methodology. Our approach required an initial set of informative tweets for which we employed two human annotators annotating a random sub-sample of the original dataset.owing dictionary augments the collection process. The process ran for 4 iterations providing us 7200 transport relevant tweets as no new lexicons were identified. In order to identify linguistic signals associated with the complaint posts, we randomly sampled a set of 2000 tweets which was used as training set, manually annotated into distinct labels: complaint relevant (702) and complaint non-relevant (1298) ($\\kappa =0.79$). We employed these features on our dataset.\n\nLinguistic markers.h, the transport relevant tweets were identified. The redundant tweets were filtered based on the cosine similarity score. Implicit information indicators were identified based on domain relevance score, a metric used to gauge the coverage of n-gram (1,2,3) when evaluated against a randomly created pool of posts.\n\nWe collected a pool of 5000 randomly sampled tweets different from the data collection period.']","According to the document 2001.09215, a total of 5000 randomly sampled tweets were collected.","$19,300$ added 2500 randomly sampled tweets",1.0,,0.5,0.911890179816174
What language is explored in this paper?,"['results, both in terms of accuracy and F1-score. The best result achieved by a sentiment model was the Stanford Sentiment ($0.63$ F1-score), with others within the same range and linguistic-based features collectively giving the best performance.\n\n\n## Conclusion and Future Work\n\nIn this paper, we presented a novel semi-supervised pipeline along with a novel dataset for identification of complaint based posts in the transport domain.y response time and address concerns effectively.\n\nPresence of inevitable noise, sparse content along with rephrased and structurally morphed instances of posts, make the task at hand difficult BIBREF1. Previous works BIBREF2 in the domain of complaint extraction have focused on static datasets only. These are not robust to changes in the trends reflected, information flow and linguistic variations.t based posts pertaining to the transport domain. We perform comprehensive evaluations along with releasing a novel dataset for the research purposes.\n\n\n## Introduction\n\nWith the advent of social media platforms, increasing user base address their grievances over these platforms, in the form of complaints. According to BIBREF0, complaint is considered to be a basic speech act used to express negative mismatch between the expectation and reality.']","The paper explores posts pertaining to the transport domain, as stated in the content from Source: 2001.09215.",English language,0.0,0.0,0.5,0.8403441407088356
What are the baselines?,"[""our test sets as well as the average performance. Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality. Besides, both pre-training and multi-task learning can improve translation quality, and the pre-training settings (2nd-4th rows) are more effective compared to multi-task settings (5th-8th rows).e compared to multi-task settings (5th-8th rows). We observe a performance degradation in the `triangle+pretrain' baseline. Compared to our method, where the decoder receives higher-level syntactic and semantic linguistic knowledge extracted from text encoder, their ASR decoder can only provide lower word-level linguistic information.orm MWER segmentation with RWTH toolkit BIBREF17. We use lowercase BLEU as evaluation metric.\n\n\n## Experiments ::: Baseline Models and Implementation\n\nWe compare our method with following baselines.\n\nVanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder.""]","According to the document 1909.07575, the baselines compared are:

* Vanilla ST baseline (speech encoder and decoder only)
* `many-to-many+pretrain' baseline
* `triangle+pretrain' baseline

These baselines are used for comparison with the proposed method in the experiments.","Vanilla ST baseline encoder pre-training, in which the ST encoder is initialized from an ASR model decoder pre-training, in which the ST decoder is initialized from an MT model encoder-decoder pre-training, where both the encoder and decoder are pre-trained many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation",,0.9999999999,0.8,0.8851458162601235
What is the attention module pretrained on?,"['significantly increases the learning difficulty.\n\nNon-pre-trained Attention Module: Previous work BIBREF6 trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST does not benefit from the pre-training.\n\nTo address these issues, we propose a Tandem Connectionist Encoding Network (TCEN), which is able to reuse all subnets in pre-training, keep the roles of subnets consistent, and pre-train the attention module.ASR decoder, so the subnet waste issue is solved. Furthermore, the two encoders work at tandem, disentangling acoustic feature extraction and linguistic feature extraction, ensuring the role consistency between pre-training and fine-tuning. Moreover, we reuse the pre-trained MT attention module in ST, so we can leverage the alignment information learned in pre-training.can be jointly trained on ASR, MT, and ST tasks. As the attention module is task-specific, three attentions are defined.\n\nUsually, the size of $\\mathcal {A}$ and $\\mathcal {M}$ is much larger than $\\mathcal {S}$. Therefore, the common training practice is to pre-train the model on ASR and MT tasks and then fine-tune it with a multi-task learning manner.']","The attention module is pretrained on MT (Machine Translation) tasks, as stated in the document 1909.07575.",the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.,0.0,0.9999999999,,0.9324671009787938
How long of dialog history is captured?,"['ity than DRNNLM when using dialog turn size of 3.\n\nTo analyze the best potential gain that may be achieved by introducing linguistic context, we compare the proposed contextual models to DACLM, the model that uses true dialog act history for dialog context modeling. As shown in Table 1 , the gap between our proposed models and DACLM is not wide.us with turn and utterance-level dialog act tags. The utterances are also tagged with part-of-speech (POS) tags. We split the data in folder sw00 to sw09 as training set, folder sw10 as test set, and folder sw11 to sw13 as validation set. The training, validation, and test sets contain 98.7K turns (190.0K utterances), 5.7K turns (11.3K utterances), and 11.9K turns (22.2K utterances) respectively. Maximum turn length is set to 160.s in BIBREF11 , we refer to this model as DRNNLM. Alternatively, in the CCDCLM model proposed in BIBREF11 , the last RNN hidden state of the previous utterance is fed to the RNN hidden state of the target utterance at each time step.\n\n\n## Interactive Dialog Context LM\n\nThe previously proposed contextual language models, such as DRNNLM and CCDCLM, treat dialog history as a sequence of inputs, without modeling dialog interactions.']","The contextual language models like DRNNLM and CCDCLM use dialog history as a sequence of inputs, which implies a sequential capture of dialog history. However, the specific length of dialog history captured is not explicitly mentioned in the provided documents.",two previous turns,,0.0,1.0,0.0
What evaluation metrics were used?,"['ted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD BIBREF1 to evaluate the performance.\n\n\n## Question Answering Model\n\nThe used architecture of the QA model is briefly summarized below. Here we choose QANet BIBREF2 as the base model due to the following reasons: 1) it achieves the second best performance on SQuAD, and 2) since there are completely no recurrent networks in QANet, its training speed is 5x faster than BiDAF BIBREF17 when reaching the same performenon in our experiments shown in Table TABREF9 . Row (a) is when QANet is trained on Text-SQuAD, row (b) is when QANet is trained on Spoken-SQuAD, and row (c) is when QANet is trained on Text-SQuAD and then finetuned on Spoken-SQuAD. The columns show the evaluation on the testing sets of Text-SQuAD and Spoken-SQuAD.\n\nIt is clear that the performance drops a lot when the training and testing data mismatch, indicating that model training on ASR hypotheses can not generalize well on reference transcriptions.of an ASR module and a question answering module. Given the ASR hypotheses of a spoken document and a question, the question answering module can output a text answer.\n\nThe most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer.']","The evaluation metrics used were Exact Match (EM) and Macro-averaged F1 scores (F1). According to the document with ID 1904.07904, the ground-truth text answer was directly compared to the predicted text answer using these metrics.",Exact Match (EM) Macro-averaged F1 scores (F1),1.0,,0.6666666666666666,0.884217651565592
What was the score of the proposed model?,"['eatures and improve the robustness to ASR errors. The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.not generalize well on reference transcriptions. The performance gap is nearly 20% F1 score (72% to 55%). The row (c) shows the improved performance when testing on S-SQuAD due to the transfer learning via fine-tuning.\n\nTo better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table TABREF12 .and their corresponding reference transcriptions. An adversarial model is applied to this domain adaptation task, which forces the model to learn domain-invariant features the QA model can effectively utilize in order to improve the SQA results. The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score.\n\n\n## Introduction\n\nQuestion answering (QA) has drawn a lot of attention in the past few years.']","The proposed model achieved a 2% EM score, as stated in the document 1904.07904.",Best results authors obtain is EM 51.10 and F1 63.11,0.0,0.0,0.5,0.8891220799076842
What was the previous best model?,"['aselines and show the results in Table TABREF12 . The baselines are: (a) trained on S-SQuAD, (b) trained on T-SQuAD and then fine-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 . We also compare to the approach proposed by Lan et al. BIBREF16 in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here.eatures and improve the robustness to ASR errors. The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.and their corresponding reference transcriptions. An adversarial model is applied to this domain adaptation task, which forces the model to learn domain-invariant features the QA model can effectively utilize in order to improve the SQA results. The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score.\n\n\n## Introduction\n\nQuestion answering (QA) has drawn a lot of attention in the past few years.']","The previous best model achieved a 2% EM score. According to the document 1904.07904, the proposed model outperformed the previous best model by this margin.",(c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20,1.0,0.9999999999,,0.8811411004754266
Which datasets did they use for evaluation?,"['menon in our experiments shown in Table TABREF9 . Row (a) is when QANet is trained on Text-SQuAD, row (b) is when QANet is trained on Spoken-SQuAD, and row (c) is when QANet is trained on Text-SQuAD and then finetuned on Spoken-SQuAD. The columns show the evaluation on the testing sets of Text-SQuAD and Spoken-SQuAD.\n\nIt is clear that the performance drops a lot when the training and testing data mismatch, indicating that model training on ASR hypotheses can not generalize well on reference transcriptions.ted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD BIBREF1 to evaluate the performance.\n\n\n## Question Answering Model\n\nThe used architecture of the QA model is briefly summarized below. Here we choose QANet BIBREF2 as the base model due to the following reasons: 1) it achieves the second best performance on SQuAD, and 2) since there are completely no recurrent networks in QANet, its training speed is 5x faster than BiDAF BIBREF17 when reaching the same perforreference transcriptions are from SQuAD BIBREF1 . There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.\n\nThe original SQuAD, Text-SQuAD, is chosen as the source domain data, where only question answering pairs appearing in Spoken-SQuAD are utilized. In our task setting, during training we train the proposed QA model on both Text-SQuAD and Spoken-SQuAD training sets.']","The datasets used for evaluation were Text-SQuAD and Spoken-SQuAD, as stated in Source: 1904.07904.",Spoken-SQuAD testing set,1.0,,0.6666666666666666,0.9149869867862152
What hyperparameters are explored?,"['most hyper-parameters explored for each dataset. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB.as very many hyper-parameters which may be tuned. Hyper-parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate (BIBREF19). BIBREF0 tried various hyper-parameters with both architectures of their model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others. In our work, we extended research to 3,000 dimensions. Different observations were noted from the many trials.ting a new implementation in python from scratch.\n\nTo form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) (BIBREF22) and data pre-processing carried out. Table TABREF2 describes most hyper-parameters explored for each dataset.']","According to the document 2003.11645, several hyperparameters are explored including vector dimension size, type of algorithm, learning rate, vocabulary size (ranging from 30,000 to 3,000,000), and number of epochs. Additionally, the document mentions that 3,000 dimensions were used in their work. (Source: 2003.11645)","Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.",0.0,0.9999999999,0.0,0.8865841884219923
What Named Entity Recognition dataset is used?,"['ameters exists and evaluate various combinations. We compare them with the original model released by Mikolov. Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out. The downstream tasks reveal that the best model is task-specific, high analogy scores don\'t necessarily correlate positively with F1 scores and the same applies for more data.ors by matrix factorization (BIBREF18, BIBREF19).\n\nIt\'s been shown that word vectors are beneficial for NLP tasks (BIBREF15), such as sentiment analysis and named entity recognition. Besides, BIBREF0 showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model. The famous, semantic example: vector(""King"") - vector(""Man"") + vector(""Woman"") $\\approx $ vector(""Queen"") can be verified using cosine distance.by BIBREF12, containing 47,959 sentence samples. The IMDb dataset used has a total of 25,000 sentences with half being positive sentiments and the other half being negative sentiments. The GMB dataset has 17 labels, with 9 main labels and 2 context tags. It is however unbalanced due to the high percentage of tokens with the label \'O\'.']","The dataset used for Named Entity Recognition is the IMDb dataset, as stated in the document 2003.11645.",Groningen Meaning Bank,,0.9999999999,,0.9270923617344243
What sentiment analysis dataset is used?,"[""and negative sampling) of both skipgram and CBoW. The corpora used for word embeddings are English Wiki News Abstract by BIBREF8 of about 15MB, English Wiki Simple (SW) Articles by BIBREF9 of about 711MB and the Billion Word (BW) of 3.9GB by BIBREF10. The corpus used for sentiment analysis is the IMDb dataset of movie reviews by BIBREF11 while that for NER is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples.by BIBREF12, containing 47,959 sentence samples. The IMDb dataset used has a total of 25,000 sentences with half being positive sentiments and the other half being negative sentiments. The GMB dataset has 17 labels, with 9 main labels and 2 context tags. It is however unbalanced due to the high percentage of tokens with the label 'O'.ameters exists and evaluate various combinations. We compare them with the original model released by Mikolov. Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out. The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data.""]","The sentiment analysis dataset used is the IMDb dataset of movie reviews, which contains a total of 25,000 sentences, with half being positive sentiments and the other half being negative sentiments. (Source: 2003.11645)",IMDb dataset of movie reviews,,0.9999999999,,0.9571326973634569
What is the state-of-the-art model for the task?,"[""IRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.\n\n\n## Introduction\n\nTable-to-text generation is an important and challenging task in natural language processing, which aims to produce the summarization of numerical table BIBREF0, BIBREF1.the model contributes to the overall performance. In addition, we compare our model with delayed copy model (DEL) BIBREF12 along with gold text, template system (TEM), conditional copy (CC) BIBREF2 and NCP+CC (NCP) BIBREF4. Li's model generate a template at first and then fill in the slots with delayed copy mechanism.CO) from 15.42 to 20.86 ($+35.28\\%$) on test set. It also exceeds the state-of-the-art model in terms of those metrics.\n\n\n## Preliminaries ::: Notations\n\nThe input to the model are tables $S=\\lbrace s^{1}, s^{2}, s^{3}\\rbrace $. $s^{1}$, $s^{2}$, and $s^{3}$ contain records about players' performance in home team, players' performance in visiting team and team's overall performance respectively. We regard each cell in the table as record.""]","According to the document 1909.02304, the state-of-the-art model for the task is the model that contributes to the overall performance, with an improvement of 2.66 in BLEU over the strong baseline and outperformance of other models. However, the specific model name or type is not explicitly mentioned in the provided context.",OpATT BIBREF6 Neural Content Planning with conditional copy (NCP+CC) BIBREF4,0.0,,0.75,0.0
What is the strong baseline?,"[""IRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.\n\n\n## Introduction\n\nTable-to-text generation is an important and challenging task in natural language processing, which aims to produce the summarization of numerical table BIBREF0, BIBREF1.ion so far. We conducted experiments on ROTOWIRE. Results show that our model outperforms existing systems, improving baseline BLEU from 14.19 to 16.85 ($+18.75\\%$), P% of relation generation (RG) from 74.80 to 91.46 ($+22.27\\%$), F1% of content selection (CS) from 32.49 to 41.21 ($+26.84\\%$) and content ordering (CO) from 15.42 to 20.86 ($+35.28\\%$) on test set.CO) from 15.42 to 20.86 ($+35.28\\%$) on test set. It also exceeds the state-of-the-art model in terms of those metrics.\n\n\n## Preliminaries ::: Notations\n\nThe input to the model are tables $S=\\lbrace s^{1}, s^{2}, s^{3}\\rbrace $. $s^{1}$, $s^{2}$, and $s^{3}$ contain records about players' performance in home team, players' performance in visiting team and team's overall performance respectively. We regard each cell in the table as record.""]",The strong baseline shows a BLEU score of 14.19. (Source: 1909.02304),Conditional Copy (CC) model,0.0,0.0,0.5,0.8996977743262371
what aspects of conversation flow do they look at?,"[""Title: Conversational flow in Oxford-style debates\n\nAbstract: Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study of Oxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow.debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation.ve and rhetorical traits of each side separately. More explicitly comparing and contrasting monologic and interactive dynamics could lead to better models of conversations. Such approaches could also help clarify some of the intuitions about conversations explored in this work, particularly that engaging in dialogue carries different strategic implications from self-promotion.\n\nOur focus in this paper is on capturing and understanding conversational flow.""]","According to the source document 1604.03114, the aspects of conversation flow they examine include how winners tend to make better use of a debate's interactive component, by actively pursuing their opponents' points, and comparing and contrasting the monologic and interactive dynamics. This analysis aims to understand the strategic implications of engaging in dialogue versus self-promotion.","The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.",,0.0,0.5,0.8853354303686517
what debates dataset was used?,"['SECREF3 ), and (iii) showing its effectiveness in a predictive setting (Section SECREF4 ).\n\n\n## Debate Dataset: Intelligence Squared\n\nIn this study we use transcripts and results of Oxford-style debates from the public debate series “Intelligence Squared Debates” (IQ2 for short). These debates are recorded live, and contain motions covering a diversity of topics ranging from foreign policy issues to the benefits of organic food.s in the various contexts in which they appeared. We expect that improving our retrieval model will also improve the robustness of our idea flow analysis. A better model of discussion points could also provide more insight into the role of these points in persuading the audience.\n\nWhile Oxford-style debates are a particularly convenient setting for studying the effects of conversational flow, our dataset is limited in terms of size.racks their relative usage throughout the debate. By applying this methodology to a setting where debate winners are known, we show that conversational flow patterns are predictive of which debater is more likely to persuade an audience.\n\nCase study: Oxford-style debates. Oxford-style debates provide a setting that is particularly convenient for studying the effects of conversational flow.']","The dataset used is the Oxford-style debates from the public debate series ""Intelligence Squared Debates"". This information is sourced from document 1604.03114.",Intelligence Squared Debates,,,0.5,
what is the state of the art?,"['uence learners based on dictionaries or word2vec. To highlight the generalization of our model to idiosyncratic domains, we run tests on common-typed data sets as well as on specialized medical documents. We compare our system on these data sets with specialized state-of-the-art systems.\n\n\n## Evaluation Set Up\n\nWe train two models with identical parameterization, each with 2000 randomly chosen labeled sentences from a standard data set.the-art annotators on four common news data sets. Both common and medical models are configured identically and trained on only 2000 labeled sentences, without any external prior knowledge. We observe that DATEXIS-NER achieves the highest recall scores of all tested annotators, with 95%–98% on all measured data sets. Moreover, DATEXIS-NER precision scores are equal or better than median.life science, fashion, engineering or automotive. For these domains, we will consider the process of detecting mentions and linking them to an ontology as a joint task and we will investigate simple and interactive workflows for creating robust personalized named entity linking systems.']","According to the document with the ID 1608.06757, the state-of-the-art system is DATEXIS-NER, which achieves the highest recall scores of all tested annotators, ranging from 95% to 98% on all measured data sets. (Source: 1608.06757, ChunkID: 34)","Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2",0.0,0.0,,0.853715911899528
what standard dataset were used?,"['ew of the standard data sets we use for training. The GENIA Corpus BIBREF3 contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 BIBREF14 is a standard NER dataset based on the Reuters RCV-1 news corpus.hosen labeled sentences from a standard data set. To show the effectiveness of the components, we evaluate different configurations of this setting with 2000 random sentences from the remaining set. The model was trained using Deeplearning4j with nd4j-x86 backend. Training the TRI+BLSTM configuration on a commodity Intel i7 notebook with 4 cores at 2.8GHz takes approximately 50 minutes.\n\nTable TABREF33 gives an overview of the standard data sets we use for training.(e.g. Reuters) or biomedical text (e.g. Medline). Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM networks. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We report from our results F1 scores in the range of 84-94% on standard datasets.\n\n\n## Introduction\n\nInformation extraction tasks have become very important not only in the Web, but also for in-house enterprise settings.']","The standard datasets used include Reuters and biomedical text, such as Medline. This information is sourced from the document with ID 1608.06757.",The GENIA Corpus  CoNLL2003,,,,0.9162375267971844
What is the Random Kitchen Sink approach?,"['dataset contains 14,460 annotated English tweets. The present paper provides a comparative analysis and Random kitchen sink (RKS) based approach for offensive language detection. We explore the effectiveness of Google sentence encoder, Fasttext, Dynamic mode decomposition (DMD) based features and Random kitchen sink (RKS) method for offensive language detection. From the experiments and evaluation we observed that RKS with fastetxt achieved competing results.encoder model, Fasttext, and DMD based features. The classification using the selected features are performed using machine learning algorithms such as Random Forest (RF), Decision Tree (DT), Naive Bayes (NB), Support vector machine (SVM) linear and RBF kernels, Logistic Regression, and Random kitchen sinks. The evaluation measures used are accuracy (Acc.), precision (Prec), recall, f1-score (F1).nd Regularized least-square based classification. The measures used for evaluation are accuracy, precision, recall, and f1-score. We observed that RKS approach improved the results. However, as a future work, the proposed approach cane be explored on large datasets.']","The Random Kitchen Sink approach is a method used for offensive language detection. It has shown competitive results when used in conjunction with Fasttext, as observed in the experiments and evaluation. (Source: 2001.03131)",Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.,,0.0,0.75,0.9201108055392004
what are the baseline systems?,"['NEFORM2 Romanian and English INLINEFORM3 Russian. Our systems are based on an attentional encoder-decoder BIBREF0 , using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary BIBREF1 . We experimented with using automatic back-translations of the monolingual News corpus as additional training data BIBREF2 , pervasive dropout BIBREF3 , and target-bidirectional models.\n\n\n## Baseline System\n\nOur systems are attentional encoder-decoder networks BIBREF0 .e baseline that is only trained on parallel data. Using an ensemble of the last 4 checkpoints gives further improvements (1.3–1.7 Bleu). Our submitted system includes reranking of the 50-best output of the left-to-right model with a right-to-left model – again an ensemble of the last 4 checkpoints – with uniform weights.ents of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.\n\n\n## Introduction\n\nWe participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English INLINEFORM0 Czech, English INLINEFORM1 German, English INLINEFORM2 Romanian and English INLINEFORM3 Russian.']","The baseline systems are attentional encoder-decoder networks, as stated in the document with the ID 1606.02891.",attentional encoder-decoder networks BIBREF0,1.0,0.9999999999,,0.8943288913708741
What word embeddings do they test?,"['cifically Bernoulli embeddings (b-embs) BIBREF1 . Word embeddings models the probability of a word INLINEFORM0 given its context INLINEFORM1 as a conditional distribution INLINEFORM2 where the context is defined as the set of words INLINEFORM3 in a window of size INLINEFORM4 that surrounds it.n results across the different embeddings models. For each model, performance results are shown on 4 latent dimension values (K=25, 50, 75 and 100). For each dimension we ran experiments by varying the context window size for words (Word Context=4, 8 and 16). In addition for the EqEmb, EqEmb-U and PV-DM models we also varied the word-equation window size (E=8 and 16). Comparisons across models are performed using the pseudo log-likelihood measure BIBREF14 .ed models to existing word embeddings approaches. These are: the Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model.\n\n\n## Datasets\n\nOur datasets are scientific articles that were published on arXiv. The sets contain articles (in LaTeX format) from four computer science domains: NLP, IR, AI, and ML.']","They test the Bernoulli embeddings (b-emb), continuous bag-of-words (CBOW), Distributed Memory version of Paragraph Vector (PV-DM), and the Global Vectors (GloVe) models. (Source: 1803.09123)","Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model",,,,
How do they define similar equations?,"[""tions and words discovered using the EqEmb model. Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ).which appears in the context of its surroundings. The resulting representation, when compared to other equations' representations and word representations, helps find both related equations and related words. These are illustrated on the right.\n\nEqEmbs build on exponential family embeddings BIBREF1 to include equations as singleton observations and to model equation elements such as variables, symbols and operators. Exponential family embeddings, like all embedding methods, define a context of each word.he same query using CBOW, PV-DM, GloVe and EqEmb. In Appendix A reader should notice the difference in performance between EqEmb-U and EqEmb compared to existing embedding models which fail to discover semantically similar equations. tab:irexample1,tab:nlpexample2 show two additional example equation and its 5 most similar equations and words discovered using the EqEmb model.""]","Similar equations are defined by computing the Cosine distance between the embedding vector representation of the query equation and the context vector representation of other equations, as per EqEmb model. This method is detailed in the source document with ID 1803.09123.",By using Euclidean distance computed between the context vector representations of the equations,,,,
What evaluation criteria and metrics were used to evaluate the generated text?,"['alance between fluent and factually correct text.\n\n\n## Text Generation ::: Automatic Evaluation of Hockey Generation\n\nIn Table TABREF21, we provide evaluation results using the five aforementioned metrics. We evaluate on event level using gold standard event selection, where each generated event description is compared to its existing reference text. As the model is trained to produce a tokenized sequence, we apply a detokenizer to be able to compare against the original untokenized reference text.evaluate the model on a popular baseline dataset. After that, we describe the training of the generation model on our ice hockey corpus and use automatic evaluation metrics to compare against existing references.\n\n\n## Text Generation ::: Model Architecture\n\nWe use a pointer-generation network BIBREF10, BIBREF11, BIBREF12, where the neural attention mechanism in the encoder-decoder model is adapted to jointly model a probability distribution over words from the known vocabulary, a distribution over words frothe evaluation script provided by the organizers. Our generation system is compared to the official shared task baseline system, TGen BIBREF24, as well as to the top performing participant system on each score (ST top). Our system outperforms the TGen baseline on 3 out of 5 metrics (BLEU, METEOR and ROUGE-L), which is on par with the official shared task results, where not a single one participant system was able to surpass the baseline on all five metrics.']","The evaluation of the generated text was conducted using five metrics, as mentioned in the document with the Source ID 1910.01863. These metrics include BLEU, METEOR, and ROUGE-L. Additionally, the system's performance was compared to the official shared task baseline system, TGen BIBREF24, and the top performing participant system on each score. The system outperformed TGen baseline on three out of five metrics.",BLEU  NIST  METEOR  ROUGE-L CIDEr  evaluation script automatic evaluation human evaluation minimum edit evaluation word error rate (WER) factual errors and their types fluency issues acceptability of the output for production use in a news agency,,,,
What are the three steps to feature elimination?,"['plying a three-step feature elimination approach. In the first experiment, we observed that lexical features are critical for identifying depressive symptoms, specifically for depressed mood (-35 points) and for disturbed sleep (-43 points). In the second experiment, we observed that the optimal F1-score performance of top ranked features in percentiles variably ranged across classes e.g., fatigue or loss of energy (5th percentile, 288 features) to depressed mood (55th percentile, 3,168 features) suggestingormance, and 3) to reduce training and run times. We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach:\n\nReduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset.cale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study. In the second experiment, we determined the percentile of top ranked features that produced the optimal classification performance by applying a three-step feature elimination approach.']","The three-step feature elimination approach includes: 
1. Reducing the dataset by eliminating features that occur less than twice in the full dataset (Source: 1701.08229).
2. Conducting a feature ablation study to assess the contribution of feature groups (Source: 1701.08229).
3. Determining the percentile of top-ranked features that produce optimal classification performance (Source: 1701.08229).",Reduction Selection Rank,,0.0,0.3333333333333333,
How is the dataset annotated?,"['applying methods to eliminate low-value features.\n\n\n## METHODS\n\nSpecifically, we conducted a feature ablation study to assess the informativeness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets.oss of energy (e.g., “the fatigue is unbearable"") BIBREF10 . For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.\n\n\n## Features\n\nFurthermore, this dataset was encoded with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying depression-related classes.aset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression"") or evidence of depression (e.g., “depressed over disappointment""). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps""), disturbed sleep (e.g., “another restless night""), or fatigue or loss of energy (e.g., “the fatigue is unbearable"")']","The dataset is annotated with 9,473 annotations for 9,300 tweets, where each tweet is marked as either no evidence of depression or evidence of depression. If marked as evidence of depression, it is further annotated with one or more depressive symptoms such as depressed mood, disturbed sleep, or fatigue or loss of energy. (Source: 1701.08229)",no evidence of depression depressed mood disturbed sleep fatigue or loss of energy,,,,
What dataset is used for this study?,"['applying methods to eliminate low-value features.\n\n\n## METHODS\n\nSpecifically, we conducted a feature ablation study to assess the informativeness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets.atures to classification performance was limited. However, notable changes in F1-score were observed for the classes lower in the hierarchy including disturbed sleep and fatigue or loss of energy. For instance, changes in F1-scores driven by both recall and precision were observed for disturbed sleep by ablating demographics, emotion, and sentiment features, suggesting that age or gender (“mid-semester exams have me restless”), polarity and subjective terms (“lack of sleep is killing me”), and emoticons (“werformance observed by ablating each feature set.\n\nBy ablating each feature group from the full dataset, we observed the following count of features - sans lexical: 185, sans syntactic: 16,935, sans emotion: 16,954, sans demographics: 16,946, sans sentiment: 16,950, sans personality: 16,946, and sans LIWC: 16,832.']","The dataset used for this study is constructed based on a hierarchical model of depression-related symptoms, as mentioned in the Source 1701.08229.","BIBREF12 , BIBREF13",,,,
what were their performance results?,"['s only and will be referred to as i2b2 NER model. We evaluated the performance of the NER models by micro-F1 score on the test set of both the synthesized queries and the i2b2 data.\n\nWe used the BiLSTM-CRF implementation provided by the flair package BIBREF16. We set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair.ation of hyperparameters. Each ran for 10 epochs. Then the results were averaged to give the performance for that particular combination of hyperparameters.\n\n\n## Experiments ::: Results\n\nFrom the hyperparameter tuning we found that the best combination was\n\nembeddings: “ELMo on pubmed”,\n\nhidden_size: 256,\n\nlearning_rate: 0.05,\n\nmini_batch_size: 32.256,\n\nlearning_rate: 0.05,\n\nmini_batch_size: 32.\n\nWith the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).\n\nSince there was no ground truth available for the retrieved terms, we randomly picked a few samples to assess its performance.']","According to the source document 1912.06262, the hybrid NER model achieved a F1 score of 0.995 on synthesized queries and 0.948 on clinical notes. The i2b2 NER model, on the other hand, achieved a F1 score of 0.441 on synthesized queries and 0.927 on clinical notes. (Source: 1912.06262)",the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes,,,,
where did they obtain the annotated clinical notes from?,"['epts from user queries on medical search engines. By training a BiLSTM-CRF model on a hybrid data consisting of synthesized user queries and sentences from clinical note, we adopt a CE framework for clinical user queries with minimal effort spent on annotating user queries. We find that the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences.Title: Extracting clinical concepts from user queries\n\nAbstract: Clinical concept extraction often begins with clinical Named Entity Recognition (NER). Often trained on annotated clinical notes, clinical NER models tend to struggle with tagging clinical entities in user queries because of the structural differences between clinical notes and user queries. User queries, unlike clinical notes, are often ungrammatical and incoherent.al notes, are often ungrammatical and incoherent. In many cases, user queries are compounded of multiple clinical entities, without comma or conjunction words separating them. By using as dataset a mixture of annotated clinical notes and synthesized user queries, we adapt a clinical NER model based on the BiLSTM-CRF architecture for tagging clinical entities in user queries.']",The documents do not provide specific information on where the annotated clinical notes were obtained from.,clinical notes from the CE task in 2010 i2b2/VA,0.0,,,0.0
Which architecture do they use for the encoder and decoder?,"['e built based on an encoder-decoder architecture. In this paper, we take a step forward to construct an end-to-end ASR using an attentional-based encoder-decoder model for processing raw speech waveform, naming it as “Attention-based Wav2Text"". We investigate the performance of our proposed models on standard ASR datasets. In practice, optimizing an encoder-decoder framework is more difficult than a standard neural network architecture BIBREF10 .character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech.postrophe, period, dash, space, noise, and “eos"".\n\n\n## Model Architectures\n\nOur attention-based Wav2Text architecture uses four convolutional layers, followed by two NIN layers at the lower part of the encoder module. For all the convolutional layers, we used a leaky rectifier unit (LReLU) BIBREF20 activation function with leakiness INLINEFORM0 . Inside the first NIN layers, we stacked three consecutive filters with LReLU activation function.']","According to the document 1709.07814, they use an architecture that incorporates several convolutional layers followed by NIN layers in the encoder, and deep bidirectional long short-term memory (Bi-LSTM) in the decoder.","we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)",0.5,,,
How does their decoder generate text?,"['der module for generating correct transcriptions. If one of these modules has difficulty fulfilling its own tasks, then the model will fail to produce good results.\n\nTo ease the burden on training the whole encoder-decoder architecture directly to predict the text transcription given the raw speech waveform, we utilize a transfer learning method on the encoder part.ures given the corresponding raw speech waveform. After that, we transfer the trained layers and parameters, integrate them with the LSTM encoder-decoder, and eventually optimize the whole structure to predict the correct output text transcription given the raw speech waveform.\n\n\n## Conclusion\n\nThis paper described the first attempt to build an end-to-end attention-based encoder-decoder speech recognition that directly predicts the text transcription given raw speech input.ntion Encoder-Decoder with Mel-scale spectrogram. The difference between our Attention Encoder-Decoder (“Att Enc-Dec (ours)"", “Att Enc-Dec Wav2Text"") with Attention Encoder-Decoder from BIBREF24 (“Att Enc-Dec Content"", “Att Enc-Dec Location"") is we used the current hidden states to generate the attention vector instead of the previous hidden states.']","According to the document 1709.07814, the decoder generates text by using the current hidden states to create an attention vector. This vector is then used in conjunction with an Attention Encoder-Decoder architecture to predict the correct output text transcription from the raw speech waveform. The process involves transfer learning on the encoder part, which is then integrated with the LSTM encoder-decoder. The whole structure is optimized to perform this task directly from the raw speech input.","decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information",,,,
Which dataset do they use?,"['yers with a feedforward layer (512 hidden units).\n\n\n## Result\n\nAn example of our transfer learning results is shown in Figure FIGREF8 , and Table TABREF14 shows the speech recognition performance in CER for both the WSJ-SI84 and WSJ-SI284 datasets. We compared our method with several published models like CTC, Attention Encoder-Decoder and Joint CTC-Attention model that utilize CTC for training the encoder part. Besides, we also train our own baseline Attention Encoder-Decoder with Mel-scale spectrogram.e built based on an encoder-decoder architecture. In this paper, we take a step forward to construct an end-to-end ASR using an attentional-based encoder-decoder model for processing raw speech waveform, naming it as “Attention-based Wav2Text"". We investigate the performance of our proposed models on standard ASR datasets. In practice, optimizing an encoder-decoder framework is more difficult than a standard neural network architecture BIBREF10 .is the target sequence with length INLINEFORM4 . It consists of encoder, decoder and attention modules. The encoder task processes an input sequence INLINEFORM5 and outputs representative information INLINEFORM6 for the decoder. The attention module is an extension scheme that assists the decoder to find relevant information on the encoder side based on the current decoder hidden states BIBREF12 , BIBREF13 .']","The context mentions ""WSJ-SI84 and WSJ-SI284 datasets"" in the context of comparing ASR performance. According to the Source document 1709.07814, the datasets used are WSJ-SI84 and WSJ-SI284.",WSJ,1.0,0.9999999999,0.5,0.8463348531349945
What model is used to encode the images?,"[""Memory (LSTM) network, as presented in Figure 2 . The image is passed through the encoder generating the image representation that is used by the decoder to know the content of the image and generate the description word by word. In the following, we describe how we extended this model for the visual storytelling task.\n\n\n## Encoder\n\nThe model's first component is a Recurrent Neural Network (RNN), more precisely an LSTM that summarizes the sequence of images.rs use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence.BIBREF0 to generate stories from image sequences. Having an independent decoder for each position of the image sequence, allowed our visual storyteller to build more specific language models using the context vector as its first state and the image embedding as its first input.""]","The model used to encode the images is a Recurrent Neural Network (RNN), specifically an LSTM (Long Short-Term Memory) network. This is detailed in the document with the source ID 1806.00738.",a Convolutional Neural Network (CNN),0.0,,,
How is the sequential nature of the story captured?,"['out generating a story from a sequence of images. Figure 1 shows the difference between descriptions of images in isolation and stories for images in sequence.\n\nIn this paper, we describe the deep neural network architecture we used for the Visual Storytelling Challenge 2018. The problem to solve in this challenge can be stated as follows: Given a sequence of 5 images, the system should output a story related to the content and events in the images.ext vector of each story from the image sequence. This context vector is used as the first state of multiple independent decoder LSTMs, each of which generates the portion of the story corresponding to each image in the sequence by taking the image embedding as the first input.e first hidden state of the decoder $h_d^{(0)}$ . With this strategy, we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story.\n\nOur model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e.']","The sequential nature of the story is captured by using multiple independent decoders, each generating the portion of the story corresponding to each image in the sequence. The context vector, derived from the image sequence, is used as the first state of these decoders, providing them with both global and local information of the images. This approach is described in the source document with ID 1806.00738. (Source: 1806.00738)",we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story,,0.9999999999,,0.9114278450334582
How are meaningful chains in the graph selected?,"['ble information regarding their contextual usage. We operationalize this notion by representing knowledge graphs not as a collection of triples but as a collection of entity-relation chains, and learn embeddings for entities and relations using deep neural models that capture such contextual usage. In particular, our model is based on Bi-Directional LSTMs and learn deep representations of entities and relations from constructed entity-relation chains.elations from constructed entity-relation chains. We show that these representations can very easily be incorporated into existing models to significantly advance the state of the art on several knowledge graph prediction tasks like link prediction, triple classification, and missing relation type prediction (in some cases by at least 9.5%).r model settings for learning Dolores embeddings. We generate 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately). Our model uses L = 4 LSTM layers with 512 units and 32 dimension projections (projected values are clipped element-wise to within $[-3, 3]$ ). We use residual connections between layers and the batch size is set to 1024 during the training process.']","The selection of meaningful chains in the graph is not explicitly detailed in the provided documents. However, it can be inferred that the process involves the generation of entity-relation chains, as mentioned in the document with the ID 1811.00147. The specifics of the selection process, such as the criteria for what constitutes a ""meaningful"" chain, are not provided in the context. Accordingly, I cannot provide a detailed answer based on the provided documents. Given the lack of specific details in the provided documents about the selection process for meaningful chains in the graph, I am unable to provide a precise methodology based on the given context. The documents mention the generation of entity-relation chains and the use of deep neural models with LSTMs to learn embeddings, but they do not elaborate on how these chains are selected or deemed meaningful. Therefore, my response is:

""I cannot answer this question based on the provided documents."" Based on the information provided in the documents, there is no direct explanation of the process used to select meaningful chains in the graph. The documents discuss the use of entity-relation chains and the application of deep learning models, specifically Bi-Directional LSTMs, to learn embeddings for entities and relations within a knowledge graph. However, the details on the criteria or methodology for determining the significance or relevance of these chains are not included in the texts. The documents reference the generation of chains (1811.00147) and the model settings for learning embeddings (1811.00147), but they do not elaborate on the selection process for meaningful chains.

Therefore, the most accurate response, strictly adhering to the guidelines, is:

""I cannot answer this question based on the provided documents.""",utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.,,0.9999999999,,
What is the dimension of the embeddings?,"['ords to a 300 dimensional vector representations. Once every word of the language has been mapped to a unique vector, vectors of words can be summed up yielding a resultant vector for any given collection of words BIBREF19 . Relationship between the words is exactly retained in this form of representation.d2vec model very attractive for textual analysis. In this representation, resultant vector which is sum of 300 dimensional vectors of all words in a tweet acts as features to the model.\n\n\n## Model Training\n\nThe features extracted using the above methods for the human annotated tweets are fed to the classifier and trained using random forest algorithm. Both the textual representations performed well and the results are comparable.movements of a company and sentiments in tweets. In an elaborate way, positive news and tweets in social media about a company would definitely encourage people to invest in the stocks of that company and as a result the stock price of that company would increase.']","The embeddings have a dimension of 300, as stated in the document with the ID 1610.09225.",300,,,,
What dataset is used to train the model?,"['classifier used for sentiment analysis of tweets. The classifier with features as Word2vec representations of human annotated tweets trained on Random Forest algorithm with a split percentage of 90 for training the model and remaining for testing the model showed an accuracy of 70.2%. With N-gram representations, the classifier model with same algorithm and with same dataset showed an accuracy of 70.5%.ore than 10,000 tweets and train the classifiers. With increasing size of training datasets, the models tend to perform better.\n\n\n## Acknowledgment\n\nThe authors would like to thank the students of IIT Bhubaneswar who contributed to the human annotation of tweets.n the model and remaining for testing operations. The classifier results show an accuracy value of 69.01% when trained using Logistic regression algorithm and the accuracy rate varied with the training set. When the model with LibSVM is trained with 90 percent of data, it gave a result of 71.82%. These results give a significant edge to the investors and they show good correlation between stock market movements and the sentiments of public expressed in twitter.']","The model is trained on more than 10,000 tweets, as mentioned in the document with the Source ID 1610.09225.","2,50,000 tweets Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016",0.0,,,0.8404858050777779
What is the previous state of the art?,"['itions given our limited computational resources.\n\nLooking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al.he""entailment""token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.\n\n\n## Introduction\n\nOther than encoder-only pretrained transformer architectures BIBREF2, BIBREF3, BIBREF4, encoder–decoder style pretrained transformers BIBREF0, BIBREF5 have been proven to be effective in text generation tasks as well as comprehension tasks.t generation tasks as well as comprehension tasks. This paper describes our submission to the commonsense reasoning task leaderboard of the AI2 WinoGrande Challenge BIBREF1, which uses the text-to-text transfer transformer (T5); our approach currently represents the state of the art.\n\nIn T5 BIBREF0, NLP tasks are formulated as text-to-text problems, where the inputs are cast into natural language templates that contain the task descriptors. Concretely, Raffel et al.']","The previous state of the art is based on RoBERTa BIBREF2, according to the document 2003.08380.",RoBERTa,,,,
Which text embedding methodologies are used?,"['e vocabulary, which is of relevance to our study.\n\n\n## Text Embedding\n\nComputational methods for text analysis rely on a choice of a mathematical representation of the base units, such as character $n$ -grams, words or documents of any length. An important consideration for our methodology is an attempt to avoid the use of labelled data at the core of many supervised or semi-supervised classification methods BIBREF18 , BIBREF19 .the free text descriptions contained within them. To do so, we combine recently developed deep neural network text-embedding methodologies based on paragraph vectors with multi-scale Markov Stability community detection applied to a similarity graph of documents obtained from sparsified text vector similarities. We showcase the approach with the analysis of incident reports submitted in Imperial College Healthcare NHS Trust, London.r than fitting with pre-designed classifications. The obtained results could thus help mitigate possible human error or effort in finding the right category in complex category classification trees.\n\nWe showcase the methodology through the analysis of a dataset of patient incidents reported to the NRLS. First, we use the 13 million records collected by the NRLS since 2004 to train our text embedding (although a much smaller corpus can be used).']",The methodologies used are deep neural network text-embedding methodologies based on paragraph vectors and multi-scale Markov Stability community detection. These are detailed in Source: 1811.05711.,Document to Vector (Doc2Vec),,0.0,,
Which race and gender are given higher sentiment intensity predictions?,"[""sk on predicting sentiment and emotion intensity. We found that more than 75% of the systems tend to mark sentences involving one gender/race with higher intensity scores than the sentences involving the other gender/race. We found such biases to be more widely prevalent for race than for gender.red task, SemEval-2018 Task 1 'Affect in Tweets'. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.\n\n\n## Introduction\n\n[0]leftmargin=* [0]leftmargin=*\n\nAutomatic systems have had a significant and beneficial impact on all walks of human life.feel angry' vs. `This woman made me feel angry'). We find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.""]","According to the documents, specifically the Source: 1805.04508, it is found that sentences associated with one race and one gender are given higher sentiment intensity predictions. The documents also indicate that race is more commonly the focus of these biases than gender. However, the specific race and gender are not explicitly mentioned in the provided context. Therefore, I cannot provide a more specific answer based on the provided documents.","Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.",,,,
"What criteria are used to select the 8,640 English sentences?","[""INEFORM3 variables with the values they can take. In total, 8,640 sentences were generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates. We manually examined the sentences to make sure they were grammatically well-formed.et for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 'Affect in Tweets'.to make sure they were grammatically well-formed. Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e.g., `My daughter feels devastated' and `My son feels devastated'). We refer to the full set of 8,640 sentences as Equity Evaluation Corpus.""]","The 8,640 English sentences are carefully chosen to examine biases towards certain races and genders, as stated in the source document 1805.04508. Additionally, they are used to derive pairs of sentences that differ only in one word corresponding to gender or race, ensuring they are grammatically well-formed.","Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.",,,,
what were the baselines?,"[""Title: Jasper: An End-to-End Convolutional Neural Acoustic Model\n\nAbstract: In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad.eech and competitive results on other benchmarks. Our Jasper architecture is highly efficient for training and inference, and serves as a good baseline approach on top of which to explore more sophisticated regularization, data augmentation, loss functions, language models, and optimization strategies. We are interested to see if our approach can continue to scale to deeper models and larger datasets.with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets.\n\n\n## Introduction\n\nConventional automatic speech recognition (ASR) systems typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words.""]","According to the document 1904.03288, the baselines were achieved using a greedy decoder on LibriSpeech test-clean dataset and competitive results were reported on the Wall Street Journal and the Hub5'00 conversational evaluation datasets.","LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC",,,,
what competitive results did they obtain?,"[""eech and competitive results on other benchmarks. Our Jasper architecture is highly efficient for training and inference, and serves as a good baseline approach on top of which to explore more sophisticated regularization, data augmentation, loss functions, language models, and optimization strategies. We are interested to see if our approach can continue to scale to deeper models and larger datasets.mong end-to-end models on LibriSpeech test-other. We show competitive results on Wall Street Journal (WSJ), and 2000hr Fisher+Switchboard (F+S). Using only greedy decoding without a language model we achieve 3.86% WER on LibriSpeech test-clean.\n\nThis paper makes the following contributions:\n\n\n## Jasper Architecture\n\nJasper is a family of end-to-end ASR models that replace acoustic and pronunciation models with a convolutional neural network.architectures for end-to-end speech recognition. Inspired by wav2letter's convolutional approach, we build a deep and scalable model, which requires a well-designed residual topology, effective regularization, and a strong optimizer. As our architecture studies demonstrated, a combination of standard components leads to SOTA results on LibriSpeech and competitive results on other benchmarks.""]","According to the document 1904.03288, they achieved competitive results on Wall Street Journal (WSJ) and 2000hr Fisher+Switchboard (F+S). They also achieved a Word Error Rate (WER) of 3.86% on LibriSpeech test-clean using greedy decoding without a language model.","In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3.",,,0.5,0.8188781143167517
By how much is performance improved with multimodality?,"['bin video) and outside objects (from road video).\n\n\n## Conclusion\n\nIn this study, we present our initial explorations towards multimodal understanding of passenger utterances in autonomous vehicles. We briefly show that our experimental results outperformed certain baselines and with multimodality, we achieved improved overall F1-scores of 0.92 for utterance-level intent detection and 0.97 for word-level slot filling.of the vehicle from the road view camera stream). Our experimental results outperformed text-only baselines and with multimodality, we achieved improved performances for utterance-level intent detection and slot filling.\n\n\n## Methodology\n\nWe explored leveraging multimodality for the NLU module in the SDS pipeline. As our AMIE in-cabin dataset has video and audio recordings, we investigated 3 modalities for the NLU: text, audio, and video.ectors have lower vocabulary coverage than GloVe. Yet, we observed that concatenating GloVe + Speech2Vec, and further GloVe + Word2Vec + Speech2Vec yields better NLU results: F1-score increased from 0.89 to 0.91 for intent recognition, from 0.96 to 0.97 for slot filling.\n\nFor multimodal (audio & video) features exploration, performance results of the compared models with varying modality/feature concatenations can be found in Table TABREF4.']","According to the information provided in Source: 1909.13714, the use of multimodality in the NLU module results in improved performance. Specifically, for intent recognition, the F1-score increased from 0.89 to 0.91, and for slot filling, the F1-score increased from 0.96 to 0.97.",by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling,,,,
What is the language model combination technique used in the paper?,"['style is conversation and in noisy environments. It is not straight-forward to tackle such problems due to difficulties in data collection for both speech and text. In this paper, we attempt to mitigate the problems using language models combination techniques that allows us to utilize both large amount of writing style text and small number of conversation text data.e model and 0.4 is given to the conversation one. On the vlsp2019 set, the ratio is change slightly by 0.7:0.3 to deliver the best result of 15.09%.\n\n\n## Conclusion\n\nIn this paper, we presented our ASR system participated in VLSP 2019 challenge that incorporates a language model combination technique to handle conversation speech with small amount of text data required. The method demonstrated that it can help to reduce WER by 3% on the VLSP 2019 challenge.system is getting biased to writing-style speech.\n\nIn this paper, we present our attempt to mitigate the problems using a large scale data set and a language model combination technique that only require a small amount of conversation data but can still handle very well conversation speech.']","The paper uses a language model combination technique that utilizes a large scale data set and a small amount of conversation data. This technique is able to handle conversation speech well, even when the system tends to be biased towards writing-style speech. (Source: 1910.05603)",system combination on the decoding lattice level combination weights,0.0,,,0.8697192011129795
What are the deep learning architectures used in the task?,"['training pipeline is described in Figure FIGREF6. First, we collect and clean large amount of text data from various sources including news, manual labeled conversation video. Then, the collected data is categorized into domains. This is an important step as the ASR performance is highly depends on the speech domain.y, language model data is much easier to collect.\n\nThe language model training for the Automatic Speech Recognition (ASR) system usually based on corpus crawled on formal text, so that some conjunctive words which often used in conversation will be missed out, leading to the system is getting biased to writing-style speech.training data and also cost for data collection. There are two components of an ASR system that contribute the most to the accuracy of it, an acoustic model and a language model. While collecting data for acoustic model is time-consuming and costly, language model data is much easier to collect.']","I'm sorry, but I cannot answer this question based on the provided documents. The documents do not provide specific information about the deep learning architectures used in the task.",DNN-based acoustic model BIBREF0,0.0,0.0,,0.0
How much is performance improved on NLI?,"['ext is effective for improving inference ability. Considering that the representation of IsNext and IsNextInadj should be coherent, we propose BERTBase-PNsmth to mitigate this problem. PNsmth further improves the performance and obtains an averaged score of 81.0.\n\n\n## Results of NLI Tasks ::: HANS\n\nAlthough BERT has shown its effectiveness in the NLI tasks. BIBREF0 pointed out that BERT is still vulnerable in the NLI task as it is prone to adopting fallible heuristics.for all the tasks, with a learning rate of 2e-5. The predictions were obtained by evaluating the training checkpoint with the best validation performance.\n\nTable TABREF21 illustrates the experimental results, showing that our method is beneficial for all of NLI tasks. The improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase. Besides NLI, our model also performs better than BERTBase in the STS task.I pair is reversed, the performance will degrade. For instance, the accuracy decreases by about 0.5% on MNLI BIBREF12 and 0.4% on QNLI after swapping the sentences in our experiments .\n\nMotivated by these problems, we propose to extend the NSP task with previous sentence prediction (PSP). Despite its simplicity, empirical results show that this is beneficial for downstream tasks, including both NLI and MRC tasks.']","The performance on NLI tasks improved by 4% absolute gain over BERTBase, as shown in Table TABREF21 (1909.03405).","improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase",,,0.5,0.8758416142195123
What BERT model do they test?,"['ard data samples constructed by three heuristics. Previous advanced models such as BERT fail on the HANS dataset, and the test accuracy can barely exceed 0% in the subset of test examples.\n\n\n## Related Work ::: Unsupervised learning from document\n\nIn recent years, many unsupervised pre-training methods have been proposed in the NLP fields to extract knowledge among sentences DBLP:conf/nips/KirosZSZUTF15,DBLP:conf/emnlp/ConneauKSBB17,DBLP:conf/iclr/LogeswaranL18,DBLP:journals/corr/abs-1903-09424.d in NSP are randomly drawn from other documents. Therefore, to discriminate against these sentences, BERT is prone to aggregating the shallow semantic, e.g., topic, neglecting context clues useful for detailed reasoning. In other words, the canonical NSP task would encourage the model to recognize the correlation between sentences, rather than obtaining the ability of semantic entailment. This setting weakens the BERT model from learning specific semantic for inference.r assumption, a primary experiment was conducted. The order of the input pair of NLI samples is reversed in the fine-tuning phase, and other hyper-parameters and settings keep the same with the BERT paper. Table TABREF19 shows the accuracy on the validation set of the MNLI and QNLI datasets. For the BERTBase model, when the sentences are swapped, the accuracy decreases by 0.5% on the MNLI task and 0.4% on the QNLI task.']","They test the BERTBase model, as indicated in the source document 1909.03405.",BERTbase,1.0,,,0.8481201086944582
What downstream tasks are evaluated?,"['ions to consider in making the stopping decision. Our results indicate that making the window size smaller helps to mitigate the degradation in stopping method performance that occurs with larger batch sizes.\n\n\n## Acknowledgment\n\nThis work was supported in part by The College of New Jersey Support of Scholarly Activities (SOSA) program, by The College of New Jersey Mentored Undergraduate Summer Experience (MUSE) program, and by usage of The College of New Jersey High Performance Computing System.b-optimally when using very large batch percents. Usage of very large batch sizes, such as 10% of the data, is not common so sub-optimal performance of stopping methods in those situations is not a major problem.\n\n\n## Conclusion\n\nActive learning has the potential to significantly reduce annotation costs.he percentage to 99 and denote this as Oracle-99. We set the percentage to 99 because it is typical for leading stopping methods to be able to achieve this level of performance (see Table 1 in BIBREF4 ). Although the Oracle Method cannot be used in practice, it is useful for contextualizing the stopping results of practical stopping methods.\n\n\n## Results\n\nWe considered different batch sizes in our experiments, based on percentages of the entire set of training data.']","The context provided does not include specific details about the downstream tasks evaluated. Therefore, I cannot answer this question based on the provided documents.",text classification,,0.0,0.5,0.0
What is active learning?,"['ore effective when smaller window sizes are used.\n\n\n## Introduction\n\nThe use of active learning has received a lot of interest for reducing annotation costs for text classification BIBREF0 , BIBREF1 , BIBREF2 .\n\nActive learning sharply increases the performance of iteratively trained machine learning models by selectively determining which unlabeled samples should be annotated.ts of active learning to be achieved in practice. Without stopping methods, the active learning process would continue until all annotations have been labeled, defeating the purpose of using active learning. Accordingly, there has been a lot of interest in the development of active learning stopping methods BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 .\n\nAnother important aspect of the active learning process is what batch size to use.b-optimally when using very large batch percents. Usage of very large batch sizes, such as 10% of the data, is not common so sub-optimal performance of stopping methods in those situations is not a major problem.\n\n\n## Conclusion\n\nActive learning has the potential to significantly reduce annotation costs.']",Active learning is a process that sharply increases the performance of iteratively trained machine learning models by selectively determining which unlabeled samples should be annotated (Source: 1801.07887). It has gained attention for its potential to significantly reduce annotation costs in text classification (Source: 1801.07887).,A process of training a model when selected unlabeled samples are annotated on each iteration.,,,,
what was the baseline?,"[""trong baseline, as summarized in Table TABREF53 . This paper contains an empirical comparison of several existing approaches and hence we hope that our paper can act as a guideline to researchers attempting to tackle extremely low-resource translation.\n\nIn the future, we plan to confirm further fine-tuning for each of specific translation directions.ow refer to those described in Section SECREF41 . First of all, the final models of our multistage fine-tuning, i.e., V and VII, achieved significantly higher BLEU scores than (b3) in Table TABREF27 , a weak baseline without using any monolingual data, and #10 in Table TABREF33 , a strong baseline established with monolingual data.\n\nThe performance of the initial model (I) depends on the language pair.ORM7 En pairs sometimes degraded from II and III. Nevertheless, the BLEU scores still retain the large margin against two baselines.\n\nThe last three rows in Table TABREF46 present BLEU scores obtained by the methods with fewer fine-tuning steps. The most naive model I', trained on the balanced mixture of whole five types of corpora from scratch, and the model II', obtained through a single-step conventional fine-tuning of I on all the in-domain data, achieved only BLEU scores consistently worse than VII.""]","According to the document 1907.03060, the weak baseline without using any monolingual data achieved BLEU scores as mentioned in Table TABREF27.",pivot-based translation relying on a helping language BIBREF10 nduction of phrase tables from monolingual data BIBREF14  attentional RNN-based model (RNMT) BIBREF2 Transformer model BIBREF18 bi-directional model BIBREF11 multi-to-multi (M2M) model BIBREF8 back-translation BIBREF17,,,,
How is segmentation quality evaluated?,"['Title: Automatic Discourse Segmentation: an evaluation in French\n\nAbstract: In this article, we describe some discursive segmentation methods as well as a preliminary evaluation of the segmentation quality. Although our experiment were carried for documents in French, we have developed three discursive segmentation models solely based on resources simultaneously available in several languages: marker lists and a statistic POS labeling.gmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The results are presented in the table tab:humains. The first row shows the performance of the $I$ segments, taking the experts as a reference, while the second presents the process in the opposite direction.\n\nWe have found that segmentation by experts and naive produces two subcorpus $E$ and $N$ with very similar characteristics.r segmenters.\n\nTwo batch of tests were performed. The first on the $D$ set of documents common to the two subcorpus “specialist” $E$ and “naive” $N$ from Annodis. $D$ contains 38 documents with 13 364 words. This first test allowed to measure the distance between the human markers. In fact, in order to get an idea of the quality of the human segmentations, the cuts in the texts made by the specialists were measured it versus the so-called “naifs” note takers and vice versa.']","According to the document 2002.04095, segmentation quality is evaluated through automatic assessments. The process involves comparing the results of two types of segmenters: those done by experts and those done by naive individuals. The evaluation is performed using a set of common documents, referred to as $D$ in the text, which contains 13,364 words. The quality of human segmentations is gauged by measuring the distance between the cuts made by specialists and those made by naive note takers, as well as vice versa.","Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.",0.0,,,
How do they compare lexicons?,"['onsidered valid, they update the lexicon entries. The crowd also evaluates existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists.\n\n\n## Data\n\nDuring January 2017, we performed a keyword based crawl for articles and comments in the Europe subreddit and tweets in Twitter, which contained the word ""Brexit"".was also performed by experts or gold standards. In our paper, we propose a crowdsourcing method for lexicon acquisition, which is scalable, cost-effective, and doesn\'t require experts or gold standards. We also compare crowd and expert evaluations of the lexicon, to assess the overall lexicon quality, and the evaluation capabilities of the crowd.\n\n\n## Introduction\n\nSentiment analysis aims to uncover the emotion conveyed through information.nd evaluation process is another limiting factor. Ideally we would prefer a fixed number of individuals, each to annotate and evaluate the whole corpus. However, the uniformity of expert judgement is replaced with the diversity and mass of contributors.\n\nThe corpus may be limiting the term groups in the lexicon to specific domain-specific subjects. Comparisons with existing lexicons, such as NRC BIBREF21 indicate a moderate overlap with 40% common terms.']","They compare lexicons by assessing the overlap of common terms, as indicated by the source document 1710.04203, which shows a moderate overlap with existing lexicons like NRC BIBREF21 at 40%.",Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.,,,,
How larger are the training sets of these versions of ELMo compared to the previous ones?,"['ly the size of the training dataset is different. ELMoForManyLangs uses only 20 million tokens and we use the whole corpus of 270 million tokens.\n\nThe Latvian ELMo model from ELMoForManyLangs project performs significantly worse than EMBEDDIA ELMo Latvian model on all categories of word analogy task (Figure FIGREF16). We also include the comparison with our Estonian ELMo embeddings in the same figure.ining the embedding models, and their evaluation. We show that the size of used training sets importantly affects the quality of produced embeddings, and therefore the existing publicly available ELMo embeddings for the processed languages are inadequate. We trained new ELMo embeddings on larger training sets and analysed their properties on the analogy task and on the NER task.ming CNN layer embeddings in semantic categories. Latvian ELMo performs worse compared to other languages we trained, especially in semantic categories, presumably due to smaller training data size. Surprisingly, the original English ELMo performs very poorly in syntactic categories and only outperforms Latvian in semantic categories.']","According to the document 1911.10049, the version used here employs a larger corpus of 270 million tokens, which is significantly larger compared to the 20 million tokens used in the ELMoForManyLangs project.",By 14 times.,,,,
What is the improvement in performance for Estonian in the NER task?,"['if it does not fit any of the other three labels. The number of words having each label is shown in Table TABREF19.\n\nTo measure the performance of ELMo embeddings on the NER task we proceeded as follows. We embedded the text in the datasets sentence by sentence, producing three vectors (one from each ELMo layer) for each token in a sentence. We calculated the average of the three vectors and used it as the input of our recognition model.ask, we used named entity recognition (NER) task. NER is an information extraction task that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. To allow comparison of results between languages, we used an adapted version of this task, which uses a reduced set of labels, available in NER datasets for all processed languages.word is a named entity or not is more difficult. For languages with the smallest NER datasets, Croatian and Lithuanian, ELMo embeddings show the largest improvement over fastText embeddings. However, we can observe significant improvements with ELMo also on English and Finnish, which are among the largest datasets (English being by far the largest).']","Based on the provided documents, specifically Document 51, it is mentioned that ELMo embeddings show the largest improvement over fastText embedds for languages with the smallest NER datasets, such as Croatian and Lithuanian. However, the exact performance improvement for Estonian is not explicitly stated. Therefore, I cannot provide a specific answer based on the provided documents. I cannot answer this question based on the provided documents.",5 percent points.,,,,
what is the state of the art on WSJ?,"['adaptation, and reaches INLINEFORM0 WER on nov92. DeepSpeech 2 shows a WER of INLINEFORM1 but uses 150 times more training data for the acoustic model and huge text datasets for LM training. Finally, the state-of-the-art among end-to-end systems trained only on WSJ, and hence the most comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets INLINEFORM2 WER.a beam score of 26 for computational efficiency. Once INLINEFORM3 are chosen, the test WER is computed with a beam size of 3000 and a beam score of 50.\n\n\n## Word Error Rate results\n\nTable TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.the n-gram model even with the same context size. Increasing the context gives a significant boost in performance, with the major gains obtained between a context of 3 to 9 ( INLINEFORM0 absolute WER).\n\n\n## Conclusion\n\nWe introduced the first fully convolutional pipeline for speech recognition, that can directly process the raw waveform and shows state-of-the art performance on Wall Street Journal and on Librispeech among end-to-end systems.']","The state-of-the-art performance on Wall Street Journal (WSJ) is achieved by a fully convolutional pipeline for speech recognition, which shows state-of-the-art performance. According to the source document 1812.06864, this system reaches an absolute Word Error Rate (WER) of INLINEFORM0 on WSJ.",CNN-DNN-BLSTM-HMM,,,,
How did they obtain the OSG dataset?,"[""mments. In total, there are 1.5 million comments. Since the created dataset is multi-threaded, we need to extract conversation threads, to eliminate paths not relevant to the original post.\n\n\n## Datasets ::: OSG ::: Conversation Thread Extraction\n\nThe thread extraction algorithm is heuristic-based and consists of two steps: (1) creation of a tree, based on a post written by a user and the related comments and (2) transformation of the tree into a list of threads.Well no. You're not alone” – {Reject, Statement}. The datasets differ with respect to the distribution of these Dialogue Acts tags, they appear more frequently in OSG.\n\n\n## Analysis ::: Sentiment of Posts and Comments\n\nTable TABREF39 presents the distribution of sentiment polarity in post and comment statements (i.e. sentences tagged as statement). For OSG, the predominant sentiment label of statements is positive and it is the highest for both posts and comments.ion in the dataset consists of at least 4 tweets. Even though, according to BIBREF23, Twitter is broadly applicable to public health research, our expectation is that it contains less therapeutic conversations in comparison to specialized on-line support forums.\n\n\n## Datasets ::: OSG\n\nOur data has been developed by crawling and pre-processing an OSG web forum. The forum has a great variety of different groups such as depression, anxiety, stress, relationship, cancer, sexually transmitted diseases, etc.""]","The OSG dataset was obtained by crawling and pre-processing an OSG web forum, as stated in the document with the ID 1911.01371.",crawling and pre-processing an OSG web forum,,,,
How large is the Twitter dataset?,"['ion in the dataset consists of at least 4 tweets. Even though, according to BIBREF23, Twitter is broadly applicable to public health research, our expectation is that it contains less therapeutic conversations in comparison to specialized on-line support forums.\n\n\n## Datasets ::: OSG\n\nOur data has been developed by crawling and pre-processing an OSG web forum. The forum has a great variety of different groups such as depression, anxiety, stress, relationship, cancer, sexually transmitted diseases, etc.ovide a fair comparison with the Twitter dataset. An example conversation from each data source is presented in Figure FIGREF19.\n\n\n## Datasets ::: Twitter\n\nWe have downloaded 1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted. A conversation in the dataset consists of at least 4 tweets.both cases the most common set is statement only. However, conversations containing only statement, emphasis or question posts and comments predominantly appear in Twitter. Which is expected due to the shorter length of Twitter posts and comments.\n\nWe can also observe that the original posters tend to ask more questions than the commenting posters – 19.83% for posts vs. 11.21% for comments (summed).']","The Twitter dataset consists of approximately 14,000 tweets, as stated in the document with the ID 1911.01371.","1,873 Twitter conversation threads, roughly 14k tweets",,,,
what is the size of the augmented dataset?,"['h datasets may not be generalizable across tasks. Building on prior work demonstrating that same-task data of healthy participants helps improve AD detection on a single-task dataset of pathological speech, we augment an AD-specific dataset consisting of subjects describing a picture with multi-task healthy data. We demonstrate that normative data from multiple speech-based tasks helps improve AD detection by up to 9%.(10 units, Adam optimizer, 500 epochs) BIBREF15 . Additionally, we augment the DB data with healthy samples from FP with varied ages.\n\n\n## Results and Discussion\n\n\n## Visualization of Class Boundaries\n\nSince data of different tasks have different noise patterns, the probability of overfitting to noise is reduced with samples from different tasks. This can also be visualized as decision boundaries of models trained on various dataset combinations. For Fig.activation function is used at each hidden layer. The network is trained using Adam for 100 epochs with other default parameters in BIBREF15 .\n\n\n## Effect of data from different tasks:\n\nThe effect of augmenting DB with data from a different structured task (HAFP) is shown in SECREF41 .\n\nF1 scores (micro and macro) increase non-linearly with the addition of data.']",The provided documents do not specify the size of the augmented dataset.,609,,,,0.0
How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?,"[""n techniques for building recommendation systems. Topic modeling based on LDA, is a powerful technique for semantic mining and perform topic extraction. In the past few years, many articles have been published based on LDA technique for building recommendation systems. In this paper, we present taxonomy of recommendation systems and applications based on LDA. In addition, we utilize LDA and Gibbs sampling algorithms to evaluate ISWC and WWW conference publications in computer science.methods in 'Question Answering and Social Media'.\n\n\n## Discussion, Open Issues and Future Directions\n\nIn this study, we focused on the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter.ontent or prior knowledge of these itemsBIBREF77.\n\n\n## Conclusion\n\nIn this paper, we presented a taxonomy of recommendation systems and applications based on LDA of the recent research, including app, travel, friend, location, scientific paper, and music recommendation. Furthermore, we applied LDA algorithm and Gibbs sampling on ISWC and WWW conference's publications from 2013-2017.""]","According to the document identified as 1909.09551, LDA (Latent Dirichlet Allocation) and Gibbs sampling algorithms are utilized to evaluate publications from ISWC and WWW conferences in the field of computer science. This is part of a study that focuses on the application of LDA approaches to recommendation systems, where a taxonomy of recommendation systems based on recent research is presented. The method involves applying these algorithms to assess the relevance and quality of the conference publications from 2013 to 2017.","the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter",,,,
What dataset do they use to evaluate their method?,"['according to word classes described in Table 1 . Moreover, Table 2 shows the average number of patterns for each word pair in our dataset.\n\n\n## Experimental Settings\n\nWe use the English Wikipedia dump from June 2016 as the corpus resource for our methods and baselines. For parsing the corpus, we rely on spaCy. For the lemma embeddings, we rely on the word embeddings of the dLCE model BIBREF10 which is the state-of-the-art vector representation for distinguishing antonyms from synonyms.tion features for the task of hypernym detection. The direction feature represented the direction of the dependency label between two nodes in a path from X to Y.\n\nFor evaluation, we make use of the same information regarding dataset and patterns as in Section ""Overall Results"" , and then replace the distance feature by the direction feature. The results are shown in Table 4 .ve patterns for training, validating and testing. For the proportion of positive and negative pairs, we keep a ratio of 1:1 positive (antonym) to negative (synonym) pairs in the dataset. In order to create the sets of training, testing and validation data, we perform random splitting with 70% train, 25% test, and 5% validation sets. The final dataset contains the number of word pairs according to word classes described in Table 1 .']","They use the English Wikipedia dump from June 2016 as the corpus resource for their methods and baselines to evaluate their method, according to the document with Source ID: 1701.02962.",antonym and synonym pairs collected from WordNet BIBREF9 and Wordnik,,,,
Why are current ELS's not sufficiently effective?,"[""marizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS's are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities.s the decoder about the constructed topic vector. Due to the imperfections of current ELS's, the extracted linked entities may be too ambiguous and coarse to be considered relevant to the summary. We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention.\n\nWe experiment on two datasets, Gigaword and CNN, with varying lengths.information to generate more coherent summaries. We find that $76.0\\%$ of the extracted linked entities are covered by the pre-trained vectors in our experimental data, proving our third observation.\n\n\n## Possible issues\n\nDespite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness.\n\nFirst, the extracted entities may be ambiguous.""]","Current ELS's are not sufficiently effective due to their introduction of unresolved ambiguities and irrelevant entities, as stated in the document 1806.05504.",Linked entities may be ambiguous or too common,,,,0.9031775334592282
What is the best model?,"['various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature., we describe our approach in building our model. This model is partly inspired from multiple models BIBREF20,BIBREF1, andBIBREF2\n\n\n## Approach ::: Bidirectional LSTM\n\nWe used Bi-directional LSTM to capture the word representation in forward as well as reverse direction of a sentence. Generally, LSTMs take inputs from left (past) of the sentence and computes the hidden state.amed Entity Recognizer (NER) for Nepali language. To best of our knowledge we are the first to propose neural based Nepali NER.\n\nAs there are not good quality dataset to train NER we release a dataset to support future research\n\nWe perform empirical evaluation of our model with state-of-the-art models with relative improvement of upto 10%\n\nIn this paper, we present works similar to ours in Section SECREF2.']","The best model obtains an overall F1 score of 86.89, as mentioned in the document `1908.05828`. This model uses a Bidirectional LSTM approach for capturing word representation in both forward and reverse directions of a sentence.",BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS,0.0,,0.5,0.8150466320421748
How many sentences does the dataset contain?,"['ost-positions boosted the F1 score by almost 10%.\n\nIn order to label our dataset with POS-tags, we first created POS annotated dataset of 6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus and trained a BiLSTM model with 95.14% accuracy which was used to create POS-tags for our dataset.\n\nThe dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline.d embeddings before passing it to BiLSTM network. A sample of data is shown in figure FIGREF13.\n\n\n## Dataset Statistics ::: OurNepali dataset\n\nSince, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG).ch is again concatenated with one-hot POS vector.\n\n\n## Experiments ::: Tagging Scheme\n\nCurrently, for our experiments we trained our model on IO (Inside, Outside) format for both the dataset, hence the dataset does not contain any B-type annotation unlike in BIO (Beginning, Inside, Outside) scheme.\n\n\n## Experiments ::: Early Stopping\n\nWe used simple early stopping technique where if the validation loss does not decrease after 10 epochs, the training was stopped, else the training will run upto 100 epochs.']",The dataset contains 6946 sentences. This information is sourced from the document with the ID 1908.05828.,3606,,,,0.9454635713697525
What is the baseline?,"['on previously reported performance in literature.\n\n\n## Introduction\n\nNamed Entity Recognition (NER) is a foremost NLP task to label each atomic elements of a sentence into specific categories like ""PERSON"", ""LOCATION"", ""ORGANIZATION"" and othersBIBREF0. There has been an extensive NER research on English, German, Dutch and Spanish language BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, and notable research on low resource South Asian languages like HindiBIBREF6, IndonesianBIBREF7 and other Indian languages (KaTitle: Named Entity Recognition for Nepali Language\n\nAbstract: Named Entity Recognition (NER) has been studied for many languages like English, German, Spanish, and others but virtually no studies have focused on the Nepali language. One key reason is the lack of an appropriate, annotated dataset. In this paper, we describe a Nepali NER dataset that we created. We discuss and compare the performance of various machine learning models on this dataset.various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.']","According to the document identified as 1908.05828, the baseline F1 score achieved by various machine learning models on the Nepali NER dataset is 86.89. This score represents a significant improvement over previously reported performances in literature. (Source: 1908.05828)",CNN modelBIBREF0 Stanford CRF modelBIBREF21,,,,0.7963018105647109
Which machine learning models do they explore?,"['various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.onclude with our observations in Section SECREF8.\n\nTo facilitate further research our code and dataset will be made available at github.com/link-yet-to-be-updated\n\n\n## Related Work\n\nThere has been a handful of research on Nepali NER task based on approaches like Support Vector Machine and gazetteer listBIBREF11 and Hidden Markov Model and gazetteer listBIBREF9,BIBREF10.language i.e limited data set to train the model. Traditional methods like Hidden Markov Model (HMM) with rule based approachesBIBREF9,BIBREF10, and Support Vector Machine (SVM) with manual feature-engineeringBIBREF11 have been applied but they perform poor compared to neural. However, there has been no research in Nepali NER using neural network. Therefore, we created the named entity annotated dataset partly with the help of Dataturk to train a neural model.']","According to the document 1908.05828, various machine learning models including neural network models are explored on this dataset.",BiLSTM BiLSTM-CNN BiLSTM-CRF BiLSTM-CNN-CRF,,,,
What is the size of the dataset?,"['en size from [8, 16, 32, 64, 128, 256, 512 1024]. Table TABREF30 shows all other hyper-parameter used in our experiment for both of the dataset.\n\n\n## Experiments ::: Effect of Dropout\n\nFigure FIGREF31 shows how we end up choosing 0.5 as dropout rate. When the dropout layer was not used, the F1 score are at the lowest. As, we slowly increase the dropout rate, the F1 score also gradually increases, however after dropout rate = 0.5, the F1 score starts falling down.nvolution, Rectified Linear Unit and Max-Pooling. The resulting vectors were concatenated and applied dropout of 0.5 before passing into linear layer to obtain the embedding size of 30 for the given word. This resulting embedding is concatenated with word embeddings, which is again concatenated with one-hot POS vector.he rare words whose count is below 5 are dropped. These word embeddings were not frozen during the training session because fine-tuning word embedding help achieve better performance compared to frozen oneBIBREF20.\n\nWe have used fasttext embeddings in particular because of its sub-word representation ability, which is very useful in highly inflectional language as shown in Table TABREF25. We have trained the word embedding in such a way that the sub-word size remains between 1 and 4.']","The size of the dataset is not explicitly mentioned in the provided documents. However, Table TABREF30 is referenced as showing all hyper-parameters used in the experiment for both datasets. For a precise size, please refer to Table TABREF30 as per the document 1908.05828.",Dataset contains 3606 total sentences and 79087 total entities.,0.0,,,
What is the source of their dataset?,"['ith the help of Dataturk to train a neural model. The texts used for this dataset are collected from various daily news sources from Nepal around the year 2015-2016.\n\nFollowing are our contributions:\n\nWe present a novel Named Entity Recognizer (NER) for Nepali language.ost-positions boosted the F1 score by almost 10%.\n\nIn order to label our dataset with POS-tags, we first created POS annotated dataset of 6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus and trained a BiLSTM model with 95.14% accuracy which was used to create POS-tags for our dataset.\n\nThe dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline.cting the NER tags had to be made on the dataset. The statistics of both the dataset is presented in table TABREF23.\n\nTable TABREF24 presents the total entities (PER, LOC, ORG and MISC) from both of the dataset used in our experiments. The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\n\n## Experiments\n\nIn this section, we present the details about training our neural network.']",The dataset used in their experiments is collected from various daily news sources from Nepal around the year 2015-2016. (Source: 1908.05828),daily newspaper of the year 2015-2016,,,,
How many different types of entities exist in the dataset?,"['cting the NER tags had to be made on the dataset. The statistics of both the dataset is presented in table TABREF23.\n\nTable TABREF24 presents the total entities (PER, LOC, ORG and MISC) from both of the dataset used in our experiments. The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\n\n## Experiments\n\nIn this section, we present the details about training our neural network.language i.e limited data set to train the model. Traditional methods like Hidden Markov Model (HMM) with rule based approachesBIBREF9,BIBREF10, and Support Vector Machine (SVM) with manual feature-engineeringBIBREF11 have been applied but they perform poor compared to neural. However, there has been no research in Nepali NER using neural network. Therefore, we created the named entity annotated dataset partly with the help of Dataturk to train a neural model.r compared to ILPRL dataset in terms of entities.\n\n\n## Conclusion and Future work\n\nIn this paper, we proposed a novel NER for Nepali language and achieved relative improvement of upto 10% and studies different factors effecting the performance of the NER for Nepali language.\n\nWe also present a neural architecture BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration.']","According to the information presented in Table TABREF24, there are four different types of entities in the dataset: PER, LOC, ORG, and MISC. This is detailed in the document with the ID 1908.05828.","OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities",,,,
How big is the new Nepali NER dataset?,"['various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.amed Entity Recognizer (NER) for Nepali language. To best of our knowledge we are the first to propose neural based Nepali NER.\n\nAs there are not good quality dataset to train NER we release a dataset to support future research\n\nWe perform empirical evaluation of our model with state-of-the-art models with relative improvement of upto 10%\n\nIn this paper, we present works similar to ours in Section SECREF2.Title: Named Entity Recognition for Nepali Language\n\nAbstract: Named Entity Recognition (NER) has been studied for many languages like English, German, Spanish, and others but virtually no studies have focused on the Nepali language. One key reason is the lack of an appropriate, annotated dataset. In this paper, we describe a Nepali NER dataset that we created. We discuss and compare the performance of various machine learning models on this dataset.']",The provided documents do not contain specific information regarding the size of the new Nepali NER dataset. I cannot answer this question based on the provided documents.,3606 sentences,,,,
What is the performance improvement of the grapheme-level representation model over the character-level model?,"['various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.sttext embedding respectively on the same corpus.\n\n\n## Approach ::: Features ::: Character-level embeddings\n\nBIBREF20 and BIBREF2 successfully presented that the character-level embeddings, extracted using CNN, when combined with word embeddings enhances the NER model performance significantly, as it is able to capture morphological features of a word. Figure FIGREF7 shows the grapheme-level CNN used in our model, where inputs to CNN are graphemes.grapheme clustered or sub-word embeddingBIBREF22. The experiments were performed on the dataset that we created and on the dataset received from ILPRL lab. Our extensive study shows that augmenting word embedding with character or grapheme-level representation and POS one-hot encoding vector yields better results compared to using general word embedding alone.\n\n\n## Approach\n\nIn this section, we describe our approach in building our model.']","According to the document 1908.05828, the grapheme-level representation model, when combined with BiLSTM models, achieves an overall F1 score of 86.89. The document also mentions that the character-level embedd0d, extracted using CNN and augmented with word embeddings, significantly enhances the NER model performance by capturing morphological features of a word. However, the specific performance improvement of the grapheme-level representation model over the character-level model is not explicitly stated in the provided documents.","On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement",,,,
Which models are used to solve NER for Nepali?,"['various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.ith the help of Dataturk to train a neural model. The texts used for this dataset are collected from various daily news sources from Nepal around the year 2015-2016.\n\nFollowing are our contributions:\n\nWe present a novel Named Entity Recognizer (NER) for Nepali language.amed Entity Recognizer (NER) for Nepali language. To best of our knowledge we are the first to propose neural based Nepali NER.\n\nAs there are not good quality dataset to train NER we release a dataset to support future research\n\nWe perform empirical evaluation of our model with state-of-the-art models with relative improvement of upto 10%\n\nIn this paper, we present works similar to ours in Section SECREF2.']","According to the document 1908.05828, various machine learning models, including a novel Named Entity Recognizer (NER) for Nepali language, are used to solve NER. The NER model is based on grapheme-level representations and has been trained using BiLSTM models. This approach has shown to outperform character-level representations, achieving an overall F1 score of 86.89. The text data used for this dataset were collected from various daily news sources in Nepal around 2015-2016. (Source: 1908.05828)","BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 CNN modelBIBREF0 and Stanford CRF modelBIBREF21",,,,
What language(s) is/are represented in the dataset?,"['he validation and test sets for these categories. To tackle this issue we applying a back-translation method BIBREF13, where we translate indirect and physical harassment tweets of the train set from english to german, french and greek. After that, we translate them back to english in order to achieve data augmentation.of a train set, a validation set and a test set. It was published for the ""First workshop on categorizing different types of online harassment languages in social media"". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment. We can see in Table TABREF1 the class distribution of our dataset.to english in order to achieve data augmentation. These ""noisy"" data that have been translated back, increase the number of indirect and physical harassment tweets and boost significantly the performance of our models.\n\nAnother way to enrich our models is the use of pre-trained word embeddings from 2B Twitter data BIBREF14 having 27B tokens, for the initialization of the embedding layer.']","The dataset includes tweets translated into German, French, and Greek, as stated in the document with the ID 1909.13104.",english,,,,
What baseline model is used?,"['ention-based approach and a few baseline methods. According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance. Also, we tackled the problem of the imbalance between the training, validation and test sets performing the technique of back-translation.\n\nIn the future, we would like to perform more experiments with this dataset applying different models using BERT BIBREF21.ore. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.et applying different models using BERT BIBREF21. Also, we would like to apply the models presented in this work, in other datasets about hate speech in social media.']","According to the information provided in Source: 1909.13104, the baseline models used include an ention-based approach and a few baseline methods. However, the specifics of these baseline methods are not explicitly mentioned.",LastStateRNN AvgRNN AttentionRNN,,,,0.0
Which variation provides the best results on this dataset?,"['from tweets having the aforementioned categories. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models. The next Section includes a short description of the related work, while the third Section includes a description of the dataset. After that, we describe our methodology., we are performing data augmentation techniques. However, the dataset is imbalanced and this has a significant impact in our results.\n\n\n## Proposed methodology ::: Data augmentation\n\nAs described before one crucial issue that we are trying to tackle in this work is that the given dataset is imbalanced. Particularly, there are only a few instances from indirect and physical harassment categories respectively in the train set, while there are much more in the validation and test sets for these categories.nd small batches might help to generalize better. Also, we set other hyperparameters as: epochs = 20, patience = 10. As early stopping criterion we choose the average AUC, because our dataset is imbalanced.\n\nThe training process is based on the optimization of the loss function mentioned below and it is carried out with the Adam optimizer BIBREF20, which is known for yielding quicker convergence.']","According to the provided documents, specifically the document with the ID 1909.13104, the best results on this dataset are achieved using the average AUC as an early stopping criterion due to the dataset's imbalance. This suggests that the methodology that prioritizes the average AUC for early stopping (possibly the one using Adam optimizer BIBREF20 with small batches and epochs set to 20 with patience at 10) may provide the best results. However, the documents do not explicitly state which variation is superior. Therefore, I cannot provide a definitive answer based on the provided documents. (1909.13104)",the model with multi-attention mechanism and a projected layer,,,,
What are the different variations of the attention-based approach which are examined?,"['from tweets having the aforementioned categories. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models. The next Section includes a short description of the related work, while the third Section includes a description of the dataset. After that, we describe our methodology.he Projected Layer had a significant enhancement.\n\n\n## Conclusion - Future work\n\nWe present an attention-based approach for the detection of harassment language in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach and a few baseline methods.eep, classification specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach.\n\n\n## Introduction\n\nIn the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women.']",The different variations of the attention-based approach examined include multi-attention and single attention models. This information is derived from the document with the Source ID 1909.13104.,"classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer",,,,
What dataset is used for this work?,"[', we are performing data augmentation techniques. However, the dataset is imbalanced and this has a significant impact in our results.\n\n\n## Proposed methodology ::: Data augmentation\n\nAs described before one crucial issue that we are trying to tackle in this work is that the given dataset is imbalanced. Particularly, there are only a few instances from indirect and physical harassment categories respectively in the train set, while there are much more in the validation and test sets for these categories.lass the abusive text belongs to. Pitsilis et al. BIBREF12 have proposed a detection scheme that is an ensemble of RNN classiﬁers, which incorporates various features associated with user related information, such as the users’ tendency towards racism or sexism\n\n\n## Dataset description\n\nThe dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set.et applying different models using BERT BIBREF21. Also, we would like to apply the models presented in this work, in other datasets about hate speech in social media.']","The dataset used for this work consists of a train set, a validation set, and a test set from Twitter, as mentioned in the document with the Source ID 1909.13104.",Twitter dataset provided by the organizers,,,,
What types of online harassment are studied?,"['hich undermines other types of online harassment. However, there is no much study focusing on different types online harassment alone attracting natural language processing techniques.\n\nIn this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks.of a train set, a validation set and a test set. It was published for the ""First workshop on categorizing different types of online harassment languages in social media"". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment. We can see in Table TABREF1 the class distribution of our dataset.amount of user generated content in these media. So, the research about the automated detection of content containing sexual or racist harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. Previous studies have been focused on collecting data about sexism and racism in very broad terms. However, there is not much study focusing on different types of online harassment alone attracting natural language processing techniques.']","The types of online harassment studied are indirect harassment, sexual harassment, and physical harassment. This information is derived from the document identified as Source: 1909.13104.","indirect harassment, sexual and physical harassment",,,,0.9276462477244068
What was the baseline?,"['ention-based approach and a few baseline methods. According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance. Also, we tackled the problem of the imbalance between the training, validation and test sets performing the technique of back-translation.\n\nIn the future, we would like to perform more experiments with this dataset applying different models using BERT BIBREF21.Title: Attention-based method for categorizing different types of online harassment language\n\nAbstract: In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter, because of the large amount of user generated content in these media.le TABREF1 the class distribution of our dataset. One important issue here is that the categories of indirect and physical harassment seem to be more rare in the train set than in the validation and test sets. To tackle this issue, as we describe in the next section, we are performing data augmentation techniques.']","According to the document 1909.13104, the baseline was an ""ention-based approach and a few baseline methods."" The multi-attention method using a projected layer, however, achieved the highest performance.",LastStateRNN AvgRNN AttentionRNN,,0.0,0.6666666666666666,0.7874410693322605
What were the datasets used in this paper?,"['of a train set, a validation set and a test set. It was published for the ""First workshop on categorizing different types of online harassment languages in social media"". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment. We can see in Table TABREF1 the class distribution of our dataset.lass the abusive text belongs to. Pitsilis et al. BIBREF12 have proposed a detection scheme that is an ensemble of RNN classiﬁers, which incorporates various features associated with user related information, such as the users’ tendency towards racism or sexism\n\n\n## Dataset description\n\nThe dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set., we are performing data augmentation techniques. However, the dataset is imbalanced and this has a significant impact in our results.\n\n\n## Proposed methodology ::: Data augmentation\n\nAs described before one crucial issue that we are trying to tackle in this work is that the given dataset is imbalanced. Particularly, there are only a few instances from indirect and physical harassment categories respectively in the train set, while there are much more in the validation and test sets for these categories.']","The datasets used in this paper were a train set, a validation set, and a test set, as stated in the document with the Source ID 1909.13104.",The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.,,,,0.9095975804544408
What are labels in car speak language dataset?,"['Title: Understanding Car-Speak: Replacing Humans in Dealerships\n\nAbstract: A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in ""car-speak"". Car-speak is abstract language that pertains to a car\'s physical attributes.eak to use for exploration and training purposes. This data set id full of vehicle reviews from U.S. News BIBREF5. These reviews provide a reasonable set of car-speak data that we can study.\n\nFinally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.\n\nIn the future we plan to use more complex models to attempt to understand car-speak.age that pertains to a car\'s physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.\n\n\n## Introduction\n\nA large portion of the car-buying experience in the United States involves interactions at a car dealership BIBREF0, BIBREF1, BIBREF2.']","According to the document 2002.02070, labels in a car speak language dataset refer to car-speak that pertains to a car's physical attributes. The document also mentions the aim to curate a reasonable data set of car-speak language, which likely includes these labels.",car,,,,0.9333883297864407
How big is dataset of car-speak language?,"[""eak to use for exploration and training purposes. This data set id full of vehicle reviews from U.S. News BIBREF5. These reviews provide a reasonable set of car-speak data that we can study.\n\nFinally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.\n\nIn the future we plan to use more complex models to attempt to understand car-speak.age that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.\n\n\n## Introduction\n\nA large portion of the car-buying experience in the United States involves interactions at a car dealership BIBREF0, BIBREF1, BIBREF2.seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.\n\n\n## Conclusion & Future Work\n\nIn this paper we aim to provide an introductory understanding of car-speak and a way to automate car dealers at dealerships. We first provide a definition of “car-speak” in Section SECREF3. We explore what constitutes car-speak and how to identify car-speak.\n\nWe also gather a data set of car-speak to use for exploration and training purposes.""]","According to the information provided in Source: 2002.02070, the dataset of car-speak language used for exploration and training purposes is substantial, although the exact size is not explicitly mentioned. It is implied to be large enough to train several classifiers and perform reasonably well across all metrics.","$3,209$ reviews",,,,
What is the performance of classifiers?,"[""view vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.\n\nIn order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics.eak to use for exploration and training purposes. This data set id full of vehicle reviews from U.S. News BIBREF5. These reviews provide a reasonable set of car-speak data that we can study.\n\nFinally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.\n\nIn the future we plan to use more complex models to attempt to understand car-speak.d convey their needs to the sales representative. Finally, the sales representative uses their internal classifier to translate this car-speak into actual physical attributes (e.g. `fast' $ \\longrightarrow $ `700 horsepower & a sleek form factor') and offers a car to the buyer.\n\nUnderstanding car-speak is not a trivial task. Figure FIGREF4 shows two cars that have high top speeds, however both cars may not be considered “fast”.""]","According to the document 2002.02070, the performance of the classifiers, specifically the K Nearest Neighbors (KNN) classifier, was evaluated using 4-fold cross validation on a shuffled data set. The KNN classifier demonstrated the best performance across all four metrics, including the F1 micro and F1 macro scores. Other classifiers, including Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP), did not perform as well. This information is based on the performance evaluation section in the document.","Table TABREF10  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject",,,,
What classifiers have been trained?,"['on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.\n\n\n## Translating Car-Speak ::: Classification Experiments\n\nWe train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8.eak to use for exploration and training purposes. This data set id full of vehicle reviews from U.S. News BIBREF5. These reviews provide a reasonable set of car-speak data that we can study.\n\nFinally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.\n\nIn the future we plan to use more complex models to attempt to understand car-speak.view vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.\n\nIn order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics.']","The classifiers that have been trained are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP). This information is based on the document with the ID 2002.02070.","KNN
RF
SVM
MLP",,,,
How does car speak pertains to a car's physical attributes?,"['Title: Understanding Car-Speak: Replacing Humans in Dealerships\n\nAbstract: A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in ""car-speak"". Car-speak is abstract language that pertains to a car\'s physical attributes.r instance, “I need a car that goes really fast”. This could mean that they need a car with a V8 engine type or a car that has 500 horsepower, but the buyer does not know that, all they know is that they need a “fast” car.\n\nThe term “fast” is car-speak. Car-speak is abstract language that pertains to a car\'s physical attribute(s). In this instance the physical attributes that the term “fast” pertains to could be the horsepower, or it could be the car\'s form factor (how the car looks).“luxury” are among the top words used in reviews. These words are very good examples of car-speak. Both words are abstract descriptions of cars, but both have physical characteristics that are associated with them as we discussed in Section SECREF3.\n\n\n## Translating Car-Speak ::: TF-IDF\n\nSo far we have compiled the most relevant terms in from the reviews. We now need to weight these terms for each review, so that we know the car-speak terms are most associated with a car.']","Car-speak pertains to a car's physical attributes through terms like ""fast"" and ""luxury"". According to the document 2002.02070, ""fast"" could relate to horsepower or the car's form factor, while ""luxury"" is an abstract description associated with specific physical characteristics of a car.",we do not know exactly,,,,
What topic is covered in the Chinese Facebook data? ,"['two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld.cs, languages, data distributions, and platforms.\n\nThe FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.']","The Chinese Facebook data covers the topic of anti-nuclear power, as stated in the document 1611.03599.",anti-nuclear-power,,,,
How many layers does the UTCNN model have?,"[""gram, bigram, and trigram n-gram representations. This is succeeded by an average pooling layer for an element-wise average of the three maximized convolution outputs.\n\n\n## UTCNN Model Description\n\nFigure FIGREF10 illustrates the UTCNN model. As more than one user may interact with a given post, we first add a maximum pooling layer after the user matrix embedding layer and user vector embedding layer to form a moderator matrix embedding INLINEFORM0 and a moderator vector embedding INLINEFORM1 for moderatorfferent because of their different engaged users. The output comment representation together with the commenter vector embedding INLINEFORM2 and topic vector embedding INLINEFORM3 are concatenated and a maximum pooling layer is added to select the most important feature for comments. Instead of requiring that the comment stance agree with the post, UTCNN simply extracts the most important features of the comment contents; they could be helpful, whether they show obvious agreement or disagreement.ther they show obvious agreement or disagreement. Therefore when combining comment information here, the maximum pooling layer is more appropriate than other pooling or merging layers. Indeed, we believe this is one reason for UTCNN's performance gains.\n\nFinally, the pooled comment representation, together with user vector embedding INLINEFORM0 , topic vector embedding INLINEFORM1 , and document representation are fed to a fully connected network, and softmax is applied to yield the final stance label predi""]","The UTCNN model includes at least three distinct layers or stages: user matrix embedding, maximum pooling layer, and fully connected network. These are referenced in the documents with the IDs 1611.03599, 1611.03599, and 1611.03599, respectively. However, the documents do not explicitly state the total number of layers in the model. Therefore, based on the provided documents, I cannot provide a definitive answer on the total number of layers in the UTCNN model.",eight layers,,,,
What topics are included in the debate data?,"['e subject of debates provides useful information. In sum, Table TABREF22 together with Table TABREF24 show that UTCNN achieves promising performance regardless of topic, language, data distribution, and platform.\n\n\n## Conclusion\n\nWe have proposed UTCNN, a neural network model that incorporates user, topic, content and comment information for stance classification on social media texts. UTCNN learns user embeddings for all users with minimum active degree, i.e., one post or one like.user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld.']","The debate data includes topics, as indicated by the content in Source: 1611.03599, which mentions ""user, topic, or comment information.""",abortion gay rights Obama marijuana,,,,0.9013085256630674
What is the size of the Chinese data?,"[""tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld.cs, languages, data distributions, and platforms.\n\nThe FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.ition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .\n\nThe FBFans dataset has more integrated functions than the CreateDebate dataset; thus our model can utilize all linguistic and extra-linguistic features.""]","The Chinese data consists of posts from anti-nuclear-power Chinese Facebook fan groups, with 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. (Source: 1611.03599)","32,595 posts",,0.0,,
What are the baselines?,"['usually more important for further applications. For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work BIBREF7 , BIBREF9 , BIBREF12 .\n\n\n## Baselines\n\nWe pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings ofof UTCNN and the baselines on the FBFans dataset. Here Majority yields good performance on Neu since FBFans is highly biased to the neutral class. The SVM models perform well on Sup and Neu but perform poorly for Uns, showing that content information in itself is insufficient to predict stance labels, especially for the minor class. With the transformed word embedding feature, SVM can achieve comparable performance as SVM with n-gram feature.ers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement.']","According to the document with the ID 1611.03599, the baselines used include: 1) SVM with unigram, bigram, and trigram features, and 2) SVM with average word embedding. These baselines are used for comparison in the CreateDebate dataset.","SVM with unigram, bigram, and trigram features SVM with average word embedding SVM with average transformed word embeddings CNN ecurrent Convolutional Neural Networks SVM and deep learning models with comment information",0.4,,,0.8678199468857334
What transfer learning tasks are evaluated?,"['ngs that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.  ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.erSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by devlin2018bert for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.']","Sentence embeddings from SBERT are evaluated on various tasks, including STS and transfer learning tasks. According to the document with the ID 1908.10084, SBERT outperforms other state-of-the-art sentence embedd0ding methods in these tasks.",MR CR SUBJ MPQA SST TREC MRPC,,0.0,,0.8291065990816119
What metrics are used for the STS tasks?,"[""STS without using any STS specific training data. We use the STS tasks 2012 - 2016 BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, the STS benchmark BIBREF10, and the SICK-Relatedness dataset BIBREF21. These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.ngs that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.  ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.ts are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?\n\nResults are presented in Table TABREF14. Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al.\n\n\n## Evaluation - SentEval\n\nSentEval BIBREF6 is a popular toolkit to evaluate the quality of sentence embeddings.""]","The metrics used for the STS tasks include semantic relatedness labels ranging from 0 to 5, and accuracy as the evaluation metric, which determines if the positive example is closer to the anchor than the negative example. These are referenced from the documents with Source IDs 1908.10084 and 1908.10084 respectively.",Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels,,0.9999999999,,
How much time takes its training?,"['tarted the training from a random initialization. In this publication, we use the pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings. This reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods.\n\n\n## Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding.F23.\n\nThe results are depicted in Table TABREF10. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points.into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regression method for the output.\n\nWe use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances BIBREF23.\n\nThe results are depicted in Table TABREF10.']","According to the document 1908.10084, SBERT can be tuned in less than 20 minutes.",20 minutes,1.0,0.9999999999,,
How are the siamese networks trained?,"['rchitecture to predict the surrounding sentences. InferSent BIBREF4 uses labeled data of the Stanford Natural Language Inference dataset BIBREF13 and the Multi-Genre NLI dataset BIBREF14 to train a siamese BiLSTM network with max-pooling over the output. Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder BIBREF5 trains a transformer network and augments unsupervised learning with training on SNLI.ts are suitable for training sentence embeddings. yang-2018-learning presented a method to train on conversations from Reddit using siamese DAN and siamese transformer networks, which yielded good results on the STS benchmark dataset.\n\npolyencoders addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-computed candidate embeddings using attention.MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks BIBREF15 to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available training data. We experiment with the following structures and objective functions.\n\nClassification Objective Function.']","Siamese networks are trained using labeled data from datasets like BIBREF13 and BIBREF14 to produce semantically meaningful sentence embeddings. This is done by fine-tuning the weights of the model using cosine-similarity as an objective function, as described in the document with the Source ID 1908.10084.",update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity. Classification Objective Function Regression Objective Function Triplet Objective Function,,,,0.9326346353974699
What other sentence embeddings methods are evaluated?,"['ided by the popular bert-as-a-service-repository. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought BIBREF12 trains an encoder-decoder architecture to predict the surrounding sentences.EF7. Section SECREF5 evaluates SBERT on SentEval. In section SECREF6, we perform an ablation study to test some design aspect of SBERT. In section SECREF7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods.\n\n\n## Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.lts for the quality of SBERT sentence embeddings. In this section, we perform an ablation study of different aspects of SBERT in order to get a better understanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs.']","According to the document 1908.10084, besides SBERT, the quality of sentence embeddings is evaluated using lts. An ablation study of different aspects of SBERT is also conducted to understand their relative importance. Different pooling strategies like MEAN, MAX, and CLS are evaluated. For the classification objective function, different concatenation methods are assessed. The performances for each possible configuration are averaged over 10 different random seeds.","GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent",,,0.0,0.8682563303481837
Which pretrained word vectors did they use?,"[""larization and finally, sigmoid activation layer. For fair comparison, both baselines were trained using the same training procedure as our method.\n\n\n## Embeddings\n\nAs a text embedding in our experiments, we use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC). Since their output dimensionality can be modified, we show the results for varying dimensionality sizes.that were successfully applied in other domains. We follow BIBREF5 and use pre-trained GloVe word vectors BIBREF16 to initialize the embedding layer (also known as look-up table). Section SECREF18 discusses the embedding layer in more details.\n\n\n## Bidirectional Long Short-Term Memory Network\n\nOur method for popularity prediction using article's title is inspired by a bidirectional LSTM architecture. The overview of the model can be seen in Fig.how the results for varying dimensionality sizes. On top of that, we evaluate two training approaches: using static word vectors and fine-tuning them during training phase.\n\n\n## Results\n\nThe results of our experiments can be seen in Tab. TABREF21 and TABREF22 . Our proposed BiLSTM approach outperforms the competing methods consistently across both datasets.""]","According to the document 1707.06806, they used pre-trained GloVe word vectors BIBREF16.",pre-trained GloVe word vectors,,,,
What evaluation metrics are used?,"['tocol with INLINEFORM1 with random dataset split. We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.\n\n\n## Datasets\n\nIn this section we present two datasets used in our experiments: The NowThisNews dataset, collected for the purpose of this paper, and The BreakingNews dataset BIBREF4 , publicly available dataset of news articles.s approaches, gives highly interpretable results. Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.aus. We also employ early stopping strategy, i.e. stopping the training algorithm before convergence based on the values of loss function on the validation set.\n\n\n## Evaluation\n\nIn this section, we evaluate our method and compare its performance against the competitive approaches. We use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split.']","The performance is measured using the standard accuracy metric, as stated in Source: 1707.06806. The accuracy metric is defined as the ratio between correctly classified data samples from the test dataset and all test samples.",standard accuracy metric,,,,
Which shallow approaches did they experiment with?,"['s approaches, gives highly interpretable results. Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.hes to this problem, such as max or mean pooling. The initial experiments showed that the highest performance was achieved using late fusion approach, that is by concatenating the last hidden state in forward and backward sequence. The intuition behind this design choice is that the importance of the first few words of the headline is relatively high, as the information contained in INLINEFORM1 , i.e.peting methods consistently across both datasets. The performance improvement is especially visible for The NowThisNews dataset and reaches over 15% with respect to the shallow architecture in terms of of accuracy. Although the improvement with respect to the other methods based on deep neural network is less evident, the recurrent nature of our method provides much more intuitive interpretation of the results and allow for parsing the contribution of each single word to the overall score.']","According to the information provided in Source: 1707.06806, they experimented with max and mean pooling as shallow approaches.",SVM,,,,0.9008288224064671
Where do they obtain the news videos from?,"['F4 , publicly available dataset of news articles.\n\ncontains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016. For each post we collected its title and the number of views of the corresponding video, which we consider our popularity metric.opularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small.ibution of each single word to the overall score.\n\nTo present how our model works in practice, we show in Tab. TABREF23 a list of 3 headlines from NowThisNews dataset that are scored with the highest probability of belonging to a popular class, as well as 3 headlines with the lowest score. As can be seen, our model correctly detected videos that become viral at the same time assigning low score to content that underperformed.']","According to the document with Source ID 1707.06806, the news videos are obtained from the NowThisNews Facebook page.",NowThisNews Facebook page,,,,
What is the source of the news articles?,"[""F4 , publicly available dataset of news articles.\n\ncontains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016. For each post we collected its title and the number of views of the corresponding video, which we consider our popularity metric.using only textual features of the article title. We also validate our proposed method on one dataset collected using a different social media platform, namely Facebook, and another one created from various news articles BIBREF4 .\n\nRecently, several works have touched on the topic of popularity prediction of news article from a multimodal perspective BIBREF4 , BIBREF14 .rities were observed across content domains, e.g. log-normal distribution of data popularity BIBREF10 , in this work we focus only on textual content and, more precisely, on the popularity of news articles and its relation to the article's title.\n\nForecasting popularity of news articles was especially well studied in the context of Twitter - a social media platform designed specifically for sharing textual data BIBREF11 , BIBREF12 .""]","The news articles are sourced from NowThisNews Facebook page, as stated in Source: 1707.06806.","main news channels, such as Yahoo News, The Guardian or The Washington Post",,,,0.9012977618471777
which non-english language had the best performance?,"['any interference from machine translation errors. Using the English data for testing, the model achieved 87.06% accuracy where a majority baseline was 68.37% and the lexicon-based baseline was 60.10%.\n\nConsidering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs.0.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages. Building separate models for each language requires both labeled and unlabeled data. Even though having lots of labeled data in every language is the perfect case, it is unrealistic.sh words might be causing the misclassifications. However, a large portion of the missclassifications is not caused due to not-translated words. At the end, the machine translation errors has some but not noticeable effects on our model. Therefore, we can claim that machine translation preserves most of the information necessary for sentiment analysis.\n\nWe also evaluated our model with an English corpus BIBREF27 to see its performance without any interference from machine translation errors.']","According to the document 1806.04511, Turkish had the best performance among non-English languages, with a relative improvement of 37.13% over the baseline.",Russian,,,,0.9321703595854143
which non-english language was the had the worst results?,"['any interference from machine translation errors. Using the English data for testing, the model achieved 87.06% accuracy where a majority baseline was 68.37% and the lexicon-based baseline was 60.10%.\n\nConsidering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs.h each token to check if the token is in English. In this way, we counted the number of observations that contained at least one non-English word and divided that with the size of the misclassifications set. We used this ratio to investigate the effect of machine translation errors.\n\nWe found that 25.84% of Dutch, 21.76% of Turkish, 24.46% Spanish, and 10.71% of Russian reviews that were misclassified had non-English words in them. These non-English words might be causing the misclassifications.fect of incorrect translations into our approach. To that end, we extracted all wrong predictions from the test set and computed the ratio of misclassifications that have non-English words in them. We first extracted all misclassifications for a given language and for each observation in the misclassification set, we iterated through each token to check if the token is in English.']","According to the data in the document 1806.04511, Russian reviews had the worst results with 10.71% of them containing non-English words, which might be causing misclassifications. (Source: 1806.04511)",Turkish,,0.9999999999,0.5,0.7759942788169408
what datasets were used in evaluation?,"['to make the model more specialized in the domain. In this paper we focus on restaurant reviews as our domain and use Yelp restaurant reviews dataset extracted from Yelp Dataset Challenge BIBREF25 and restaurant reviews dataset as part of a Kaggle competition BIBREF26 .\n\nFor evaluation of the multilingual approach, we use four languages. These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 .elines in all four datasets (see Table TABREF9 ). Also for Spanish restaurant review, the lexicon-based baseline is below the majority baseline which shows that solely translating data and using lexicons is not sufficient to achieve good results in multilingual sentiment analysis.\n\nAmong the wrong classifications for each test set, we calculated the percentage of false positives and false negatives. Table TABREF10 shows the distribution of false positives and false negatives for each class.lways predicts the majority class in the dataset. For example, if the dataset has 60% of all reviews positive and 40% negative, majority baseline would be 60% because a model that always predicts “positive” will be 60% accurate and will make mistakes 40% of the time.\n\nIn addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review.']",The datasets used in evaluation were Yelp restaurant reviews dataset extracted from Yelp Dataset Challenge BIBREF25 and restaurant reviews dataset as part of a Kaggle competition BIBREF26. (Source: 1806.04511),"SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28",,0.9999999999,,
what are the baselines?,"[""rms the baselines in several different languages.\n\n\n## Introduction\n\nWith the steady growth in the commercial websites and social media venues, the access to users' reviews have become easier. As the amount of data that can be mined for opinion increased, commercial companies' interests for sentiment analysis increased as well.s: one for each language) and compared the means. Post hoc comparisons using the Tukey HSD test indicated that the mean accuracies for baselines (majority and lexicon-based) are significantly different than RNN accuracies as can be seen in Table TABREF12 (family-wise error rate=0.06). When RNN is compared with lexicon-based baseline and majority baseline, the null hypothesis can be rejected meaning that each test is significant.lse positives and false negatives for each class. In all four classes, the number of false negatives are more than the number of false positives. This can be explained by the unbalanced training dataset where the number of positive reviews are more than the number of negative reviews (59,577 vs 17,132).\n\nTo be able to see the difference between baseline and RNN, we took each method's results as a group (4 values: one for each language) and compared the means.""]","The baselines mentioned are majority and lexicon-based, as indicated in the source document with the ID 1806.04511.",majority baseline lexicon-based approach,,,,
how did the authors translate the reviews to other languages?,"['use it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.in the language that had largest data available. We took a general-to-specific model building strategy where the larger corpus that had reviews from different domains was first used to train the RNN model and a smaller single-domain corpus of sentiment reviews was used to specialize the model on the given domain. During scoring time, we used corpora for the given domain in different languages and translated them to English to be able to classify sentiments with the trained model.the model more specialized for a specific domain. We further train the model with domain-specific English reviews and use this trained model to score reviews that share the same domain from different languages. To be able to employ the trained model, test sets are first translated to English via machine translation and then inference takes place. Figure FIGREF1 shows our multilingual sentiment analysis approach.']","The authors translated the reviews into English via machine translation, as indicated in the document with the ID 1806.04511.",Using Google translation API.,,0.0,0.6666666666666666,
what dataset was used for training?,"['rent training sets as provided in Table TABREF5 . One of these three datasets (Amazon reviews BIBREF23 , BIBREF24 ) is larger and has product reviews from several different categories including book reviews, electronics products reviews, and application reviews. The other two datasets are to make the model more specialized in the domain.ent languages (Spanish, Turkish, Dutch, Russian). We focus on polarity detection in reviews, therefore all datasets in this study have two class values (positive, negative).\n\nWith the goal of building a generalizable sentiment analysis model, we used three different training sets as provided in Table TABREF5 .further training with data on a specific domain. To evaluate the reusability of the sentiment analysis model, we test with non-English datasets. We first translate the test set to English and use the pre-trained model to score polarity in the translated text. In this way, our proposed approach eliminates the need to train language-dependent models, use of sentiment lexicons and word embeddings for each language.']",The training was conducted using three different datasets as provided in Table TABREF5 (Source: 1806.04511).,Amazon reviews Yelp restaurant reviews restaurant reviews,,0.9999999999,,
How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?,"['nificant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.\n\n\n## Introduction\n\nDecoding intended speech or motor activity from brain signals is one of the major research areas in Brain Computer Interface (BCI) systems BIBREF0 , BIBREF1 .23.45% improvement of accuracy over the baseline.\n\nProduction of articulatory speech is an extremely complicated process, thereby rendering understanding of the discriminative EEG manifold corresponding to imagined speech highly challenging. As a result, most of the existing approaches failed to achieve satisfactory accuracy on decoding speech tokens from the speech imagery EEG data. Perhaps, for these reasons, very little work has been devoted to relating the brain signals to the underlying articulation.lassification tasks from speech imagery EEG data. Unlike previous approaches which mostly deal with subject-dependent classification of EEG into discrete vowel or word labels, this work investigates a subject-invariant mapping of EEG data with different phonological categories, varying widely in terms of underlying articulator motions (eg: involvement or non-involvement of lips and velum, variation of tongue movements etc).']","The research demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for speech by showing a 22.5% improvement over previous methods in their work, as indicated in the Source document with ID 1904.04358. They also investigated a subject-invariant mapping of EEG data with different phonological categories, which varies widely in terms of underlying articulator motions. This suggests that the EEG data can be used to understand the complex process of articulatory speech.",we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .,,0.9999999999,,
What are the five different binary classification tasks?,"['hort term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods.er the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.problem and compared our performance with theirs. Our proposed method highly outperforms the existing methods across all the five binary classification tasks by a large average margin of 22.51%.\n\n\n## Acknowledgments\n\nThis work was funded by the Natural Sciences and Engineering Research Council (NSERC) of Canada and Canadian Institutes for Health Research (CIHR).']","The five different binary classification tasks are: presence/absence of consonants, phonemic nasal, bilabial, high-front vowels, and high-back vowels. (Source: 1904.04358)","presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.",,,,
How was the spatial aspect of the EEG signal computed?,"['ing based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder.s through measurement of related electric fields. EEG is non-invasive, portable, low cost, and provides satisfactory temporal resolution. This makes EEG suitable to realize BCI systems. EEG data, however, is challenging: these data are high dimensional, have poor SNR, and suffer from low spatial resolution and a multitude of artifacts. For these reasons, it is not particularly obvious how to decode the desired information from raw EEG signals.ntent from the signals in the selected bandwidth.\n\n\n## Joint variability of electrodes\n\nMultichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space.']","The spatial aspect of the EEG signal was computed by measuring the related electric fields, as stated in the document with the Source ID: 1904.04358.","we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.",,,,
What data was presented to the subjects to elicit event-related responses?,"['/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech.(cross-validation experiment) in Table TABREF15 . Since the model encounters the unseen data of a new subject for testing, and given the high inter-subject variability of the EEG data, a reduction in the accuracy was expected. However, our network still managed to achieve an improvement of 18.91, 9.95, 67.15, 2.83 and 13.70 % over BIBREF17 .lassification tasks from speech imagery EEG data. Unlike previous approaches which mostly deal with subject-dependent classification of EEG into discrete vowel or word labels, this work investigates a subject-invariant mapping of EEG data with different phonological categories, varying widely in terms of underlying articulator motions (eg: involvement or non-involvement of lips and velum, variation of tongue movements etc).']","The subjects were presented with words like ""pat"", ""pot"", ""knew"", and ""gnaw"" to elicit event-related responses, as per the information in Source: 1904.04358.","7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)",,,,
How many electrodes were used on the subject in EEG sessions?,"['(cross-validation experiment) in Table TABREF15 . Since the model encounters the unseen data of a new subject for testing, and given the high inter-subject variability of the EEG data, a reduction in the accuracy was expected. However, our network still managed to achieve an improvement of 18.91, 9.95, 67.15, 2.83 and 13.70 % over BIBREF17 .ntent from the signals in the selected bandwidth.\n\n\n## Joint variability of electrodes\n\nMultichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space.ature representations from single-trial EEG data. Besides, we found that instead of using the raw multi-channel high-dimensional EEG requiring large training times and resource requirements, it is advantageous to first reduce its dimensionality by capturing the information transfer among the electrodes.']",The provided documents do not specify the number of electrodes used in the EEG sessions.,1913 signals,,,1.0,0.0
How many subjects does the EEG data come from?,"['/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech.(cross-validation experiment) in Table TABREF15 . Since the model encounters the unseen data of a new subject for testing, and given the high inter-subject variability of the EEG data, a reduction in the accuracy was expected. However, our network still managed to achieve an improvement of 18.91, 9.95, 67.15, 2.83 and 13.70 % over BIBREF17 .lassification tasks from speech imagery EEG data. Unlike previous approaches which mostly deal with subject-dependent classification of EEG into discrete vowel or word labels, this work investigates a subject-invariant mapping of EEG data with different phonological categories, varying widely in terms of underlying articulator motions (eg: involvement or non-involvement of lips and velum, variation of tongue movements etc).']","According to the information in the document with the Source ID 1904.04358, the EEG data come from 14 different subjects.",14,1.0,0.9999999999,,

=== Evaluation Summary ===
{'context_recall': 0.3790, 'context_precision': 0.4868, 'faithfulness': 0.5956, 'answer_relevancy': 0.7828}